{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# PheKnowLator - Ontology Cleaning\n",
    "***\n",
    "***\n",
    "\n",
    "**Author:** [TJCallahan](https://mail.google.com/mail/u/0/?view=cm&fs=1&tf=1&to=callahantiff@gmail.com)  \n",
    "**GitHub Repository:** [PheKnowLator](https://github.com/callahantiff/PheKnowLator/wiki)  \n",
    "**Release:** **[v2.0.0](https://github.com/callahantiff/PheKnowLator/wiki/v2.0.0)**\n",
    "  \n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook serves as a script to help prepare ontologies prior to be ingested into the knowledge graph build algorithm. This script performs the following steps:  \n",
    "1. [Clean Ontologies](#clean-ontologies)  \n",
    "2. [Merge Ontologies](#merge-ontologies)  \n",
    "3. [Normalize Classes](#normalize-classes)\n",
    "\n",
    "## Assumptions and Dependencies  \n",
    "  \n",
    "**Assumptions:**   \n",
    "- Knowledge Graph Build Steps 1-2 (i.e. data downloading and master edge list creation) have already been performed  \n",
    "- Directory of Imported Ontologies ➞ `./resources/ontologies`    \n",
    "- Processed data write location ➞ `./resources/ontologies`  \n",
    "\n",
    "**Dependencies:**   \n",
    "- This notebook utilizes several helper functions, which are stored in the [`kg_utils.py`](https://github.com/callahantiff/PheKnowLator/blob/master/pkt_kg/utils/kg_utils.py) script. Hyperlinks to all downloaded and generated data sources are provided on the [Data Sources](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources) Wiki page as well as within each source subsection of this notebook. All generated data is freely available for download from DropBox. \n",
    "- [`OWLTools`](https://github.com/owlcollab/owltools)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "_____\n",
    "\n",
    "### PheKnowLator Build V2.0.0 Ontology Cleaning Summary  \n",
    "**Date:** `12/01/2020` \n",
    "***\n",
    "\n",
    "The table below is meant to provide a high-level overview of the modifications that we applied to each individual ontology as well as to the merged ontology file for the latest PheKnowLator build.\n",
    "\n",
    "Ontology | Value Errors | Punning Errors | Class Identifier Errors | Obsolete/Deprecated Classes | Normalization Errors  \n",
    ":---:    | :---:        | :---:          | :---:                   | :---:                       | :---:  \n",
    "ChEBI    | 0            | 0              | 0                       | 14/18498                    | X    \n",
    "CLO      | 1            | 8              | 0                       | 14/2                        | X    \n",
    "MONDO    | 0            | 0              | 0                       | 1945/2196                   | X    \n",
    "GO       | 0            | 0              | 0                       | 3121/6302                   | X    \n",
    "HP       | 0            | 0              | 0                       | 297/348                     | X  \n",
    "PW       | 0            | 0              | 0                       | 6/42                        | X    \n",
    "PRO - Human | 0         | 0              | 0                       | 0/0                         | X  \n",
    "SO       | 0            | 0              | 0                       | 0/0                         | X    \n",
    "UBERON   | 0            | 0              | 0                       | 1062/1564                   | X   \n",
    "VO       | 0            | 0              | 2                       | 10/0                        | X    \n",
    "RO       | 0            | 0              | 0                       | 0/0                         | X  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***  \n",
    "## Set-Up Environment\n",
    "***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from owlready2 import *\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, Literal\n",
    "from rdflib.namespace import OWL, RDF, RDFS \n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "\n",
    "# import script containing helper functions\n",
    "from pkt_kg.utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment variables\n",
    "ontologies = ['chebi', 'clo', 'ext', 'go', 'hp', 'mondo', 'pro', 'pw', 'ro', 'so', 'vo']\n",
    "write_location = 'resources/ontologies'\n",
    "merged_ontology_file = '/PheKnowLator_MergedOntologies.owl'\n",
    "ontology_repository = glob.glob('*/ontologies/*.owl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Clean Ontologies <a class=\"anchor\" id=\"clean-ontologies\"></a>\n",
    "***\n",
    "\n",
    "**Purpose:** In this step, we read in the ontologies using the [`owlready2`](https://pypi.org/project/Owlready2/) library and use it to indicate the presence of errors in the ontology files. We use this tool because it has strict filters. Using this tool we performed the following checks to clean the ontologies:\n",
    "\n",
    "* [Value Errors](#value-error)  \n",
    "* [Punning Errors](#punning-error)  \n",
    "* [Class Identifier Check](#identifier-check)  \n",
    "* [Obsolete/Deprecated Classes](#obsolete-classes)  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Value Errors <a class=\"anchor\" id=\"value-error\"></a>\n",
    "***\n",
    "\n",
    "This check utilizes the [`owlready2`](https://pypi.org/project/Owlready2/) library to read in each of the ontologies. This library is strict and will catch a wide variety of value errors. \n",
    "\n",
    "#### Findings  \n",
    "Of the 11 ontologies utilized in the PheKnowLator `v2.0` build, only 1 had a value error. The [Cell Line Ontology](http://www.clo-ontology.org/) yield the following error message:\n",
    "\n",
    "```python\n",
    "ValueError: invalid literal for int() with base 10: '永生的乳腺衍生细胞系细胞'\n",
    "...\n",
    "OwlReadyOntologyParsingError: RDF/XML parsing error in file ./resources/knowledge_graphs/PheKnowLator_MergedOntologies.owl, line 2363344, column 99.\n",
    "```\n",
    "\n",
    "**Interpretation:** This tells us that we need to repair the triple containing the Literal '永生的乳腺衍生细胞系细胞' by removing it and redefining it as a `string`, rather than an `int` as it is currently defined as. \n",
    "\n",
    "**Solution:** Retype the edge correctly. This is currently noted as an issue in the [Cell Line Ontology's](http://www.clo-ontology.org/) GitHub repo ([issue #48](https://github.com/CLO-ontology/CLO/issues/48)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: resources/ontologies/chebi_lite_with_imports.owl\n",
      "Loading: resources/ontologies/clo_with_imports.owl\n",
      "Loading: resources/ontologies/ext_with_imports.owl\n",
      "Loading: resources/ontologies/go_with_imports.owl\n",
      "Loading: resources/ontologies/hp_with_imports.owl\n",
      "Loading: resources/ontologies/human_pro_closed.owl\n",
      "Loading: resources/ontologies/mondo_with_imports.owl\n",
      "Loading: resources/ontologies/pw_with_imports.owl\n",
      "Loading: resources/ontologies/ro_with_imports.owl\n",
      "Loading: resources/ontologies/so_with_imports.owl\n",
      "Loading: resources/ontologies/vo_with_imports.owl\n"
     ]
    }
   ],
   "source": [
    "for ont in ontology_repository:\n",
    "    print('Loading: {}'.format(ont))\n",
    "    load_onto = get_ontology(ont).load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value Error Repairs**  \n",
    "Code to fix the Cell Ontology *Value Error* is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n"
     ]
    }
   ],
   "source": [
    "# load the graph\n",
    "graph = Graph().parse(write_location + '/clo_with_imports.owl')\n",
    "\n",
    "# fix the error\n",
    "for edge in graph:\n",
    "    if '永生的乳腺衍生细胞系细胞' in str(edge[0]) or '永生的乳腺衍生细胞系细胞' in str(edge[2]):\n",
    "        \n",
    "        # repair broken triple\n",
    "        graph.add((edge[0], edge[1], Literal(str(edge[2]), datatype=URIRef('http://www.w3.org/2001/XMLSchema#string'))))\n",
    "        graph.remove(edge)\n",
    "        break\n",
    "\n",
    "# save cleaned up ontology\n",
    "graph.serialize(destination=write_location + '/clo_with_imports.owl', format='xml')\n",
    "ontology_file_formatter(write_location, '/clo_with_imports.owl', './pkt_kg/libs/owltools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try reading in the cleaned ontology again -- there should be no errors this time!\n",
    "clo_onto = get_ontology(write_location + '/clo_with_imports.owl').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Punning Errors <a class=\"anchor\" id=\"punning-error\"></a>\n",
    "***\n",
    "\n",
    "[Punning](https://www.w3.org/2007/OWL/wiki/Punning) or redeclaration errors occur for a few different reasons, but the primary or most prevalent cause observed in the ontologies used in `PheKnowLator` is due to an `owl:ObjectProperty` being incorrectly redeclared as an `owl:AnnotationProperty` or an `owl:Class` also being defined as an `OWL:ObjectProeprty`. To detect these types of errors we currently rely on [OWLTools](https://github.com/owlcollab/owltools) and the [`owlready2`](https://pypi.org/project/Owlready2/) Python library. The easiest way to check for these types of errors is to merge the ontologies together, during which, an error message will be generated should any errors be found.\n",
    "\n",
    "#### Findings  \n",
    "The [Cell Line Ontology](http://www.clo-ontology.org/) had 7 object properties that were illegally redeclared and triggered punning errors. More details regarding these errors are shown below. \n",
    "\n",
    "```bash\n",
    "2020-12-03 20:57:15,616 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002091 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002091>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002091>))]\n",
    "2020-12-03 20:57:15,619 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/BFO_0000062 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/BFO_0000062>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/BFO_0000062>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/BFO_0000063 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/BFO_0000063>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/BFO_0000063>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002222 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002222>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002222>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0000087 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0000087>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0000087>))]\n",
    "2020-12-03 20:57:15,620 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002161 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002161>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002161>))]\n",
    "```\n",
    "\n",
    "From this message, we can see that we need to remove the following `owl:ObjectProperty` redeclared to `owl:AnnotationProperty`: `RO_0002091`, `BFO_0000062`, `BFO_0000063`, `RO_0002222`, `RO_0000087`, `RO_0002161`. There were also 2 classes (i.e. `CLO_0054407` and `CLO_0054409`) defined as being a `owl:Class` and an `owl:ObjectProperty`.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Solution:** Consistent with the solution described [here](https://github.com/oborel/obo-relations/issues/130), we removed all `owl:AnnotationProperty` declarations. For the class defined as a class and object property, we removed the class annotation. This is currently noted as an issue in the Cell Line Ontology's GitHub repo [issue #43](https://github.com/CLO-ontology/CLO/issues/43))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Processing Ontology: resources/ontologies/chebi_lite_with_imports.owl\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "\n",
      "\n",
      "Processing Ontology: resources/ontologies/clo_with_imports.owl\n",
      "Punning Error: http://purl.obolibrary.org/obo/CLO_0054407 defined as an owl:Class and owl:ObjectProperty\n",
      "Punning Error: http://purl.obolibrary.org/obo/BFO_0000062 defined as an owl:ObjectProperty and owl:AnnotationProperty\n",
      "Punning Error: http://purl.obolibrary.org/obo/CLO_0054409 defined as an owl:Class and owl:ObjectProperty\n",
      "Punning Error: http://purl.obolibrary.org/obo/BFO_0000063 defined as an owl:ObjectProperty and owl:AnnotationProperty\n",
      "Punning Error: http://purl.obolibrary.org/obo/RO_0002161 defined as an owl:ObjectProperty and owl:AnnotationProperty\n",
      "Punning Error: http://purl.obolibrary.org/obo/RO_0002222 defined as an owl:ObjectProperty and owl:AnnotationProperty\n",
      "Punning Error: http://purl.obolibrary.org/obo/RO_0000087 defined as an owl:ObjectProperty and owl:AnnotationProperty\n",
      "Punning Error: http://purl.obolibrary.org/obo/RO_0002091 defined as an owl:ObjectProperty and owl:AnnotationProperty\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "\n",
      "\n",
      "Processing Ontology: resources/ontologies/ext_with_imports.owl\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "\n",
      "\n",
      "Processing Ontology: resources/ontologies/go_with_imports.owl\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "\n",
      "\n",
      "Processing Ontology: resources/ontologies/hp_with_imports.owl\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "\n",
      "\n",
      "Processing Ontology: resources/ontologies/human_pro_closed.owl\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "\n",
      "\n",
      "Processing Ontology: resources/ontologies/mondo_with_imports.owl\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "\n",
      "\n",
      "Processing Ontology: resources/ontologies/pw_with_imports.owl\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "\n",
      "\n",
      "Processing Ontology: resources/ontologies/ro_with_imports.owl\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "\n",
      "\n",
      "Processing Ontology: resources/ontologies/so_with_imports.owl\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "\n",
      "\n",
      "Processing Ontology: resources/ontologies/vo_with_imports.owl\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n"
     ]
    }
   ],
   "source": [
    "for ont in ontology_repository:\n",
    "    print('\\n\\n\\nProcessing Ontology: {}'.format(ont))\n",
    "    graph, bad_classes = Graph().parse(ont), set()\n",
    "    \n",
    "    for s, p, o in graph:\n",
    "        triples = list(graph.triples((s, None, None)))\n",
    "        \n",
    "        # check for objects defined as classes and object properties\n",
    "        class_prop, obj_prop = (s, RDF.type, OWL.Class), (s, RDF.type, OWL.ObjectProperty)\n",
    "        if (class_prop in triples and obj_prop in triples) and str(s) not in bad_classes:\n",
    "            bad_classes.add(str(s))\n",
    "            print('Punning Error: {} defined as an owl:Class and owl:ObjectProperty'.format(str(s)))\n",
    "            graph.remove(class_prop)\n",
    "    \n",
    "        # check for objects defined as object properties and annotation properties\n",
    "        if o == URIRef('http://www.w3.org/2002/07/owl#ObjectProperty'):\n",
    "            obj_prop, annot_prop = (s, RDF.type, OWL.ObjectProperty), (s, RDF.type, OWL.AnnotationProperty)\n",
    "            if obj_prop in triples and annot_prop in triples:\n",
    "                print('Punning Error: {} defined as an owl:ObjectProperty and owl:AnnotationProperty'.format(str(s)))\n",
    "                graph.remove(annot_prop)\n",
    "    \n",
    "    # save cleaned up ontology\n",
    "    graph.serialize(destination=ont, format='xml') \n",
    "    ontology_file_formatter(write_location, '/' + ont.split('/')[-1], './pkt_kg/libs/owltools')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Class Identifier Check  <a class=\"anchor\" id=\"identifier-check\"></a>\n",
    "***\n",
    "\n",
    "Check class identifiers to ensure consistency in identifier prefixes. For example, we want to identifiers that are incorrectly formatted like occurrences of `PRO_XXXXXXX` which should be `PR_XXXXXXX`.\n",
    "\n",
    "#### Findings  \n",
    "Running this check revealed mislabeling of `18` [pROtein Ontology](https://proconsortium.org/) identifiers in the [Vaccine Ontology](http://www.violinet.org/vaccineontology/) (see [this](https://github.com/vaccineontology/VO/issues/4) GitHub issue).\n",
    "\n",
    "\n",
    "**Solution:** Reformat the incorrectly formatted class identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing Ontology: chebi\n",
      "Unique Identifier Types: CHEBI\n",
      "\n",
      "\n",
      "Processing Ontology: clo\n",
      "Unique Identifier Types: BFO, CARO, CHEBI, CL, CLO, CP, DOID, GO, HGNC, IAO, NCBITaxon, NCIT, OBI, OGG, PATO, PR, SO, UBERON, VO\n",
      "\n",
      "\n",
      "Processing Ontology: ext\n",
      "Unique Identifier Types: BFO, BSPOTEMP, CARO, CHEBI, CL, CP, D96882F1-8709-49AB-BCA9-772A67EA6C33, EHDAA2, EMAPA, ENVO, FBbt, FMA, GO, NBO, NCBITaxon, PATO, PR, RO, RnorDv, UBERON, UBERON#, UBERONTEMP, ZFA\n",
      "\n",
      "\n",
      "Processing Ontology: go\n",
      "Unique Identifier Types: GO\n",
      "\n",
      "\n",
      "Processing Ontology: hp\n",
      "Unique Identifier Types: BFO, CARO, CHEBI, CL, CP, ENVO, GO, HP, HsapDv, MOD, MPATH, NBO, NCBITaxon, OBI, PATO, PR, RO, SO, UBERON\n",
      "\n",
      "\n",
      "Processing Ontology: mondo\n",
      "Unique Identifier Types: BFO, CARO, CHEBI, CL, CP, ECTO, ENVO, ExO, FOODON, GO, HP, IAO, MF, MFOEM, MFOMD, MONDO, NBO, NCBITaxon, NCIT, OBA, OBI, OGMS, PATO, PCO, PO, PR, RO, SO, UBERON, UMLS, UPHENO\n",
      "\n",
      "\n",
      "Processing Ontology: pro\n",
      "Unique Identifier Types: BFO, CHEBI, CL, GO, MOD, NCBITaxon, OBI, PR, SO\n",
      "\n",
      "\n",
      "Processing Ontology: pw\n",
      "Unique Identifier Types: OMIM, PW\n",
      "\n",
      "\n",
      "Processing Ontology: ro\n",
      "Unique Identifier Types: BFO, CHEBI, CL, GO, MOD, NCBITaxon, OBI, PR, SO\n",
      "\n",
      "\n",
      "Processing Ontology: so\n",
      "Unique Identifier Types: CHEBI\n",
      "\n",
      "\n",
      "Processing Ontology: vo\n",
      "Unique Identifier Types: BFO, CHEBI, DOID, FMA, GO, IAO, IDO, NCBITaxon, OAE, OBI, OGG, OGMS, PATO, PRO, UBERON, UO, VO\n"
     ]
    }
   ],
   "source": [
    "for ont in ontologies:\n",
    "    print('\\n\\nProcessing Ontology: {}'.format(ont))\n",
    "    ont_data = [x for x in ontology_repository if ont.lower() in x][0]\n",
    "    graph = Graph().parse(ont_data)\n",
    "    \n",
    "    # get classes\n",
    "    kg_classes = set([x for x in graph.subjects(RDF.type, OWL.Class)])\n",
    "    \n",
    "    # convert results to list of classes and only keep hgnc identifiers\n",
    "    class_list = [res for res in kg_classes if isinstance(res, URIRef) and 'obo/' in str(res)]\n",
    "\n",
    "    # print unique identifier types for all classes in each ontology\n",
    "    print('Unique Identifier Types: {}'.format(', '.join(sorted(set([x.split('/')[-1].split('_')[0] for x in class_list])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following classes were updated:\n",
      "http://purl.obolibrary.org/obo/PRO_000000001\n",
      "http://purl.obolibrary.org/obo/PRO_000015399\n"
     ]
    }
   ],
   "source": [
    "# replace badly formatted identifiers with the correct ones\n",
    "graph = Graph().parse('resources/ontologies/vo_with_imports.owl')\n",
    "bad_classes = set()\n",
    "\n",
    "for edge in graph:\n",
    "    if 'http://purl.obolibrary.org/obo/PRO_' in str(edge[0]):\n",
    "        updated_subj = str(edge[0]).replace('http://purl.obolibrary.org/obo/PRO_', 'http://purl.obolibrary.org/obo/PR')\n",
    "        graph.add((URIRef(updated_subj), edge[1], edge[2]))\n",
    "        graph.remove(edge)\n",
    "        bad_classes.add(str(edge[0]))\n",
    "    if 'http://purl.obolibrary.org/obo/PRO_' in str(edge[2]):\n",
    "        updated_obj = str(edge[0]).replace('http://purl.obolibrary.org/obo/PRO_', 'http://purl.obolibrary.org/obo/PR')\n",
    "        graph.add((edge[0], edge[1], URIRef(updated_obj)))\n",
    "        graph.remove(edge)\n",
    "        bad_classes.add(str(edge[2]))\n",
    "\n",
    "print('The following classes were updated:\\n{}'.format('\\n'.join(bad_classes)))\n",
    "\n",
    "# save cleaned up ontology\n",
    "graph.serialize(destination='resources/ontologies/vo_with_imports.owl', format='xml') \n",
    "ontology_file_formatter(write_location, '/' + 'vo_with_imports.owl', './pkt_kg/libs/owltools')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Remove Obsolete and/or Deprecated Classes    <a class=\"anchor\" id=\"obsolete-classes\"></a>\n",
    "***\n",
    "\n",
    "To make sure that the ontology only contains current information, all obsolete classes and any triples that they participate in are removed from the ontologies. For build `V2.0`, we removed the following number of entities related to or containing a `deprecated` or `obsolete` ontology class:    \n",
    "\n",
    "Ontology | Obsolete | Deprecated\n",
    ":--: | :--: | :--:\n",
    "CheBI Lite | 14 | 18498   \n",
    "CLO | 14 | 2    \n",
    "MONDO | 1945 | 2196    \n",
    "GO | 3121 | 6302    \n",
    "HPO | 297 | 348   \n",
    "PW | 6 | 42   \n",
    "PRO - Human | 0 | 0   \n",
    "SO | 0 | 0   \n",
    "UBERON | 1062 | 1564    \n",
    "VO | 10 | 0   \n",
    "RO | 0 | 0   \n",
    "\n",
    "\n",
    "_NOTE._ In addition to running the code below, it may also be necessary to check for classes that are a sub-class of `oboInOwl:ObsoleteClass`, as well as any obsolete or deprecated annotations, individuals, or `owl:ObjectProperty`. Finally, we recommend verifying each ontology using an ontology debugger (i.e. by running a reasoner) to ensure that your changes and edits have not introduced unexpected errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading: chebi\n",
      "Removed 14 obsolete classes and 18498 deprecated classes\n",
      "\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "Loading: clo\n",
      "Removed 14 obsolete classes and 2 deprecated classes\n",
      "\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "Loading: ext\n",
      "Removed 1062 obsolete classes and 1564 deprecated classes\n",
      "\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "None\n",
      "\n",
      "Loading: go\n",
      "Removed 3121 obsolete classes and 6302 deprecated classes\n",
      "\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "Loading: hp\n",
      "Removed 297 obsolete classes and 348 deprecated classes\n",
      "\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "Loading: mondo\n",
      "Removed 1945 obsolete classes and 2196 deprecated classes\n",
      "\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "None\n",
      "\n",
      "Loading: pro\n",
      "Removed 0 obsolete classes and 0 deprecated classes\n",
      "\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "Loading: pw\n",
      "Removed 6 obsolete classes and 42 deprecated classes\n",
      "\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "Loading: ro\n",
      "Removed 0 obsolete classes and 0 deprecated classes\n",
      "\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "Loading: so\n",
      "Removed 0 obsolete classes and 0 deprecated classes\n",
      "\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n",
      "\n",
      "Loading: vo\n",
      "Removed 10 obsolete classes and 0 deprecated classes\n",
      "\n",
      "\n",
      "*** Applying OWL API Formatting to Knowledge Graph OWL File ***\n"
     ]
    }
   ],
   "source": [
    "# remove triples containing deprecated classes\n",
    "for ont in ontologies:    \n",
    "    print('\\nLoading: {}'.format(ont))\n",
    "    ont_switch = ont if ont != 'ext' else 'uberon'\n",
    "    ont_prefix = 'http://purl.obolibrary.org/obo/' + ont_switch.upper()\n",
    "    ont_data = [x for x in ontology_repository if ont.lower() in x][0]\n",
    "    graph = Graph().parse(ont_data)\n",
    "\n",
    "    # get deprecated classes and triples\n",
    "    dep_cls = [x[0] for x in list(graph.triples((None, OWL.deprecated, Literal('true', datatype=URIRef('http://www.w3.org/2001/XMLSchema#boolean')))))]\n",
    "    dep_triples = [(s, p, o) for s, p, o in graph if 'deprecated' in ', '.join([str(s).lower(), str(p).lower(), str(o).lower()]) and str(s).startswith(ont_prefix)]\n",
    "    oth_dep_triple_classes = [x[0] for x in dep_triples]\n",
    "    deprecated_classes = set(dep_cls + oth_dep_triple_classes)\n",
    "    # get obsolete classes and triples\n",
    "    obs_cls = [x[0] for x in list(graph.triples((None, RDFS.subClassOf, URIRef('http://www.geneontology.org/formats/oboInOwl#ObsoleteClass'))))]\n",
    "    obs_triples = [(s, p, o) for s, p, o in graph if 'obsolete' in ', '.join([str(s).lower(), str(p).lower(), str(o).lower()]) and str(s).startswith(ont_prefix)]\n",
    "    oth_obs_triple_classes = [x[0] for x in obs_triples]\n",
    "    obsolete_classes = set(obs_cls + oth_obs_triple_classes)\n",
    "    \n",
    "    # remove deprecated/obsolete classes\n",
    "    for node in list(deprecated_classes) + list(obsolete_classes):        \n",
    "        graph.remove((node, None, None))  # remove all triples about node\n",
    "        graph.remove((None, None, node))  # remove all triples pointing to node\n",
    "        \n",
    "    # remove deprecated/obsolete triples\n",
    "    for triple in dep_triples + obs_triples:        \n",
    "        graph.remove(triple)\n",
    "\n",
    "    print('Removed {} obsolete classes and {} deprecated classes\\n'.format(len(obsolete_classes), len(deprecated_classes)))\n",
    "    \n",
    "    # serialize graph\n",
    "    graph.serialize(destination=ont_data, format='xml')\n",
    "    ontology_file_formatter(write_location, '/' + ont_data.split('/')[-1], './pkt_kg/libs/owltools')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "## MERGE ONTOLOGIES <a class=\"anchor\" id=\"merge-ontologies\"></a>\n",
    "***\n",
    "\n",
    "**Purpose:** In this step, the [`OWLTools`](https://github.com/owlcollab/owltools) library is designed to merge a directory of ontology files into a single ontology file. This merged ontology file is required as input to the knowledge graph build algorithm.  \n",
    "\n",
    "**Inputs:** A directory of ontology files (`.owl`)  \n",
    "**Outputs:** [`PheKnowLator_MergedOntologies.owl`](https://www.dropbox.com/s/1lhh4hdwbjzds74/PheKnowLator_MergedOntologiesGeneID_Normalized_Cleaned.owl?dl=1)\n",
    "\n",
    "#### Findings\n",
    "\n",
    "The merge set of ontologies contained `366,828` classes, `3,841,408` axioms `818` object properties, and `151` individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging Ontologies: vo_with_imports.owl, so_with_imports.owl\n",
      "\n",
      "Merging Ontologies: ro_with_imports.owl, PheKnowLator_MergedOntologies.owl\n",
      "\n",
      "Merging Ontologies: pw_with_imports.owl, PheKnowLator_MergedOntologies.owl\n",
      "\n",
      "Merging Ontologies: mondo_with_imports.owl, PheKnowLator_MergedOntologies.owl\n",
      "None\n",
      "\n",
      "Merging Ontologies: human_pro_closed.owl, PheKnowLator_MergedOntologies.owl\n",
      "\n",
      "Merging Ontologies: hp_with_imports.owl, PheKnowLator_MergedOntologies.owl\n",
      "\n",
      "Merging Ontologies: go_with_imports.owl, PheKnowLator_MergedOntologies.owl\n",
      "\n",
      "Merging Ontologies: ext_with_imports.owl, PheKnowLator_MergedOntologies.owl\n",
      "None\n",
      "\n",
      "Merging Ontologies: clo_with_imports.owl, PheKnowLator_MergedOntologies.owl\n",
      "\n",
      "Merging Ontologies: chebi_lite_with_imports.owl, PheKnowLator_MergedOntologies.owl\n",
      "\n",
      "The knowledge graph contains 366828 classes, 3841408 axioms, 818 object properties, and 151 individuals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge ontologies\n",
    "if write_location + merged_ontology_file in glob.glob(write_location + '/*.owl'):\n",
    "    graph = Graph().parse(write_location + merged_ontology_file)\n",
    "    gets_ontology_statistics(write_location + merged_ontology_file)\n",
    "else:\n",
    "    merges_ontologies(ontology_repository, write_location, merged_ontology_file)\n",
    "    gets_ontology_statistics(write_location + merged_ontology_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use owlready2 to read in merged ontology file\n",
    "merged_onts = get_ontology(write_location + merged_ontology_file).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "## Normalize Classes <a class=\"anchor\" id=\"normalize-classes\"></a>\n",
    "***\n",
    "\n",
    "**Purpose:** The goal of this section is to checked the cleaned merged ontology file to ensure that there is consistency between the existing classes. To do this, we check two things: (1) [Aligning Existing Ontology Classes](#aligning-existing-ontologies); and (2) [Aligning Ontology Classes and New Edge Data](#aligning-new-data). More details for each type of checkare provided below.\n",
    "  \n",
    "\n",
    "**Dependencies:** The Merged Gene, RNA, Protein Map ([`Merged_gene_rna_protein_identifiers.pkl`](https://www.dropbox.com/s/6idnt7b3i322hlh/Merged_gene_rna_protein_identifiers.pkl?dl=1)) we generated in order to map genomic identifier data sources.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning Existing Ontology Classes <a class=\"anchor\" id=\"aligning-existing-ontologies\"></a>\n",
    "***\n",
    "\n",
    "**Purpose:** For this check, we want to make sure that all classes that represent the same entity are connected to each other. For example, consider the following:  \n",
    "- Ontologies: [Sequence Ontology](http://www.sequenceontology.org/), [ChEBI](https://www.ebi.ac.uk/chebi), and [PRotein Ontology](https://proconsortium.org/) all include terms for protein, but none of these classes are connected to each other. \n",
    "\n",
    "#### Findings  \n",
    "*Normalize Duplicate Ontology Concepts*  \n",
    "The follow classes occur in all of the ontologies used in the current build and have to be normalizesd so that there are not multiple versions of the same concept:  \n",
    "\n",
    "- Gene: [VO](http://purl.obolibrary.org/obo/OGG_0000000002)  \n",
    "  - <u>Solution</u>: Make the `VO` imported `OGG` class a subclass of the `SO` gene term  \n",
    "\n",
    "- Protein: [SO](http://purl.obolibrary.org/obo/SO_0000104), [PRO](http://purl.obolibrary.org/obo/PR_000000001), [ChEBI](http://purl.obolibrary.org/obo/CHEBI_36080) \n",
    "  - <u>Solution</u>: Make the `CHEBI` and `PRO` classes a subclass of the `SO` protein term  \n",
    "  \n",
    "- Disorder: [VO](http://purl.obolibrary.org/obo/OGMS_0000045)  \n",
    "  - <u>Solution</u>: Make the `VO` imported `OGMS` class a subclass of the `MONDO` disease term  \n",
    "\n",
    "- Antigen: [VO](http://purl.obolibrary.org/obo/OBI_1110034)  \n",
    "  - <u>Solution</u>: Make the `VO` imported OBI class a subclass of the `CHEBI` antigen term  \n",
    "\n",
    "- Gelatin: [VO]('http://purl.obolibrary.org/obo/VO_0003030') \n",
    "  - <u>Solution</u>: Make the `VO` class a subclass of the `CHEBI` gelatin term \n",
    "\n",
    "- Hormone: [VO](http://purl.obolibrary.org/obo/FMA_12278) \n",
    "  - <u>Solution</u>: Make the `VO` imported `FMA` class a subclass of the `CHEBI` hormone term\n",
    "\n",
    "*Normalize Existing Ontology Classes*  \n",
    "`19,820` `HGNC` identifiers were found and need to be converted to `Entrez` gene identifiers, which is the identifier type for genes utilized by the majority of ontologies.\n",
    "\n",
    "\n",
    "**Solution:**   \n",
    "*Normalize Duplicate Ontology Concepts*  \n",
    "Choose a primary concept for all duplicate scenarios and make duplicate concepts an `RDFS:subClassOf` the primary concept.\n",
    "\n",
    "*Normalize Existing Ontology Classes*   \n",
    "Convert all classes that reference an HGNC gene (`n=19,820`) to an Entrez identifier. To do this, we will utilize the genomic identifier mapping information ([`Merged_gene_rna_protein_identifiers.pkl`](https://www.dropbox.com/s/6idnt7b3i322hlh/Merged_gene_rna_protein_identifiers.pkl?dl=1)) we constructed in the [`Data_Preparation.ipynb`](https://github.com/callahantiff/PheKnowLator/blob/master/Data_Preparation.ipynb) Jupyter notebook. Note that we are only updating identifiers and not verifying labels or other metadata.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Load Merged Ontology Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in merged data\n",
    "merged_onts = Graph().parse(write_location + merged_ontology_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize Duplicate Ontology Concepts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix gene class inconsistencies\n",
    "graph.add((URIRef('http://purl.obolibrary.org/obo/OGG_0000000002'),\n",
    "           URIRef('http://www.w3.org/2000/01/rdf-schema#subClassOf'),\n",
    "           URIRef('http://purl.obolibrary.org/obo/SO_0000704')))\n",
    "\n",
    "# fix protein class inconsistencies\n",
    "graph.add((URIRef('http://purl.obolibrary.org/obo/PR_000000001'),\n",
    "           URIRef('http://www.w3.org/2000/01/rdf-schema#subClassOf'),\n",
    "           URIRef('http://purl.obolibrary.org/obo/SO_0000104')))\n",
    "\n",
    "graph.add((URIRef('http://purl.obolibrary.org/obo/CHEBI_36080'),\n",
    "           URIRef('http://www.w3.org/2000/01/rdf-schema#subClassOf'),\n",
    "           URIRef('http://purl.obolibrary.org/obo/SO_0000104')))\n",
    "\n",
    "# fix disorder class inconsistencies\n",
    "graph.add((URIRef('http://purl.obolibrary.org/obo/OGMS_0000045'),\n",
    "           URIRef('http://www.w3.org/2000/01/rdf-schema#subClassOf'),\n",
    "           URIRef('http://purl.obolibrary.org/obo/DOID_4')))\n",
    "\n",
    "# fix antigen class inconsistencies\n",
    "graph.add((URIRef('http://purl.obolibrary.org/obo/OBI_1110034'),\n",
    "           URIRef('http://www.w3.org/2000/01/rdf-schema#subClassOf'),\n",
    "           URIRef('http://purl.obolibrary.org/obo/CHEBI_59132')))\n",
    "\n",
    "# fix gelatin class inconsistencies\n",
    "graph.add((URIRef('http://purl.obolibrary.org/obo/VO_0003030'),\n",
    "           URIRef('http://www.w3.org/2000/01/rdf-schema#subClassOf'),\n",
    "           URIRef('http://purl.obolibrary.org/obo/CHEBI_5291')))\n",
    "\n",
    "# fix hormone class inconsistencies\n",
    "graph.add((URIRef('http://purl.obolibrary.org/obo/FMA_12278'),\n",
    "           URIRef('http://www.w3.org/2000/01/rdf-schema#subClassOf'),\n",
    "           URIRef('http://purl.obolibrary.org/obo/CHEBI_24621')))\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize Existing Ontology Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all classes in graph\n",
    "kg_classes = merged_onts.query(\n",
    "    \"\"\"SELECT DISTINCT ?c\n",
    "           WHERE {?c rdf:type owl:Class . }\n",
    "           \"\"\", initNs={'rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "                        'owl': 'http://www.w3.org/2002/07/owl#'}\n",
    ")\n",
    "\n",
    "# convert results to list of classes and only keep hgnc identifiers\n",
    "class_list_gene = [res[0] for res in kg_classes if isinstance(res[0], URIRef) and 'hgnc' in str(res[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load genomic identifier mapping dictionary\n",
    "genomic_id_map = pickle.load(open('resources/processed_data/Merged_gene_rna_protein_identifiers.pkl', 'rb'), encoding='bytes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each gene class and get entrez gene id equivalent\n",
    "matches, not_matched = {}, []\n",
    "gene_url = 'https://www.ncbi.nlm.nih.gov/gene/'\n",
    "\n",
    "for gene_class in class_list_gene:\n",
    "    key = 'hgnc_id_' + str(gene_class).split('=')[-1]\n",
    "    \n",
    "    if key in genomic_id_map.keys() and any(x for x in genomic_id_map[key] if x.startswith('entrez_id')):        \n",
    "        matches[str(gene_class)] = [gene_url + x.split('_')[-1] for x in genomic_id_map[key] if 'entrez_id' in x]\n",
    "    else:\n",
    "        not_matched.append(gene_class)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print non-matching gene uris\n",
    "not_matched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Investigate UnMatched Genes**  \n",
    "Only 3 of the HGNC genes were not found in our dictionary ([`HGNC:24033`](http://www.genenames.org/cgi-bin/gene_symbol_report?hgnc_id=24033), [`HGNC:31447`](http://www.genenames.org/cgi-bin/gene_symbol_report?hgnc_id=31447), [`HGNC:33870`](http://www.genenames.org/cgi-bin/gene_symbol_report?hgnc_id=33870)). Investigating these revealed that HGNC made these identifiers obsolete and replaced them with new identifiers. Until this term is updated in the PRO ontology, we have to manually make the same fix. \n",
    "\n",
    "Additionally, there were 8 HGNC ids that have all been withdrawn (i.e. [`HGNC:26619`](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/26619), [`HGNC:13392`](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/13392), [`HGNC:31424`](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/31424), [`HGNC:8103`](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/8103), \n",
    "[`HGNC:25943`](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/25943), [`HGNC:16957`](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/16957), [`HGNC:23418`](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/23418), [`HGNC:32021`](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/32021)) and for our purposes, will be removed. _Note_. Please verify the output file to ensure that no errors were added when removing these genes (as mentioned in a prior cell, these `8` genes were each deleted from the PRO ontology prior to merging).\n",
    "\n",
    "This issue has been reported to [PRotein Ontology](https://proconsortium.org/pro.shtml) (see [this](https://github.com/PROconsortium/PRoteinOntology/issues/176) GitHub issue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate HGNC genes with no mappings to Entrez\n",
    "not_matched\n",
    "\n",
    "# update mapping dictionary\n",
    "gene_url = 'https://www.ncbi.nlm.nih.gov/gene/'\n",
    "matches['http://www.genenames.org/cgi-bin/gene_symbol_report?hgnc_id=24033'] =  [gene_url + x.split('_')[-1] for x in genomic_id_map['hgnc_id_26545'] if 'entrez_id' in x]\n",
    "matches['http://www.genenames.org/cgi-bin/gene_symbol_report?hgnc_id=31447'] =  [gene_url + x.split('_')[-1] for x in genomic_id_map['hgnc_id_16932'] if 'entrez_id' in x]\n",
    "matches['http://www.genenames.org/cgi-bin/gene_symbol_report?hgnc_id=33870'] =  [gene_url + x.split('_')[-1] for x in genomic_id_map['hgnc_id_20667'] if 'entrez_id' in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated gene identifiers in graph\n",
    "for edge in graph:\n",
    "    if str(edge[0]) in matches.keys():\n",
    "        for mapped_id in matches[str(edge[0])]:\n",
    "            graph.add((URIRef(mapped_id), edge[1], edge[2]))\n",
    "            graph.remove(edge)\n",
    "    elif str(edge[2]) in matches.keys():\n",
    "        for mapped_id in matches[str(edge[2])]:\n",
    "            graph.add((edge[0], edge[1], URIRef(mapped_id)))\n",
    "            graph.remove(edge)\n",
    "    else:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning Existing Ontology Classes and New Edge Data<a class=\"anchor\" id=\"aligning-new-data\"></a>\n",
    "***\n",
    "\n",
    "**Purpose:** For this check, we want to make sure that any of the existing ontology classes can be aligned with any of the new data entities that we want to add to the knowledge graph. For example:  \n",
    "  - Gene Classes: there are several gene classes that use [HGNC](https://www.genenames.org/) identifiers. We also want to add genes, but prefer to use [Entrez gene](https://www.ncbi.nlm.nih.gov/gene) identifiers. In order to be used with our data, we must first normalize all of the HPO gene classes to Entrez gene identifiers.\n",
    "\n",
    "#### Findings  \n",
    "From looking into the knowledge graph we identified `20` classes that existed in the new edge list and in the merged ontologies, but that had differing URIs (`http://www.ncbi.nlm.nih.gov/` vs. `https://www.ncbi.nlm.nih.gov/`). Those classes were:  \n",
    "- `http://www.ncbi.nlm.nih.gov/gene/100129307`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/100131107`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/101927789`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/102723383`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/105373297`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/107987235`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/140606`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/157285`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/163404`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/390928`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/392490`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/50810`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/51714`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/54886`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/58515`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/64748`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/79948`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/83642`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/84717`\n",
    "- `http://www.ncbi.nlm.nih.gov/gene/9890`\n",
    "\n",
    "**Solution:** Replace the occurrences of `HTTP` URLs with `HTTPS`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in Master Edge List**  \n",
    "The [`master edge list`](https://www.dropbox.com/s/t8sgzd847t1rof4/Master_Edge_List_Dict.json?dl=1) that is created in Step 2 of the `pkt_kg` algorithm is read in and processed into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in master edge list\n",
    "edge_data = json.load(open('./resources/Master_Edge_List_Dict.json', 'r'))\n",
    "\n",
    "# convert to dictionary\n",
    "edge_dict = dict()\n",
    "\n",
    "# iterate over master edges to \n",
    "for k in edge_data:\n",
    "    rel = edge_data[k]['uri']\n",
    "    for edge in edge_data[k]['edge_list']:\n",
    "        for x in edge:\n",
    "            if x in edge_dict.keys():\n",
    "                edge_dict[x] |= {rel[edge.index(x)]}\n",
    "            else:\n",
    "                edge_dict[x] = {rel[edge.index(x)]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Process merged Ontologies and Verify New Edge Relations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in graph:\n",
    "    if str(edge[0]).startswith('http://www.ncbi.nlm.nih.gov/gene/'):\n",
    "        updated_subj = str(edge[0]).replace('http://www.ncbi.nlm.nih.gov/gene/', 'https://www.ncbi.nlm.nih.gov/gene/')\n",
    "        graph.add((URIRef(updated_subj), edge[1], edge[2]))\n",
    "        graph.remove(edge)\n",
    "        \n",
    "    if str(edge[2]).startswith('http://www.ncbi.nlm.nih.gov/gene/'):\n",
    "        updated_obj = str(edge[2]).replace('http://www.ncbi.nlm.nih.gov/gene/', 'https://www.ncbi.nlm.nih.gov/gene/')\n",
    "        graph.add((edge[0], edge[1], URIRef(updated_obj)))\n",
    "        graph.remove(edge)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save normalized ontology\n",
    "graph.serialize(destination=write_location + merged_ontology_file[:-4] + 'GeneID_Normalized_Cleaned.owl', format='xml')\n",
    "\n",
    "# apply OWL API formatting to file\n",
    "ontology_file_formatter(write_location, merged_ontology_file[:-4] + 'GeneID_Normalized_Cleaned.owl')\n",
    "\n",
    "# get ontology stats\n",
    "gets_ontology_statistics(write_location + merged_ontology_file[:-4] + 'GeneID_Normalized_Cleaned.owl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
