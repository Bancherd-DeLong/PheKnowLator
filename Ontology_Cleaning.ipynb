{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# PheKnowLator - Ontology Cleaning\n",
    "***\n",
    "***\n",
    "\n",
    "**Author:** [TJCallahan](https://mail.google.com/mail/u/0/?view=cm&fs=1&tf=1&to=callahantiff@gmail.com)  \n",
    "**GitHub Repository:** [PheKnowLator](https://github.com/callahantiff/PheKnowLator/wiki)  \n",
    "**Release:** **[v2.0.0](https://github.com/callahantiff/PheKnowLator/wiki/v2.0.0)**\n",
    "  \n",
    "<br>  \n",
    "  \n",
    "**Purpose:** This notebook serves as a script to help prepare ontologies prior to be ingested into the knowledge graph build algorithm. This script focuses on preparing ontologies for digestion by performing the following steps:  \n",
    "1. [Clean Ontologies](#clean-ontologies)  \n",
    "2. [Merge Ontologies](#merge-ontologies)  \n",
    "3. [Normalize Classes](#normalize-classes)  \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Assumptions:**   \n",
    "- Directory of Imported Ontologies ➞ `./resources/ontologies`    \n",
    "- Processed data write location ➞ `./resources/ontologies`  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Dependencies:**   \n",
    "- This notebook utilizes several helper functions, which are stored in the [`kg_utils.py`](https://github.com/callahantiff/PheKnowLator/blob/master/pkt_kg/utils/kg_utils.py) script. Hyperlinks to all downloaded and generated data sources are provided on the [Data Sources](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources) Wiki page as well as within each source subsection of this notebook. All generated data is freely available for download from DropBox. \n",
    "- [`OWL Tools`](https://github.com/owlcollab/owltools)\n",
    "\n",
    "_____\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-Up Environment\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from owlready2 import *\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, Literal\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import script containing helper functions\n",
    "from pkt_kg.utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment variables\n",
    "write_location = './resources/knowledge_graphs'\n",
    "merged_ontology_file = '/PheKnowLator_MergedOntologies.owl'\n",
    "ontology_repository = glob.glob('*/ontologies/*.owl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Clean Ontologies <a class=\"anchor\" id=\"clean-ontologies\"></a>\n",
    "\n",
    "**Purpose:** In this step, we read in the ontologies using the [`owlready2`](https://pypi.org/project/Owlready2/) library and use it to indicate the presence of errors in the ontology files. We use this tool because it has strict filters.\n",
    "\n",
    "Errors were found in the following ontologies:  \n",
    "- [Vaccine Ontology](http://www.violinet.org/vaccineontology/): GitHub [issue #4](https://github.com/vaccineontology/VO/issues/4)\n",
    "- [Cell Line Ontology](http://www.clo-ontology.org/): GitHub [issue #42](https://github.com/CLO-ontology/CLO/issues/42), [issue #45](https://github.com/CLO-ontology/CLO/issues/45)  \n",
    "- [PRotein Ontology](https://proconsortium.org/pro.shtml): GitHub [issue #176](https://github.com/PROconsortium/PRoteinOntology/issues/176)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Ontology Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ont in tqdm(ontology_repository):\n",
    "    load_onto = get_ontology(ont).load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Cell Line Ontology](http://www.clo-ontology.org/) yield the following error message:\n",
    "\n",
    "```python\n",
    "\n",
    "ValueError: invalid literal for int() with base 10: '永生的乳腺衍生细胞系细胞'\n",
    "\n",
    "...\n",
    "\n",
    "OwlReadyOntologyParsingError: RDF/XML parsing error in file ./resources/knowledge_graphs/PheKnowLator_MergedOntologies.owl, line 2363344, column 99.\n",
    "```\n",
    "\n",
    "This tells us that we need to repair the triple containing the Literal '永生的乳腺衍生细胞系细胞' by removing it and redefining it as a `string`, rather than an `int` as it is currently defined as. \n",
    "\n",
    "<br>\n",
    "\n",
    "This is currently noted as an issue in the [Cell Line Ontology's](http://www.clo-ontology.org/) GitHub repo ([issue #45](https://github.com/CLO-ontology/CLO/issues/48)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in tqdm(graph):\n",
    "    if '永生的乳腺衍生细胞系细胞' in str(edge[0]) or '永生的乳腺衍生细胞系细胞' in str(edge[2]):\n",
    "        \n",
    "        # repair broken triple\n",
    "        graph.add((edge[0], edge[1], Literal(str(edge[2]), datatype=URIRef('http://www.w3.org/2001/XMLSchema#string'))))\n",
    "        graph.remove(edge)\n",
    "        break\n",
    "\n",
    "# save cleaned up ontology\n",
    "graph.serialize(destination='./resources/ontologies/clo_with_imports', format='xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try reading in the cleaned ontology again\n",
    "merged_onto = get_ontology('./resources/ontologies/clo_with_imports').load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next errors that are generated are related to punning, specifically that the following OWL object properties had been incorrectly redeclared as OWL annotation properties:\n",
    "\n",
    "```bash\n",
    "2020-03-24 16:48:25,458 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002091 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002091>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002091>))]\n",
    "2020-03-24 16:48:25,460 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/BFO_0000062 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/BFO_0000062>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/BFO_0000062>))]\n",
    "2020-03-24 16:48:25,460 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/BFO_0000063 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/BFO_0000063>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/BFO_0000063>))]\n",
    "2020-03-24 16:48:25,460 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002222 in punning not allowed [Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002222>)), Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002222>))]\n",
    "2020-03-24 16:48:25,460 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0000087 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0000087>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0000087>))]\n",
    "2020-03-24 16:48:25,460 ERROR (OWLOntologyManagerImpl:1138) Illegal redeclarations of entities: reuse of entity http://purl.obolibrary.org/obo/RO_0002161 in punning not allowed [Declaration(ObjectProperty(<http://purl.obolibrary.org/obo/RO_0002161>)), Declaration(AnnotationProperty(<http://purl.obolibrary.org/obo/RO_0002161>))]\n",
    "```\n",
    "\n",
    "From this message, we can see that we need to remove the following redeclaration to Annotation Properties for the following object properties:  \n",
    "- RO_0002091  \n",
    "- BFO_0000062  \n",
    "- BFO_0000063  \n",
    "- RO_0002222  \n",
    "- RO_0000087  \n",
    "- RO_0002161  \n",
    "\n",
    "<br>\n",
    "\n",
    "This is another error caused by the [Cell Line Ontology](http://www.clo-ontology.org/) and has been posted to GitHub ([issue #42](https://github.com/CLO-ontology/CLO/issues/42)).\n",
    "\n",
    "We also removed RO_0002161 (the Annotation Property) from GO, UBERON, and HPO.\n",
    "\n",
    "<br>\n",
    "\n",
    "Consistent with the solution described [here](https://github.com/oborel/obo-relations/issues/130), we removed all `AnnotationProperty` declarations from the merged ontology file. The Annotation Properties for each of the Object Properties listed above were removed using Protége."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Identifier Check**  \n",
    "Check class identifiers to ensure consistency in identifier prefixes. Running this check revealed mislabeling of two [pROtein Ontology](https://proconsortium.org/) identifiers in the [Vaccine Ontology](http://www.violinet.org/vaccineontology/) (see [this](https://github.com/vaccineontology/VO/issues) GitHub issue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all classes in graph\n",
    "kg_classes = graph.query(\n",
    "    \"\"\"SELECT DISTINCT ?c\n",
    "           WHERE {?c rdf:type owl:Class . }\n",
    "           \"\"\", initNs={'rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "                        'owl': 'http://www.w3.org/2002/07/owl#'}\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert results to list of classes and only keep hgnc identifiers\n",
    "class_list = [res[0] for res in tqdm(kg_classes) if isinstance(res[0], URIRef) and 'obo' in str(res[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_types = []\n",
    "\n",
    "for cls in class_list2:\n",
    "    class_types.append(cls.split('/')[-1].split('_')[0])\n",
    "    \n",
    "set(class_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### MERGE ONTOLOGIES <a class=\"anchor\" id=\"merge-ontologies\"></a>\n",
    "\n",
    "**Purpose:** In this step, the `OWL Tools` library is designed to merge a directory of ontology files into a single ontology file. This merged ontology file is required as input to the knowledge graph build algorithm.  \n",
    "\n",
    "**Inputs:** A directory of ontology files (`.owl`)  \n",
    "**Outputs:** [`PheKnowLator_MergedOntologies.owl`](https://www.dropbox.com/s/6e7gf8r229nbu67/PheKnowLator_MergedOntologies.owl?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/ontologies/chebi_lite_with_imports.owl\n",
      "resources/ontologies/clo_with_imports.owl\n",
      "resources/ontologies/doid_with_imports.owl\n",
      "resources/ontologies/ext_with_imports.owl\n",
      "resources/ontologies/go_with_imports.owl\n",
      "resources/ontologies/hp_with_imports.owl\n",
      "resources/ontologies/human_pro_closed_with_imports.owl\n",
      "resources/ontologies/pw_with_imports.owl\n",
      "resources/ontologies/so_with_imports.owl\n",
      "resources/ontologies/vo_with_imports.owl\n"
     ]
    }
   ],
   "source": [
    "# verify there are ontology files in ontology repo\n",
    "for ont in ontology_repository:\n",
    "    print(ont)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The knowledge graph contains 395290 classes, 4168504 axioms, 558 object properties, and 137 individuals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge ontologies\n",
    "if write_location + merged_ontology_file in glob.glob(write_location + '/*.owl'):\n",
    "    graph = Graph()\n",
    "    graph.parse(write_location + merged_ontology_file)\n",
    "    gets_ontology_statistics(write_location + merged_ontology_file)\n",
    "else:\n",
    "    merges_ontologies(ontology_repository, write_location, merged_ontology_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Normalize Classes <a class=\"anchor\" id=\"normalize-classes\"></a>\n",
    "\n",
    "**Purpose:** The goal of this section is to checked the cleaned merged ontology file to ensure that there is consistency between the existing classes. To do this, we check the following two things:  \n",
    "- <u>Connectivity Between Existing Classes</u>: For this check, we want to make sure that all classes that represent the same entity are connected to each other. For example, consider the following:  \n",
    "    - Ontologies: [Sequence Ontology](http://www.sequenceontology.org/), [ChEBI](https://www.ebi.ac.uk/chebi), and [PRotein Ontology](https://proconsortium.org/) all include terms for protein, but none of these classes are connected to each other. \n",
    "    \n",
    "    \n",
    "- <u>Consistency Between Ontology Classes and New Edge Data</u>: For this check, we want to make sure that any of the existing ontology classes can be aligned with any of the new data entities that we want to add to the knowledge graph. For example:  \n",
    "  - Gene Classes: there are several gene classes that use [HGNC](https://www.genenames.org/) identifiers. We also want to add genes, but prefer to use [Entrez gene](https://www.ncbi.nlm.nih.gov/gene) identifiers. In order to be used with our data, we must first normalize all of the HPO gene classes to Entrez gene identifiers.\n",
    "  \n",
    "<br>\n",
    "\n",
    "**Dependencies:** The Merged Gene, RNA, Protein Map ([`Merged_gene_rna_protein_identifiers.pkl`](https://www.dropbox.com/s/9zlysbqvpdtfq62/Merged_gene_rna_protein_identifiers.pkl?dl=1)) we generated in order to map genomic identifier data sources.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connectivity Between Existing Classes**\n",
    "\n",
    "The follow classes occur in all of the ontologies used in the current build and have to be resolved:  \n",
    "- Protein: [SO](http://purl.obolibrary.org/obo/SO_0000104), [PRO](http://purl.obolibrary.org/obo/PR_000000001), [ChEBI](http://purl.obolibrary.org/obo/CHEBI_36080)  \n",
    "  - <u>Solution</u>: Make the ChEBI and PRO classes a subclass of the SO term  \n",
    "    ```python\n",
    "    PR_000000001, rdfs:subClassOf, SO_0000104   \n",
    "    CHEBI_36080, rdfs:subClassOf, SO_0000104 \n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix protein class inconsistencies\n",
    "graph.add((URIRef('http://purl.obolibrary.org/obo/PR_000000001'),\n",
    "           URIRef('http://www.w3.org/2000/01/rdf-schema#subClassOf'),\n",
    "           URIRef('http://purl.obolibrary.org/obo/SO_0000104')))\n",
    "\n",
    "graph.add((URIRef('http://purl.obolibrary.org/obo/CHEBI_36080'),\n",
    "           URIRef('http://www.w3.org/2000/01/rdf-schema#subClassOf'),\n",
    "           URIRef('http://purl.obolibrary.org/obo/SO_0000104')))\n",
    "\n",
    "# save cleaned up ontology\n",
    "graph.serialize(destination=write_location + merged_ontology_file, format='xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consistency Between Ontology Classes and New Edge Data**  \n",
    "The first step to normalizing ontology classes with multiple identifiers is to query the ontology and obtain all classes that are not part of the [Open Biomedical Ontology](http://www.obofoundry.org/) namespace.\n",
    "\n",
    "For the current build, the primary focus of this task is to convert all classes that reference an HGNC gene (`n=19,820`) to an Entrez identifier. To do this, we will utilize the genomic identifier mapping information ([`Merged_gene_rna_protein_identifiers.pkl`](https://www.dropbox.com/s/9zlysbqvpdtfq62/Merged_gene_rna_protein_identifiers.pkl?dl=1)) we constructed in the [`Data_Preparation.ipynb`](https://github.com/callahantiff/PheKnowLator/blob/master/Data_Preparation.ipynb) Jupyter notebook. Note that we aree only updating identifiers and not verifying labels or other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert results to list of classes and only keep hgnc identifiers\n",
    "class_list_gene = [res[0] for res in tqdm(kg_classes) if isinstance(res[0], URIRef) and 'hgnc' in str(res[0])]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load genomic identifier mapping dictionary\n",
    "genomic_id_map = pickle.load(open('resources/processed_data/Merged_gene_rna_protein_identifiers.pkl', 'rb'), encoding='bytes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each gene class and get entrez gene id equivalent\n",
    "matches, not_matched = {}, []\n",
    "gene_url = 'https://www.ncbi.nlm.nih.gov/gene/'\n",
    "\n",
    "for gene_class in tqdm(class_list_gene):\n",
    "    key = 'hgnc_id_' + str(gene_class).split('=')[-1]\n",
    "    \n",
    "    if key in genomic_id_map.keys():\n",
    "        matches[str(gene_class)] = [gene_url + x.split('_')[-1] for x in genomic_id_map[key] if 'entrez_id' in x]\n",
    "    else:\n",
    "        not_matched.append(gene_class)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Investigate UnMatched Genes_  \n",
    "Only 1 of the HGNC genes was not found in our dictionary ([HGNC_24033](http://www.genenames.org/cgi-bin/gene_symbol_report?hgnc_id=24033)). Investigating this issue revealed that HGNC made this identifier obsolete and replaced it with [`HGNC:26545`](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/24033). Until this term is updated in the ontology, we have to manually fix it. \n",
    "\n",
    "This issue has been reported to [PRotein Ontology](https://proconsortium.org/pro.shtml) (see [this](https://github.com/PROconsortium/PRoteinOntology/issues/176) GitHub issue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate HGNC genes with no mappings to Entrez\n",
    "not_matched\n",
    "\n",
    "# update mapping dictionary\n",
    "gene_url = 'https://www.ncbi.nlm.nih.gov/gene/'\n",
    "matches[str(not_matched[0])] =  [gene_url + x.split('_')[-1] for x in genomic_id_map['hgnc_id_26545'] if 'entrez_id' in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated gene identifiers in graph\n",
    "for edge in tqdm(graph):\n",
    "    if str(edge[0]) in matches.keys():\n",
    "        for mapped_id in matches[str(edge[0])]:\n",
    "            graph.add((URIRef(mapped_id), edge[1], edge[2]))\n",
    "            graph.remove(edge)\n",
    "    elif str(edge[2]) in matches.keys():\n",
    "        for mapped_id in matches[str(edge[2])]:\n",
    "            graph.add((edge[0], edge[1], URIRef(mapped_id)))\n",
    "            graph.remove(edge)\n",
    "    else:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save normalized ontology\n",
    "graph.serialize(destination=write_location + merged_ontology_file[:-4] + 'GeneID_Normalized.owl', format='xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to crreate pairwise ontology merges -- -for owl reasonere challenge\n",
    "# import itertools\n",
    "\n",
    "# # get all pairwise merges of ontologies\n",
    "# ontology_merge_list_2 = list(itertools.combinations(ontology_repository, 2))\n",
    "# ontology_merge_list_3 = list(itertools.combinations(ontology_repository, 3))\n",
    "# ontology_merge_list_4 = list(itertools.combinations(ontology_repository, 4))\n",
    "# ontology_merge_list_5 = list(itertools.combinations(ontology_repository, 5))\n",
    "# ontology_merge_list_6 = list(itertools.combinations(ontology_repository, 6))\n",
    "# ontology_merge_list_7 = list(itertools.combinations(ontology_repository, 7))\n",
    "# ontology_merge_list_8 = list(itertools.combinations(ontology_repository, 8))\n",
    "# ontology_merge_list_9 = list(itertools.combinations(ontology_repository, 9))\n",
    "\n",
    "# # iterate\n",
    "# Ontology_list = ontology_merge_list_3\n",
    "# write_location = './resources/ontologies/reasoner_challenge/'\n",
    "\n",
    "# for ont_list in tqdm(Ontology_list):\n",
    "#     merged_file_name = '_'.join(['_'.join(x.split('/')[-1].split('_')[:-2]) for x in ont_list]) + '_merged.owl'\n",
    "#     merges_ontologies(list(ont_list), './resources/ontologies/reasoner_challenge/', merged_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
