{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# PheKnowLator - Data Preparation\n",
    "***\n",
    "***\n",
    "\n",
    "**Author:** [TJCallahan](https://mail.google.com/mail/u/0/?view=cm&fs=1&tf=1&to=callahantiff@gmail.com)  \n",
    "**GitHub Repository:** [PheKnowLator](https://github.com/callahantiff/PheKnowLator/wiki)  \n",
    "**Release:** **[v2.0.0](https://github.com/callahantiff/PheKnowLator/wiki/v2.0.0)**\n",
    "  \n",
    "<br>  \n",
    "  \n",
    "**Purpose:** This notebook serves as a script to download and process data in order to generate mapping and filtering data needed to build edges for the PheKnowLator knowledge graph. For more information on the data sources utilize within this script, please see the [Data Sources](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources) Wiki page.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Assumptions:**   \n",
    "- Raw data downloads ➞ `./resources/processed_data/unprocessed_data`    \n",
    "- Processed data write location ➞ `./resources/processed_data`  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Dependencies:** This notebook utilizes several helper functions, which are stored in the [`data_preparation_helper_functions.py`](https://github.com/callahantiff/PheKnowLator/blob/master/scripts/python/data_preparation_helper_functions.py) script. Hyperlinks to all downloaded and generated data sources are provided on the [Data Sources](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources) Wiki page as well as within each source subsection of this notebook. All generated data is freely available for download from DropBox. \n",
    "\n",
    "_____\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "***\n",
    "\n",
    "### [Create Identifier Maps ](#create-identifier-maps)  \n",
    "- [HUMAN TRANSCRIPT, GENE, AND PROTEIN IDENTIFIER MAPPING](#human-transcript,-gene,-and-protein-identifier-mapping)\n",
    "  - [Entrez Gene-Ensembl Transcript](#entrezgene-ensembltranscript)  \n",
    "  - [Entrez Gene-Protein Ontology](#entrezgene-proteinontology)  \n",
    "  - [Ensembl Gene-Entrez Gene](#ensemblgene-entrezgene)\n",
    "  - [Gene Symbol-Ensembl Transcript](#genesymbol-ensembltranscript)  \n",
    "  - [STRING-Protein Ontology](#string-proteinontology)  \n",
    "  - [Uniprot Accession-Protein Ontology](#uniprotaccession-proteinontology)\n",
    "  \n",
    "\n",
    "- [OTHER IDENTIFIER MAPPING](#other-identifier-mapping) \n",
    "  - [ChEBI Identifiers](#mesh-chebi) \n",
    "  - [Human Disease and Phenotype Identifiers](#disease-identifiers)\n",
    "  - [Human Protein Atlas Tissue and Cell Types](#hpa-uberon)  \n",
    "\n",
    "<br>\n",
    "\n",
    "### [Create Edge Datasets](#create-edge-datasets)\n",
    "- [ONTOLOGIES](#ontologies)  \n",
    "  - [Protein Ontology](#protein-ontology)  \n",
    "  - [Relations Ontology](#relations-ontology)  \n",
    "\n",
    "\n",
    "- [LINKED DATA](#linked-data)  \n",
    "  - [Clinvar Variant-Diseases and Phenotypes](#clinvar-variant)\n",
    "  - [Reactome Chemical-Complex Data](#reactome-chemical-complex)\n",
    "  - [Reactome Complex-Complex Data](#reactome-complex-complex)\n",
    "  - [Reactome Complex-Pathway Data](#reactome-complex-pathway)\n",
    "  - [Reactome Protein-Complex Data](#reactome-protein-complex)\n",
    "  - [Uniprot Protein-Cofactor and Protein-Catalyst](#uniprot-protein-cofactorcatalyst)  \n",
    "\n",
    "<br>\n",
    "\n",
    "### [Create Instance Data Metadata](#create-instance-metadata)  \n",
    "- [Genes/RNA](#gene-and-rna-metadata)\n",
    "- [Pathways](#pathway-metadata)\n",
    "- [Complexes](#complex-metadata)\n",
    "- [Reactions](#reaction-metadata)\n",
    "- [Variants](#variant-metadata) \n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-Up Environment\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import datetime\n",
    "import glob\n",
    "import itertools\n",
    "import networkx\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from owlready2 import subprocess\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, extras, Literal\n",
    "from rdflib.extras.external_graph_libs import *\n",
    "from reactome2py import content\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import script containing helper functions\n",
    "from scripts.python.data_preparation_helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to read unprocessed data files from\n",
    "unprocessed_data_location = 'resources/processed_data/unprocessed_data/'\n",
    "\n",
    "# directory to write processed data files to\n",
    "processed_data_location = 'resources/processed_data/'\n",
    "\n",
    "# directory to write relations data to\n",
    "relations_data_location = 'resources/relations_data/'\n",
    "\n",
    "# directory to write node metadata to\n",
    "node_data_location = 'resources/node_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### CREATE MAPPING DATASETS  <a class=\"anchor\" id=\"create-identifier-maps\"></a>\n",
    "***\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Transcript, Gene, and Protein Identifier Mapping  <a class=\"anchor\" id=\"human-transcript,-gene,-and-protein-identifier-mapping\"></a>\n",
    "***\n",
    "\n",
    "**Data Source Wiki Pages:**   \n",
    "- [Ensembl](https://uswest.ensembl.org/)  \n",
    "- [Uniprot Knowledgebase](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#uniprot-knowledgebase)  \n",
    "- [HGNC](ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt) \n",
    "- [NCBI Gene](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#ncbi-gene) \n",
    "- [Protein Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#protein-ontology)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Purpose:** To map create protein-coding gene-protein relations and mappings between the identifiers listed below. The edges types produced from each of these mappings will be further described within each identifier mapping section:  \n",
    "- [Entrez Gene-Ensembl Transcript](#entrezgene-ensembltranscript)  \n",
    "- [Entrez Gene-Protein Ontology](#entrezgene-proteinontology)  \n",
    "- [Ensembl Gene-Entrez Gene](#ensemblgene-entrezgene)\n",
    "- [Gene Symbol-Ensembl Transcript](#genesymbol-ensembltranscript)  \n",
    "- [STRING-Protein Ontology](#string-proteinontology)  \n",
    "- [Uniprot Accession-Protein Ontology](#uniprotaccession-proteinontology)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Gene and Trainscript Types:** The transcript and gene/locus types were reviewed by a PhD Molecular biologist to confirm whether or not they should be treated as `protein-coding` or not, which is useful for creating `genomic-rna`, `genomic-protein`, and `rna-protein` edges in the knowledge graph. For more information on this classification, please see the table below. Definitions of concepts in the table have been taken from [HGNC](https://www.genenames.org/help/symbol-report/), [Ensembl](https://uswest.ensembl.org/info/genome/genebuild/biotypes.html), [NCBI](https://www.ncbi.nlm.nih.gov/IEB/ToolBox/CPP_DOC/lxr/source/src/objects/entrezgene/entrezgene.asn), and Wikipedia.\n",
    "\n",
    "Gene and Transcript Type | Definition | Type | Genomic material Transcribed to RNA | RNA Translated to Protein | Genomic material has_gene_product Protein\n",
    "-- | -- | -- | -- | -- | --\n",
    "biological-region | biological_region (SO:0001411; Special note: This is a parental feature spanning all other feature annotation on each RefSeq Functional Element record. It is a 'misc_feature' in GenBank flat files but a 'Region' feature in ASN.1 and GFF3 formats | gene | yes | no | no\n",
    "IG_C_gene | IG C gene: Constant chain immunoglobulin gene that undergoes somatic recombination before transcription | gene | yes | no | no\n",
    "IG_C_gene | IG C gene: Constant chain immunoglobulin gene that undergoes somatic recombination before transcription | transcript | yes | yes | no\n",
    "IG_C_pseudogene | Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | gene | yes | no | no\n",
    "IG_C_pseudogene | Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | transcript | yes | yes | no\n",
    "IG_D_gene | IG D gene: Diversity chain immunoglobulin gene that undergoes somatic recombination before transcription | gene | yes | no | no\n",
    "IG_D_gene | IG D gene: Diversity chain immunoglobulin gene that undergoes somatic recombination before transcription | transcript | yes | yes | no\n",
    "IG_J_gene | IG J gene: Joining chain immunoglobulin gene that undergoes somatic recombination before transcription | gene | yes | no | no\n",
    "IG_J_gene | IG J gene: Joining chain immunoglobulin gene that undergoes somatic recombination before transcription | transcript | yes | yes | no\n",
    "IG_J_pseudogene | Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | gene | yes | no | no\n",
    "IG_J_pseudogene | Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | transcript | yes | yes | no\n",
    "IG_pseudogene | Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | gene | yes | no | no\n",
    "IG_pseudogene | Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | transcript | yes | yes | no\n",
    "IG_V_gene | IG V gene: Variable chain immunoglobulin gene that undergoes somatic recombination before transcription | gene | yes | no | no\n",
    "IG_V_gene | IG V gene: Variable chain immunoglobulin gene that undergoes somatic recombination before transcription | transcript | yes | yes | no\n",
    "IG_V_pseudogene | Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | gene | yes | no | no\n",
    "IG_V_pseudogene | Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | transcript | yes | yes | no\n",
    "lncRNA | RNA, long non-coding - non-protein coding genes that encode long non-coding RNAs (lncRNAs) (SO:0001877); these are at least 200 nt in length. Subtypes include intergenic (SO:0001463), intronic (SO:0001903) and antisense (SO:0001904) | gene | yes | no | no\n",
    "lncRNA | RNA, long non-coding - non-protein coding genes that encode long non-coding RNAs (lncRNAs) (SO:0001877); these are at least 200 nt in length. Subtypes include intergenic (SO:0001463), intronic (SO:0001903) and antisense (SO:0001904) | transcript | yes | no | no\n",
    "miRNA | RNA, micro - non-protein coding genes that encode microRNAs (miRNAs) (SO:0001265) | gene | yes | no | no\n",
    "miRNA | RNA, micro - non-protein coding genes that encode microRNAs (miRNAs) (SO:0001265) | transcript | yes | no | no\n",
    "misc_RNA | non-protein coding genes that encode miscellaneous types of small ncRNAs, such as vault (SO:0000404) and Y (SO:0000405) RNA genes | gene | yes | no | no\n",
    "misc_RNA | non-protein coding genes that encode miscellaneous types of small ncRNAs, such as vault (SO:0000404) and Y (SO:0000405) RNA genes | transcript | yes | no | no\n",
    "Mt_rRNA | mitochondrial rRNA | gene | yes | no | no\n",
    "Mt_rRNA | mitochondrial rRNA | transcript | yes | no | no\n",
    "Mt_tRNA | mitochondrial tRNA | gene | yes | no | no\n",
    "Mt_tRNA | mitochondrial tRNA | transcript | yes | no | no\n",
    "ncRNA | Noncoding RNA | gene | yes | no | no\n",
    "non_stop_decay | Nonstop decay. Transcripts that have polyA features (including signal) without a prior stop codon in the CDS, i.e. a non-genomic polyA tail attached directly to the CDS without 3' UTR. These transcripts are subject to degradation. | transcript | yes | no | no\n",
    "None | None | gene | yes | no | no\n",
    "None | None | transcript | yes | no | no\n",
    "nonsense_mediated_decay | Nonsense mediated decay. If the coding sequence (following the appropriate reference) of a transcript finishes >50bp from a downstream splice site then it is tagged as NMD. If the variant does not cover the full reference coding sequence then it is annotated as NMD if NMD is unavoidable i.e. no matter what the exon structure of the missing portion is the transcript will be subject to NMD | transcript | yes | yes | no\n",
    "other | Other | gene | yes | no | no\n",
    "phenotype | --- | gene | yes | no | no\n",
    "polymorphic_pseudogene | Polymorphic pseudogene. Pseudogene owing to a SNP/DIP but in other individuals/haplotypes/strains the gene is translated | gene | yes | no | no\n",
    "polymorphic_pseudogene | Polymorphic pseudogene. Pseudogene owing to a SNP/DIP but in other individuals/haplotypes/strains the gene is translated | transcript | yes | yes | no\n",
    "processed_pseudogene | Processed pseudogene. Pseudogene that lack introns and is thought to arise from reverse transcription of mRNA followed by reinsertion of DNA into the genome | gene | yes | no | no\n",
    "processed_pseudogene | Processed pseudogene. Pseudogene that lack introns and is thought to arise from reverse transcription of mRNA followed by reinsertion of DNA into the genome | transcript | yes | yes | no\n",
    "processed_transcript | Processed transcript: Gene/transcript that doesn't contain an open reading frame | transcript | yes | yes | no\n",
    "protein_coding | Protein coding. Contains an open reading frame (ORF) | transcript | yes | yes | yes\n",
    "protein-coding | Protein coding. Contains an open reading frame (ORF) | gene | yes | no | yes\n",
    "pseudogene | Similar to known proteins but contain a frameshift and/or stop codon(s) which disrupts the ORF | gene | yes | no | no\n",
    "pseudogene | Pseudogene. Have homology to proteins but generally suffer from a disrupted coding sequence and an active homologous gene can be found at another locus. Sometimes these entries have an intact coding sequence or an open but truncated ORF, in which case there is other evidence used (for example genomic polyA stretches at the 3' end) to classify them as a pseudogene | transcript | yes | yes | no\n",
    "retained_intron | Retained_intron. Has an alternatively spliced transcript believed to contain intronic sequence relative to other, coding, variants | transcript | yes | yes | no\n",
    "ribozyme | Ribozymes are RNA molecules that have the ability to catalyze specific biochemical reactions, including RNA splicing in gene expression, similar to the action of protein enzymes | gene | yes | no | no\n",
    "ribozyme | Ribozymes are RNA molecules that have the ability to catalyze specific biochemical reactions, including RNA splicing in gene expression, similar to the action of protein enzymes | transcript | yes | yes | no\n",
    "rRNA | RNA, ribosomal - non-protein coding genes that encode ribosomal RNAs (rRNAs) (SO:0001637) | gene | yes | no | no\n",
    "rRNA | RNA, ribosomal - non-protein coding genes that encode ribosomal RNAs (rRNAs) (SO:0001637) | transcript | yes | no | no\n",
    "rRNA_pseudogene | Pseudogene: A gene that has homology to known protein-coding genes but contain a frameshift and/or stop codon(s) which disrupts the ORF. Thought to have arisen through duplication followed by loss of function | gene | yes | no | no\n",
    "rRNA_pseudogene | Pseudogene: A gene that has homology to known protein-coding genes but contain a frameshift and/or stop codon(s) which disrupts the ORF. Thought to have arisen through duplication followed by loss of function | transcript | yes | yes | no\n",
    "scaRNA | Small Cajal body-specific RNAs are a class of small nucleolar RNAs that specifically localise to the Cajal body, a nuclear organelle involved in the biogenesis of small nuclear ribonucleoproteins | gene | yes | no | no\n",
    "scaRNA | Small Cajal body-specific RNAs are a class of small nucleolar RNAs that specifically localise to the Cajal body, a nuclear organelle involved in the biogenesis of small nuclear ribonucleoproteins | transcript | yes | no | no\n",
    "scRNA | RNA, small cytoplasmic - non-protein coding genes that encode small cytoplasmic RNAs (scRNAs) (SO:0001266) | gene | yes | no | no\n",
    "scRNA | RNA, small cytoplasmic - non-protein coding genes that encode small cytoplasmic RNAs (scRNAs) (SO:0001266) | transcript | yes | no | no\n",
    "snoRNA | RNA, small nucleolar - non-protein coding genes that encode small nucleolar RNAs (snoRNAs) containing C/D or H/ACA box domains (SO:0001267) | gene | yes | no | no\n",
    "snoRNA | RNA, small nucleolar - non-protein coding genes that encode small nucleolar RNAs (snoRNAs) containing C/D or H/ACA box domains (SO:0001267) | transcript | yes | no | no\n",
    "snRNA | RNA, small nuclear - non-protein coding genes that encode small nuclear RNAs (snRNAs) (SO:0001268) | gene | yes | no | no\n",
    "snRNA | RNA, small nuclear - non-protein coding genes that encode small nuclear RNAs (snRNAs) (SO:0001268) | transcript | yes | no | no\n",
    "sRNA | Bacterial small RNAs (sRNA) are small RNAs produced by bacteria; they are 50- to 500-nucleotide non-coding RNA molecules, highly structured and containing several stem-loops | gene | yes | no | no\n",
    "sRNA | Bacterial small RNAs (sRNA) are small RNAs produced by bacteria; they are 50- to 500-nucleotide non-coding RNA molecules, highly structured and containing several stem-loops | transcript | yes | no | no\n",
    "TEC | TEC (To be Experimentally Confirmed). This is used for non-spliced EST clusters that have polyA features. This category has been specifically created for the ENCODE project to highlight regions that could indicate the presence of protein coding genes that require experimental validation, either by 5' RACE or RT-PCR to extend the transcripts, or by confirming expression of the putatively-encoded peptide with specific antibodies | gene | yes | no | no\n",
    "TEC | TEC (To be Experimentally Confirmed). This is used for non-spliced EST clusters that have polyA features. This category has been specifically created for the ENCODE project to highlight regions that could indicate the presence of protein coding genes that require experimental validation, either by 5' RACE or RT-PCR to extend the transcripts, or by confirming expression of the putatively-encoded peptide with specific antibodies | transcript | yes | yes | no\n",
    "TR_C_gene | TR C gene: Constant chain T cell receptor gene that undergoes somatic recombination before transcription | gene | yes | no | no\n",
    "TR_C_gene | TR C gene: Constant chain T cell receptor gene that undergoes somatic recombination before transcription | transcript | yes | yes | no\n",
    "TR_D_gene | TR D gene: Diversity chain T cell receptor gene that undergoes somatic recombination before transcription | gene | yes | no | no\n",
    "TR_D_gene | TR D gene: Diversity chain T cell receptor gene that undergoes somatic recombination before transcription | transcript | yes | yes | no\n",
    "TR_J_gene | TR J gene: Joining chain T cell receptor gene that undergoes somatic recombination before transcription | gene | yes | no | no\n",
    "TR_J_gene | TR J gene: Joining chain T cell receptor gene that undergoes somatic recombination before transcription | transcript | yes | yes | no\n",
    "TR_J_pseudogene | T cell receptor pseudogene - T cell receptor gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | gene | yes | no | no\n",
    "TR_J_pseudogene | T cell receptor pseudogene - T cell receptor gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | transcript | yes | yes | no\n",
    "TR_V_gene | TR V gene: Variable chain T cell receptor gene that undergoes somatic recombination before transcription | gene | yes | no | no\n",
    "TR_V_gene | TR V gene: Variable chain T cell receptor gene that undergoes somatic recombination before transcription | transcript | yes | yes | no\n",
    "TR_V_pseudogene | T cell receptor pseudogene - T cell receptor gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | gene | yes | no | no\n",
    "TR_V_pseudogene | T cell receptor pseudogene - T cell receptor gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame | transcript | yes | yes | no\n",
    "transcribed_processed_pseudogene | Transcribed pseudogene: Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary' | gene | yes | no | no\n",
    "transcribed_processed_pseudogene | Transcribed pseudogene: Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary' | transcript | yes | no | no\n",
    "transcribed_unitary_pseudogene | Transcribed pseudogene: Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary' | gene | yes | no | no\n",
    "transcribed_unitary_pseudogene | Transcribed pseudogene: Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary' | transcript | yes | no | no\n",
    "transcribed_unprocessed_pseudogene | Transcribed pseudogene: Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary' | gene | yes | no | no\n",
    "transcribed_unprocessed_pseudogene | Transcribed pseudogene: Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary' | transcript | yes | no | no\n",
    "translated_processed_pseudogene | Translated pseudogene: Pseudogenes that have mass spec data suggesting that they are also translated. These can be classified into 'Processed', 'Unprocessed' | gene | yes | no | no\n",
    "translated_processed_pseudogene | Translated pseudogene: Pseudogenes that have mass spec data suggesting that they are also translated. These can be classified into 'Processed', 'Unprocessed' | transcript | yes | yes | no\n",
    "translated_unprocessed_pseudogene | Translated pseudogene: Pseudogenes that have mass spec data suggesting that they are also translated. These can be classified into 'Processed', 'Unprocessed' | gene | yes | no | no\n",
    "translated_unprocessed_pseudogene | Translated pseudogene: Pseudogenes that have mass spec data suggesting that they are also translated. These can be classified into 'Processed', 'Unprocessed' | transcript | yes | yes | no\n",
    "tRNA | tRNA. transfer RNA | gene | yes | no | no\n",
    "unitary_pseudogene | Unitary pseudogene: A species specific unprocessed pseudogene without a parent gene, as it has an active orthologue in another species | gene | yes | no | no\n",
    "unitary_pseudogene | Unitary pseudogene: A species specific unprocessed pseudogene without a parent gene, as it has an active orthologue in another species | transcript | yes | yes | no\n",
    "unknown | entries where the locus type is currently unknown | gene | yes | no | no\n",
    "unprocessed_pseudogene | Unprocessed pseudogene: Pseudogene that can contain introns since produced by gene duplication | gene | yes | no | no\n",
    "unprocessed_pseudogene | Unprocessed pseudogene: Pseudogene that can contain introns since produced by gene duplication | transcript | yes | yes | no\n",
    "vaultRNA | vaultRNA. Short non coding RNA genes that form part of the vault ribonucleoprotein complex | gene | yes | no | no\n",
    "vaultRNA | vaultRNA. Short non coding RNA genes that form part of the vault ribonucleoprotein complex | transcript | yes | no | no\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Output:** This script downloads and saves the following data:  \n",
    "- Human Ensembl Gene Set ➞ [`Homo_sapiens.GRCh38.99.gtf`](ftp://ftp.ensembl.org/pub/release-99/gtf/homo_sapiens/Homo_sapiens.GRCh38.99.gtf.gz)\n",
    "- Human Ensembl-UniProt Identifiers ➞ [`Homo_sapiens.GRCh38.98.uniprot.tsv`](https://www.dropbox.com/s/cesjvqz1b8c7ami/Homo_sapiens.GRCh38.98.uniprot.tsv?dl=1) \n",
    "- Human Ensembl-Entrez Identifiers ➞ [`Homo_sapiens.GRCh38.98.entrez.tsv`](https://www.dropbox.com/s/5kstw70py0azvws/Homo_sapiens.GRCh38.98.entrez.tsv?dl=1) \n",
    "- Human Gene Identifiers ➞ [`Homo_sapiens.gene_info`](https://www.dropbox.com/s/vazlmzxydgv6xzz/Homo_sapiens.gene_info?dl=1), [`hgnc_complete_set.txt`](ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt)  \n",
    "- Human Protein Identifiers ➞ [`promapping.txt`](https://www.dropbox.com/s/x7wdimv6ph6bl8k/promapping.txt?dl=1)  \n",
    "- UniProt Identifiers ➞ [`uniprot_identifier_mapping.tab`](https://www.dropbox.com/s/3wbd3wq35328a2v/uniprot_identifier_mapping.tab?dl=1)\n",
    "\n",
    "_All Merged Data Sets:_ [`Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt`](https://www.dropbox.com/s/fiek6h5rowi7dh0/Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt?dl=1)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**HGNC Data** \n",
    "\n",
    "_Human Gene Set Data_ - `hgnc_complete_set.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnc = pandas.read_csv(unprocessed_data_location + 'hgnc_complete_set.txt',\n",
    "                       header = 0,\n",
    "                       delimiter = '\\t',\n",
    "                       low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, updating data types (i.e. making all columns type `str`), and unnesting `|` delimited data. The final step is to update the gene_type variable such that each of the variable values is re-grouped to be protein-coding, other or ncRNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows thave don't have 'approved' status\n",
    "hgnc = hgnc.loc[hgnc['status'].apply(lambda x: x == 'Approved')]\n",
    "\n",
    "# drop uneeded columns\n",
    "hgnc = hgnc[['hgnc_id', 'entrez_id', 'ensembl_gene_id', 'uniprot_ids', 'symbol', 'locus_group',\n",
    "             'alias_symbol', 'name', 'location', 'alias_name']]\n",
    "\n",
    "# rename columns\n",
    "hgnc.rename(columns={'uniprot_ids': 'uniprot_id', 'location': 'map_location'}, inplace=True)\n",
    "\n",
    "# strip 'HGNC' off of the identifiers\n",
    "hgnc['hgnc_id'].replace('.*\\:','', inplace=True, regex=True)\n",
    "\n",
    "# combine certain columns into single column\n",
    "hgnc['primary_symbol'] = hgnc['symbol']\n",
    "hgnc['symbol'] = hgnc['symbol'] + '|' + hgnc['alias_symbol']\n",
    "hgnc['name'] = hgnc['name'] + '|' + hgnc['alias_name']\n",
    "hgnc['synonyms'] = hgnc['alias_symbol'] + '|' + hgnc['alias_name']\n",
    "\n",
    "# replace NaN with 'None'\n",
    "hgnc.fillna('None', inplace=True)\n",
    "\n",
    "# make data columns of type string\n",
    "hgnc['entrez_id'] = hgnc['entrez_id'].apply(lambda x: str(int(x)) if x != 'None' else 'None')\n",
    "\n",
    "# explode nested data\n",
    "explode_df_hgnc = explode(hgnc.copy(), ['ensembl_gene_id', 'uniprot_id', 'symbol', 'name'], '|')\n",
    "\n",
    "# update cell values\n",
    "explode_df_hgnc['gene_type_update'] = explode_df_hgnc['locus_group']\n",
    "explode_df_hgnc['gene_type_update'].replace('protein-coding gene', 'protein-coding', inplace=True, regex=True)\n",
    "explode_df_hgnc['gene_type_update'].replace('pseudogene', 'other', inplace=True, regex=True)\n",
    "explode_df_hgnc['gene_type_update'].replace('non-coding RNA', 'ncRNA', inplace=True, regex=True)\n",
    "explode_df_hgnc['gene_type_update'].replace('phenotype', 'other', inplace=True, regex=True)\n",
    "\n",
    "# remove original gene type column\n",
    "explode_df_hgnc.drop(['locus_group', 'alias_symbol', 'alias_name'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "explode_df_hgnc.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_hgnc.head(n=3)\n",
    "explode_df_hgnc.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "**Ensembl Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Human Gene Set Data_ - `Homo_sapiens.GRCh38.99.gtf.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ensembl.org/pub/release-99/gtf/homo_sapiens/Homo_sapiens.GRCh38.99.gtf.gz'\n",
    "data_downloader(url, unprocessed_data_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_geneset = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.99.gtf',\n",
    "                                  header = None,\n",
    "                                  delimiter = '\\t',\n",
    "                                  skiprows = 5,\n",
    "                                  low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be reformatted in order for it to be able to be merged with the other gene, RNA, and protein identifier data. To do this, we iterate over each row of the data and extract the fields shown below in `column_names`, making each of these extracted fields their own column. The final step is to update the gene_type variable such that each of the variable values is re-grouped to be `protein-coding`, `other` or `ncRNA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the nested column and unravel it\n",
    "column_names = ['gene_id', 'gene_version', 'transcript_id', 'transcript_version', 'exon_number', 'gene_name',\n",
    "                'gene_source', 'gene_biotype', 'transcript_name', 'transcript_source', 'transcript_biotype',\n",
    "                'exon_id', 'exon_version', 'tag', 'transcript_support_level']\n",
    "\n",
    "cleaned_column = []\n",
    "\n",
    "for idx, row in tqdm(ensembl_geneset.iterrows(), total=ensembl_geneset.shape[0]):\n",
    "    row_results, col_res = [], row[8].split(';')\n",
    "\n",
    "    for col in column_names:\n",
    "        match = [x.replace(col, '').strip().strip('\"') for x in col_res if col in x]\n",
    "        row_results.append(match[0].replace(col, '') if len(match) > 0 else 'None')\n",
    "\n",
    "    cleaned_column += [row_results]\n",
    "\n",
    "# add columns back to data frame\n",
    "ensembl_geneset['ensembl_gene_id'] = [x[0] for x in cleaned_column]\n",
    "ensembl_geneset['transcript_stable_id'] = [x[2] for x in cleaned_column]\n",
    "ensembl_geneset['symbol'] = [x[5] for x in cleaned_column]\n",
    "ensembl_geneset['primary_symbol'] = [x[5] for x in cleaned_column]\n",
    "ensembl_geneset['gene_type'] = [x[7] for x in cleaned_column]\n",
    "ensembl_geneset['transcript_type'] = [x[10] for x in cleaned_column]\n",
    "ensembl_geneset['transcript_name'] = [x[8] for x in cleaned_column]\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_geneset.fillna('None', inplace=True)\n",
    "\n",
    "# remove uneeded columns\n",
    "ensembl_geneset.drop(list(range(9)), axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_geneset.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_geneset.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensembl Annotation Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ensembl-UniProt_ - `Homo_sapiens.GRCh38.98.uniprot.tsv`  \n",
    "Once the main ensembl gene set has been read in, the next step is to read in the `ensembl-uniprot` mapping file. These files are vital for successfully merging the ensembl identifiers with the uniprot data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_uniprot = 'ftp://ftp.ensembl.org/pub/release-99/tsv/homo_sapiens/Homo_sapiens.GRCh38.99.uniprot.tsv.gz'\n",
    "data_downloader(url_uniprot, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_uniprot = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.99.uniprot.tsv',\n",
    "                           header = 0,\n",
    "                           delimiter = '\\t',\n",
    "                           low_memory=False)\n",
    "\n",
    "# rename columns\n",
    "ensembl_uniprot.rename(columns={'xref': 'uniprot_id', 'gene_stable_id': 'ensembl_gene_id'}, inplace=True)\n",
    "\n",
    "# replace \"-\"\n",
    "ensembl_uniprot.replace('-', 'None', inplace=True)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_uniprot.fillna('None', inplace=True)\n",
    "\n",
    "# remove uneeded columns\n",
    "ensembl_uniprot.drop(['db_name', 'info_type', 'source_identity', 'xref_identity', 'linkage_type'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_uniprot.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ensembl-Entrez_ - `Homo_sapiens.GRCh38.98.entrez.tsv`  \n",
    "Once the main ensembl gene set has been read in, the next step is to read in the `ensembl-entrez` mapping file. These files are vital for successfully merging the ensembl identifiers with the entrez data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_entrez = 'ftp://ftp.ensembl.org/pub/release-99/tsv/homo_sapiens/Homo_sapiens.GRCh38.99.entrez.tsv.gz'\n",
    "data_downloader(url_entrex, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ensembl-entrez data\n",
    "ensembl_entrez = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.99.entrez.tsv',\n",
    "                                 header = 0,\n",
    "                                 delimiter = '\\t',\n",
    "                                 low_memory=False)\n",
    "\n",
    "# rename columns\n",
    "ensembl_entrez.rename(columns={'xref': 'entrez_id', 'gene_stable_id': 'ensembl_gene_id'}, inplace=True)\n",
    "\n",
    "# remove all rows that are not dbname \"EntrezGene\"\n",
    "ensembl_entrez = ensembl_entrez.loc[ensembl_entrez['db_name'].apply(lambda x: x == 'EntrezGene')]\n",
    "\n",
    "# replace \"-\"\n",
    "ensembl_entrez.replace('-', 'None', inplace=True)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_entrez.fillna('None', inplace=True)\n",
    "\n",
    "# remove uneeded columns\n",
    "ensembl_entrez.drop(['db_name', 'info_type', 'source_identity', 'xref_identity', 'linkage_type'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_entrez.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Merge Annotation Data_ - `ensembl_uniprot` + `ensembl_entrez`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_annot = pandas.merge(ensembl_uniprot,\n",
    "                             ensembl_entrez,\n",
    "                             left_on=['ensembl_gene_id', 'transcript_stable_id', 'protein_stable_id'],\n",
    "                             right_on=['ensembl_gene_id', 'transcript_stable_id', 'protein_stable_id'],\n",
    "                             how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_annot.fillna('None', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_annot.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Merge Ensembl Annotation and Gene Set Data_ - `ensembl_geneset` + `ensembl_annot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl = pandas.merge(ensembl_geneset,\n",
    "                       ensembl_annot,\n",
    "                       left_on = ['ensembl_gene_id', 'transcript_stable_id'],\n",
    "                       right_on = ['ensembl_gene_id', 'transcript_stable_id'],\n",
    "                       how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl.fillna('None', inplace=True)\n",
    "ensembl.replace('NA','None', inplace=True, regex=False)\n",
    "\n",
    "# preview data\n",
    "ensembl.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Clean Merged Data_  \n",
    "The final step once the gene set and annotation data have been merged is to update the `gene_type` and `transcript_type` variables such that each of the variable values is re-grouped to be `protein-coding`, `other` or `ncRNA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update cell values for transcript and gene type\n",
    "ensembl['gene_type_update'] = ensembl['gene_type']\n",
    "ensembl['transcript_type_update'] = ensembl['transcript_type']\n",
    "\n",
    "# protein-coding values\n",
    "for val in ['protein_coding', 'polymorphic', 'IG_C_gene', 'IG_C_pseudogene', 'IG_D_gene', 'IG_J_gene',\n",
    "            'IG_J_pseudogene', 'IG_V_gene', 'IG_V_pseudogene', 'TEC', 'TR_C_gene', 'TR_D_gene', 'nonsense_mediated_decay',\n",
    "            'TR_J_gene', 'TR_J_pseudogene', 'TR_V_gene', 'TR_V_pseudogene', 'translated_processed_pseudogene',\n",
    "            'translated_unprocessed_pseudogene', 'IG_pseudogene', 'unitary_pseudogene', 'rRNA_pseudogene', 'ribozyme',\n",
    "            'pseudogene', 'retained_intron', 'polymorphic_pseudogene', 'processed_pseudogene', 'processed_transcript', 'unprocessed_pseudogene']:\n",
    "    \n",
    "    if val == 'protein_coding':\n",
    "        ensembl['gene_type_update'].replace(val, 'protein-coding', inplace=True, regex=False)\n",
    "        ensembl['transcript_type_update'].replace(val, 'protein-coding', inplace=True, regex=False)\n",
    "    else:\n",
    "        ensembl['gene_type_update'].replace(val, 'other', inplace=True, regex=False)\n",
    "        ensembl['transcript_type_update'].replace(val, 'protein-coding', inplace=True, regex=False)\n",
    "\n",
    "# noncoding values\n",
    "for val in ['miRNA', 'vaultRNA', 'lnRNA', 'sRNA', 'scRNA', 'scaRNA', 'snRNA', 'snoRNA', 'lncRNA']:\n",
    "    ensembl['gene_type_update'].replace(val, 'ncRNA', inplace=True, regex=False)\n",
    "    ensembl['transcript_type_update'].replace(val, 'ncRNA', inplace=True, regex=False)\n",
    "\n",
    "# other values\n",
    "for val in ['rRNA', 'misc_RNA', 'mt_rRNA', 'mt_tRNA', 'Mt_rRNA', 'Mt_tRNA', 'Mt_other','transcribed_processed_pseudogene', 'non_stop_decay',\n",
    "            'transcribed_unprocessed_pseudogene', 'transcribed_processed_protein-coding', 'transcribed_unitary_pseudogene',\n",
    "            'transcribed_protein-coding', 'transcribed_unprocessed_protein-coding', 'transcribed_unprotein-coding']:\n",
    "    \n",
    "    ensembl['gene_type_update'].replace(val, 'other', inplace=True, regex=False)\n",
    "    ensembl['transcript_type_update'].replace(val, 'other', inplace=True, regex=False)\n",
    "        \n",
    "# remove uneeded columns\n",
    "ensembl.drop(['gene_type', 'transcript_type'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "        \n",
    "# preview data\n",
    "ensembl.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**UniProt Data**   \n",
    "_Human Gene Set Data_ - `uniprot_identifier_mapping.tab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.uniprot.org/uniprot/?query=&fil=organism%3A%22Homo%20sapiens%20(Human)%20%5B9606%5D%22&columns=id%2Cdatabase(GeneID)%2Cdatabase(Ensembl)%2Cdatabase(HGNC)%2Cgenes(PREFERRED)%2Cgenes(ALTERNATIVE)&format=tab'\n",
    "data_downloader(url, unprocessed_data_location, 'uniprot_identifier_mapping.tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot = pandas.read_csv(unprocessed_data_location + 'uniprot_identifier_mapping.tab',\n",
    "                          header = 0,\n",
    "                          delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, and unnesting `;` and `\" \"` delimited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with 'None'\n",
    "uniprot.fillna('None', inplace=True)\n",
    "\n",
    "# rename columns\n",
    "uniprot.rename(columns={'Entry': 'uniprot_id',\n",
    "                        'Cross-reference (GeneID)': 'entrez_id',\n",
    "                        'Ensembl transcript': 'transcript_stable_id',\n",
    "                        'Cross-reference (HGNC)': 'hgnc_id',\n",
    "                        'Gene names  (synonym )': 'synonyms',\n",
    "                        'Gene names  (primary )' :'symbol'}, inplace=True)\n",
    "\n",
    "# combine symbols and synonyms into single column\n",
    "uniprot['primary_symbol'] = uniprot['symbol']\n",
    "uniprot['symbol'] = uniprot['symbol'] + ' ' + uniprot['synonyms']\n",
    "\n",
    "# explode nested data\n",
    "explode_df_uniprot = explode(uniprot.copy(), ['transcript_stable_id', 'entrez_id', 'hgnc_id'], ';')\n",
    "explode_df_uniprot = explode(explode_df_uniprot.copy(), ['symbol'], ' ')\n",
    "\n",
    "# strip out uniprot names\n",
    "explode_df_uniprot['transcript_stable_id'].replace('\\s.*','', inplace=True, regex=True)\n",
    "\n",
    "# remove duplicates\n",
    "explode_df_uniprot.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_uniprot.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "**NCBI Data**   \n",
    "_Human Gene Set Data_ - `Homo_sapiens.gene_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi_gene = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.gene_info',\n",
    "                            header = 0,\n",
    "                            delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, updating data types (i.e. making all columns type `str`), and unnesting `|` delimited data. Then, the `gene_type` variable is cleaned such that each of the variable's values are re-grouped to be `protein-coding`, `other` or `ncRNA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows that are not human\n",
    "ncbi_gene = ncbi_gene.loc[ncbi_gene['#tax_id'].apply(lambda x: x == 9606)]\n",
    "\n",
    "# replace \"-\" with \"None\"\n",
    "ncbi_gene.replace('-','None', inplace=True)\n",
    "\n",
    "# rename columns before merging\n",
    "ncbi_gene.rename(columns={'GeneID': 'entrez_id', 'Symbol': 'symbol', 'Synonyms': 'synonyms'}, inplace=True)\n",
    "\n",
    "# combine symbol columns into single column\n",
    "ncbi_gene['primary_symbol'] = ncbi_gene['symbol']\n",
    "ncbi_gene['symbol'] = ncbi_gene['Symbol_from_nomenclature_authority'] + '|' + ncbi_gene['symbol'] + '|' + ncbi_gene['synonyms']\n",
    "ncbi_gene['name'] = ncbi_gene['Full_name_from_nomenclature_authority'] + '|' + ncbi_gene['description']\n",
    "\n",
    "# explode nested data\n",
    "explode_df_ncbi_gene = explode(ncbi_gene.copy(), ['symbol', 'name'], '|')\n",
    "\n",
    "# make sure that merge columns are of same type\n",
    "explode_df_ncbi_gene['entrez_id'] = explode_df_ncbi_gene['entrez_id'].astype(str)\n",
    "\n",
    "# update cell values\n",
    "explode_df_ncbi_gene['gene_type_update'] = explode_df_ncbi_gene['type_of_gene']\n",
    "explode_df_ncbi_gene['gene_type_update'].replace('pseudo', 'other', inplace=True, regex=True)\n",
    "explode_df_ncbi_gene['gene_type_update'].replace('biological-region', 'other', inplace=True, regex=True)\n",
    "explode_df_ncbi_gene['gene_type_update'].replace('tRNA', 'other', inplace=True, regex=True)\n",
    "explode_df_ncbi_gene['gene_type_update'].replace('rRNA', 'other', inplace=True, regex=True)\n",
    "explode_df_ncbi_gene['gene_type_update'].replace('scRNA', 'ncRNA', inplace=True, regex=True)\n",
    "explode_df_ncbi_gene['gene_type_update'].replace('snRNA', 'ncRNA', inplace=True, regex=True)\n",
    "explode_df_ncbi_gene['gene_type_update'].replace('snoRNA', 'ncRNA', inplace=True, regex=True)\n",
    "\n",
    "# remove uneeded columns\n",
    "explode_df_ncbi_gene.drop(['type_of_gene', 'dbXrefs', 'description', 'Nomenclature_status', 'Modification_date',\n",
    "                           'LocusTag', '#tax_id', 'Full_name_from_nomenclature_authority', 'Feature_type',\n",
    "                           'Symbol_from_nomenclature_authority'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "explode_df_ncbi_gene.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_ncbi_gene.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Protein Ontology Identifier Mapping Data**   \n",
    "_Protein Ontology Identifier Data_ - `promapping.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://proconsortium.org/download/current/promapping.txt'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pro_mapping = pandas.read_csv(unprocessed_data_location + 'promapping.txt',\n",
    "                              header = None,\n",
    "                              names = ['pro_id', 'Entry', 'pro_mapping'],\n",
    "                              delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Basic filtering to to include `Protein Ontology` mappings to `Uniprot` identifiers and cleaning to update formatting of accession values in order to remove `UniProtKB:` is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows without 'UniProtKB'\n",
    "pro_mapping = pro_mapping.loc[pro_mapping['Entry'].apply(lambda x: x.startswith('UniProtKB:'))] \n",
    "\n",
    "# remove identifier type, which appears before ':'\n",
    "pro_mapping['Entry'].replace('(^\\w*\\:)','', inplace=True, regex=True)\n",
    "\n",
    "# rename columns before merging\n",
    "pro_mapping.rename(columns={'Entry': 'uniprot_id'}, inplace=True)\n",
    "\n",
    "# remove uneeded columns\n",
    "pro_mapping.drop(['pro_mapping'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "pro_mapping.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "pro_mapping.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "**Merge Processed Data:** `hgnc` + `ensembl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge uniprot and ncbi data\n",
    "ensembl_hgnc_merged_data = pandas.merge(ensembl,\n",
    "                                        explode_df_hgnc,\n",
    "                                        left_on=['ensembl_gene_id',\n",
    "                                                 'entrez_id',\n",
    "                                                 'uniprot_id',\n",
    "                                                 'gene_type_update',\n",
    "                                                 'symbol',\n",
    "                                                 'primary_symbol'],\n",
    "                                        right_on=['ensembl_gene_id',\n",
    "                                                  'entrez_id',\n",
    "                                                  'uniprot_id',\n",
    "                                                  'gene_type_update',\n",
    "                                                  'symbol',\n",
    "                                                  'primary_symbol'],\n",
    "                                        how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_hgnc_merged_data.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_hgnc_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "**Merge Processed Data:** `ensembl_hgnc_merged_data` + `explode_df_uniprot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge uniprot and ncbi data\n",
    "ensembl_hgnc_uniprot_merged_data = pandas.merge(ensembl_hgnc_merged_data,\n",
    "                                                explode_df_uniprot,\n",
    "                                                left_on=['entrez_id',\n",
    "                                                         'hgnc_id',\n",
    "                                                         'uniprot_id',\n",
    "                                                         'transcript_stable_id',\n",
    "                                                         'symbol',\n",
    "                                                         'primary_symbol',\n",
    "                                                         'synonyms'],\n",
    "                                                right_on=['entrez_id',\n",
    "                                                          'hgnc_id',\n",
    "                                                          'uniprot_id',\n",
    "                                                          'transcript_stable_id',\n",
    "                                                          'symbol',\n",
    "                                                          'primary_symbol',\n",
    "                                                          'synonyms'],\n",
    "                                                how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_hgnc_uniprot_merged_data.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_hgnc_uniprot_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_uniprot_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Merge Processed Data:** `ensembl_hgnc_uniprot_merged_data` + `Homo_sapiens.gene_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ensembl_hgnc_uniprot_ncbi_merged_data = pandas.merge(ensembl_hgnc_uniprot_merged_data,\n",
    "                                                     explode_df_ncbi_gene,\n",
    "                                                     left_on=['entrez_id',\n",
    "                                                              'gene_type_update',\n",
    "                                                              'symbol',\n",
    "                                                              'primary_symbol',\n",
    "                                                              'synonyms',\n",
    "                                                              'name',\n",
    "                                                              'map_location'],\n",
    "                                                     right_on=['entrez_id',\n",
    "                                                               'gene_type_update',\n",
    "                                                               'symbol',\n",
    "                                                               'primary_symbol',\n",
    "                                                               'synonyms',\n",
    "                                                               'name',\n",
    "                                                               'map_location'],\n",
    "                                                     how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Merge Processed Data:** `ensembl_hgnc_uniprot_ncbi_merged_data` + `promapping.txt`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "merged_data = pandas.merge(ensembl_hgnc_uniprot_ncbi_merged_data,\n",
    "                           pro_mapping,\n",
    "                           left_on='uniprot_id',\n",
    "                           right_on='uniprot_id',\n",
    "                           how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "merged_data.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fix Symbol Formatting_  \n",
    "some genes are formatted similarly to dates (e.g. `DEC1`), which can be erroneously re-formatted during input as a date value (i.e. `1-DEC`). In order for the data to be successfully merged with other data sources, all date-formatted genes need to be resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dates = []\n",
    "\n",
    "for x in tqdm(list(merged_data['symbol'])):\n",
    "    if '-' in x and len(x.split('-')[0]) < 3 and len(x.split('-')[1]) == 3:\n",
    "        clean_dates.append(x.split('-')[1].upper() + x.split('-')[0])\n",
    "    else:\n",
    "        clean_dates.append(x)\n",
    "\n",
    "# add cleaned date var back to data set\n",
    "merged_data['symbol'] = clean_dates\n",
    "\n",
    "# remove duplicates\n",
    "merged_data.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write Merged Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "merged_data_clean = merged_data.drop_duplicates(subset=None, keep='first')\n",
    "\n",
    "# write data\n",
    "merged_data_clean.to_csv(processed_data_location + 'Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt',\n",
    "                         header = True,\n",
    "                         sep = '\\t',\n",
    "                         index = False)\n",
    "    \n",
    "# preview data\n",
    "merged_data_clean.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Master Mapping Dictionary**  \n",
    "Although the above steps result in a `pandas.Dataframe` of the merged identifiers, there is still work needed in order to be able to obtain a complete mapping between the identifiers. For example, if you were to search for entrez identifier `entrez_259234` you would find the following mappings: `entrez_259234-ENSG00000233316`, `entrez_259234-DSCR10`. If you only had `ENSG00000233316`, from the current data you would be unable to obtain the gene symbol without first mapping to the Entrez gene identifier. \n",
    "\n",
    "To solve this problem, we build a master dictionary where the keys are `ensembl_gene_id`, `transcript_stable_id`, `symbol`, `protein_stable_id`, `uniprot_id`, `entrez_id`, `hgnc_id`, and `pro_id` identifiers and values are the list of identifiers that match to each identifier. It's important to note that there are several labeling identifiers (i.e. `name`, `primary_symbol`, `chromosome`, `map_location`, `Other_designations`, `synonyms`, `transcript_name`, `gene_type_update`, and `trasnscript_type_update`), which will only be mapped when clustered against one of the primary identifier types (i.e. the keys described above).\n",
    "\n",
    "_Note_. The next chunk takes approximately ~80 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all permutations of identifiers (e.g. ['entrez_id', 'ensembl_gene_id'])\n",
    "identifiers = ['ensembl_gene_id', 'transcript_stable_id', 'symbol', 'primary_symbol',\n",
    "               'protein_stable_id', 'uniprot_id', 'entrez_id', 'gene_type_update',\n",
    "               'transcript_type_update', 'hgnc_id', 'name', 'map_location', 'synonyms',\n",
    "               'chromosome', 'Other_designations', 'pro_id', 'transcript_name']\n",
    "\n",
    "identifier_list = list(itertools.permutations(identifiers, 2))\n",
    "\n",
    "# create master dictionary of all identifiers\n",
    "master_dict = {}\n",
    "\n",
    "for ids in tqdm(identifier_list):\n",
    "    if ids[0] not in ['primary_symbol', 'gene_type_update', 'transcript_type_update', 'name',\n",
    "                      'map_location', 'synonyms', 'chromosome', 'Other_designations', 'transcript_name']:\n",
    "        \n",
    "        print(ids)\n",
    "        \n",
    "        maps = {k: [ids[1] + '_' + x for x in set(g[ids[1]].tolist()) if x != 'None'] for k,g in merged_data.groupby(ids[0])}\n",
    "        \n",
    "        for key in tqdm(maps.keys()):\n",
    "            if ids[0] + '_' + key in master_dict.keys():\n",
    "                master_dict[ids[0] + '_' + key] += maps[key]\n",
    "            else:\n",
    "                master_dict[ids[0] + '_' + key] = maps[key]\n",
    "\n",
    "# save a copy of the dictionary\n",
    "json.dump(master_dict, open(processed_data_location + 'Merged_gene_rna_protein_identifiers.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write Dictionary Locally_  \n",
    "Convert dictionary to a pairwise list of identifiers and include a third column that contains the gene or transcript type mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set globals\n",
    "gene_type_var = 'gene_type_update'\n",
    "transcript_type_var = 'transcript_type_update'\n",
    "\n",
    "# loop over dict and create master dicitonary\n",
    "master_map_list = []\n",
    "\n",
    "for ident in tqdm(master_dict.keys()):    \n",
    "    mapped_ints = master_dict[ident].copy()\n",
    "    \n",
    "    # get gene and transcript types\n",
    "    gene_types = [x.split('_')[-1] for x in mapped_ints if x.startswith(gene_type_var)]\n",
    "    trans_types = [x.split('_')[-1] for x in mapped_ints if x.startswith(transcript_type_var)]\n",
    "    mapped_ints = [x for x in mapped_ints if not x.startswith(gene_type_var) and not x.startswith(transcript_type_var)]\n",
    "    \n",
    "    # clean gene and transcript types mapped to multiple types\n",
    "    updates = []\n",
    "    for types in [gene_types if len(gene_types) > 0 else ['None'], trans_types if len(trans_types) > 0 else ['None']]:\n",
    "        if len(set(types)) == 1:\n",
    "            updates.append(list(set(types))[0])\n",
    "        elif 'protein-coding' in set(types):\n",
    "            updates.append('protein-coding')\n",
    "        else:\n",
    "            updates.append('not protein-coding')\n",
    "    \n",
    "    gene_type = updates[0]\n",
    "    trans_type = updates[1]\n",
    "    \n",
    "    # iterate over identifiers and add to master list\n",
    "    for mapped_id in mapped_ints:\n",
    "        master_map_list.append([ident, mapped_id, gene_type, trans_type])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembl Gene-Entrez Gene <a class=\"anchor\" id=\"ensemblgene-entrezgene\"></a>\n",
    "\n",
    "\n",
    "**Purpose:** To map Ensembl gene identifiers to Entrez gene identifiers when creating the following edges:   \n",
    "- gene-gene\n",
    "\n",
    "**Output:** [`ENSEMBL_GENE_ENTREZ_GENE_MAP.txt`](https://www.dropbox.com/s/crghjh2we5v7pws/ENSEMBL_GENE_ENTREZ_GENE_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'ENSEMBL_GENE_ENTREZ_GENE_MAP.txt', 'w') as outfile:\n",
    "    for row in tqdm(master_map_list):\n",
    "        if 'ensembl_gene_id' in row[0] and 'entrez_id' in row[1]:\n",
    "            outfile.write(row[0].split('_')[-1] + '\\t' + row[1].split('_')[-1] + '\\t' + row[2] + '\\n')\n",
    "\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egeg_data = pandas.read_csv(processed_data_location + 'ENSEMBL_GENE_ENTREZ_GENE_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['Ensembl_Gene_IDs', 'Entrez_Gene_IDs', 'Gene_Type'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} ensembl gene-entrez gene edges'.format(edge_count=len(egeg_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egeg_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembl Transcript-Protein Ontology <a class=\"anchor\" id=\"ensembltranscript-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Ensembl transcript identifiers to Protein Ontology identifiers when creating the following edges: \n",
    "- rna-protein  \n",
    "\n",
    "**Output:** [`ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/ckrw11nfyu6a08c/ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt', 'w') as outfile:\n",
    "    for row in tqdm(master_map_list):\n",
    "        if 'transcript_stable_id' in row[0] and 'pro_id' in row[1]:\n",
    "            outfile.write(row[0].split('_')[-1] + '\\t' + row[1].split('_')[-1].replace(':', '_') + '\\t' + row[3] + '\\n')\n",
    "\n",
    "outfile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etpr_data = pandas.read_csv(processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['Ensembl_Transcript_IDs', 'Protein_Ontology_IDs', 'Transcript_Type'],\n",
    "                            delimiter = '\\t',\n",
    "                            low_memory=False)\n",
    "\n",
    "print('There are {edge_count} ensembl transcript-protein ontology edges'.format(edge_count=len(etpr_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrez Gene-Ensembl Transcript <a class=\"anchor\" id=\"entrezgene-ensembltranscript\"></a>\n",
    "\n",
    "**Purpose:** To map entrez gene identifiers to Ensembl transcript identifiers when creating the following edges: \n",
    "- gene-rna \n",
    "\n",
    "**Output:** [`ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt`](https://www.dropbox.com/s/yqnofd8h90luygu/ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt', 'w') as outfile:\n",
    "    for row in tqdm(master_map_list):\n",
    "        if 'entrez_id' in row[0] and 'transcript_stable_id' in row[1]:\n",
    "            outfile.write(row[0].split('_')[-1] + '\\t' + row[1].split('_')[-1] + '\\t' + row[3] + '\\t' + row[2] + '\\n')\n",
    "\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eet_data = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                            header=None,\n",
    "                            names=['Entrez_Gene_IDs', 'Ensembl_Transcript_IDs', 'Gene_Type', 'Transcript_Type'],\n",
    "                            delimiter='\\t',\n",
    "                           low_memory=False)\n",
    "\n",
    "print('There are {edge_count} entrez gene identifiers-ensembl transcript edges'.format(edge_count=len(eet_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eet_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrez Gene-Protein Ontology <a class=\"anchor\" id=\"entrezgene-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Protein Ontology identifiers to Ensembl transcript identifiers when creating the following edges:   \n",
    "- chemical-protein  \n",
    "- gene-protein\n",
    "\n",
    "**Output:** [`ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/ufbp5o6zgagriw7/ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt', 'w') as outfile:\n",
    "    for row in tqdm(master_map_list):\n",
    "        if 'entrez_id' in row[0] and 'pro_id' in row[1]:\n",
    "            outfile.write(row[0].split('_')[-1] + '\\t' + row[1].split('_')[-1].replace(':', '_') + '\\t' + row[2] + '\\n')\n",
    "\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egpr_data = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['Gene_IDs', 'Protein_Ontology_IDs', 'Gene_Type'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} entrez gene-protein ontology edges'.format(edge_count=len(egpr_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gene Symbol-Ensembl Transcript <a class=\"anchor\" id=\"genesymbol-ensembltranscript\"></a>\n",
    "\n",
    "**Purpose:** To map gene symbols to Ensembl transcript identifiers when creating the following edges: \n",
    "- chemical-rna  \n",
    "- rna-anatomy  \n",
    "- rna-cell  \n",
    "\n",
    "**Output:** [`GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt`](https://www.dropbox.com/s/5o8yt7eejbf819x/GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt', 'w') as outfile:\n",
    "    for row in tqdm(master_map_list):\n",
    "        if 'symbol' in row[0] and 'transcript_stable_id' in row[1]:\n",
    "            outfile.write(row[0].split('_')[-1] + '\\t' + row[1].split('_')[-1] + '\\t' + row[2] + '\\t' + row[2] + '\\n')\n",
    "\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data = pandas.read_csv(processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['Gene_Symbols', 'Ensembl_Transcript_IDs', 'Gene_Type', 'Transcript_Type'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} gene symbol-ensembl transcript edges'.format(edge_count=len(set_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STRING-Protein Ontology <a class=\"anchor\" id=\"string-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map STRING identifiers to Protein Ontology identifiers when creating the following edges:   \n",
    "- protein-protein  \n",
    "\n",
    "**Output:** [`STRING_PRO_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/mekh5lr3bxp7gvu/STRING_PRO_ONTOLOGY_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt', 'w') as outfile:\n",
    "    for row in tqdm(master_map_list):\n",
    "        if 'protein_stable_id' in row[0] and 'pro_id' in row[1]:\n",
    "            outfile.write('9606.' + row[0].split('_')[-1] + '\\t' + row[1].split('_')[-1].replace(':', '_') + '\\n')\n",
    "\n",
    "outfile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpr_data = pandas.read_csv(processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['STRING_IDs', 'Protein_Ontology_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} string-protein ontology edges'.format(edge_count=len(stpr_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniprot Accession-Protein Ontology <a class=\"anchor\" id=\"uniprotaccession-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Uniprot accession identifiers to Protein Ontology identifiers when creating the following edges:  \n",
    "- protein-gobp  \n",
    "- protein-gomf  \n",
    "- protein-gocc  \n",
    "- protein-complex  \n",
    "- protein-cofactor  \n",
    "- protein-catalyst \n",
    "- protein-reaction  \n",
    "- protein-pathway\n",
    "\n",
    "**Output:** [`UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/txp8tqdipzwus9p/UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt', 'w') as outfile:\n",
    "    for row in tqdm(master_map_list):\n",
    "        if 'uniprot_id' in row[0] and 'pro_id' in row[1]:\n",
    "            outfile.write(row[0].split('_')[-1] + '\\t' + row[1].split('_')[-1].replace(':', '_') + '\\n')\n",
    "\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uapr_data = pandas.read_csv(processed_data_location + 'UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['Uniprot_Accession_IDs', 'Protein_Ontology_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} uniprot accession-protein ontology edges'.format(edge_count=len(uapr_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uapr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Other Identifier Mapping <a class=\"anchor\" id=\"other-identifier-mapping\"></a>\n",
    "***\n",
    "* [ChEBI Identifiers](#mesh-chebi)  \n",
    "* [Human Protein Atlas Tissue and Cell Types](#hpa-uberon) \n",
    "* [Human Disease and Phenotype Identifiers](#disease-identifiers) \n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChEBI-MeSH Identifiers <a class=\"anchor\" id=\"mesh-chebi\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [mapping-mesh-to-chebi](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#mapping-mesh-identifiers-to-chebi-identifiers)  \n",
    "\n",
    "**Purpose:** Map MeSH identifiers to ChEBI identifiers when creating the following edges:  \n",
    "- chemical-gene  \n",
    "- chemical-disease\n",
    "\n",
    "**Dependencies:** This script assumes that the [`ncbo_rest_api.py`](https://github.com/callahantiff/PheKnowLator/blob/development/scripts/python/ncbo_rest_api.py) script was run and the data generated from this file was written to `./resources/processed_data/temp`. \n",
    "\n",
    "**Output:** [`MESH_CHEBI_MAP.txt`](https://www.dropbox.com/s/5nr87v5h6x8oc1b/MESH_CHEBI_MAP.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'MESH_CHEBI_MAP.txt', 'w') as out:\n",
    "    for filename in tqdm(glob.glob(processed_data_location + 'temp/*.txt')):\n",
    "        for row in list(filter(None, open(filename, 'r').read().split('\\n'))):\n",
    "            mesh = '_'.join(row.split('\\t')[0].split('/')[-2:])\n",
    "            chebi = row.split('\\t')[1].split('/')[-1]\n",
    "            out.write(mesh + '\\t' + chebi + '\\n')\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_data = pandas.read_csv(processed_data_location + 'MESH_CHEBI_MAP.txt',\n",
    "                          delimiter = '\\t',\n",
    "                          header=None,\n",
    "                          names=['MeSH_IDs', 'ChEBI_IDs'])\n",
    "\n",
    "print('There are {edge_count} MeSH-ChEBI edges'.format(edge_count=len(mc_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disease and Phenotype Identifiers <a class=\"anchor\" id=\"disease-identifiers\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [disgenet](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#disgenet)  \n",
    "\n",
    "**Purpose:** This script downloads the [disease_mappings.tsv](https://www.disgenet.org/static/disgenet_ap1/files/downloads/disease_mappings.tsv.gz) to map UMLS identifiers to Human Disease and Human Phenotype identifiers when creating the following edges:  \n",
    "- chemical-disease  \n",
    "- disease-phenotype\n",
    "\n",
    "**Output:**   \n",
    "- Human Disease Ontology Mappings ➞ [`DISEASE_DOID_MAP.txt`](https://www.dropbox.com/s/q30ferujl7k574j/DISEASE_DOID_MAP.txt?dl=1)  \n",
    "- Human Phenotype Ontology Mappings ➞ [`PHENOTYPE_HPO_MAP.txt`](https://www.dropbox.com/s/5ayl0c5qm7r4tdm/PHENOTYPE_HPO_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.disgenet.org/static/disgenet_ap1/files/downloads/disease_mappings.tsv.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_data = pandas.read_csv(unprocessed_data_location + 'disease_mappings.tsv',\n",
    "                               header = 0,\n",
    "                               delimiter = '|')\n",
    "\n",
    "disease_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Build Disease Identifier Dictionary_  \n",
    "In order to improve efficiency when mapping different disease terminology identifiers to the [Human Disease Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-disease-ontology) and [Human Phenotype Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-phenotype-ontology), we create a dictionary of disease identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dictionary\n",
    "disease_dict = {}\n",
    "\n",
    "for idx, row in tqdm(disease_data.iterrows(), total=disease_data.shape[0]):\n",
    "    if row['vocabulary'] == 'MSH':\n",
    "        mesh_finder(disease_data, row['code'], 'MESH:', disease_dict)\n",
    "    elif row['vocabulary'] == 'OMIM':\n",
    "        mesh_finder(disease_data, row['code'], 'OMIM:', disease_dict)\n",
    "    elif row['vocabulary'] == 'ORDO':\n",
    "        mesh_finder(disease_data, row['code'], 'ORPHA:', disease_dict)\n",
    "    elif row['diseaseId'] in disease_dict.keys():\n",
    "        if row['vocabulary'] == 'DO':\n",
    "            disease_dict[row['diseaseId']].append('DOID_' + row['code']) \n",
    "        if row['vocabulary'] == 'HPO':\n",
    "            disease_dict[row['diseaseId']].append(row['code'].replace('HP:', 'HP_'))\n",
    "    else:\n",
    "        if row['vocabulary'] == 'DO':\n",
    "            disease_dict[row['diseaseId']] = ['DOID_' + row['code']] \n",
    "        if row['vocabulary'] == 'HPO':\n",
    "            disease_dict[row['diseaseId']] = [row['code'].replace('HP:', 'HP_')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write Mapping Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'DISEASE_DOID_MAP.txt', 'w') as outfile1,open(processed_data_location + 'PHENOTYPE_HPO_MAP.txt', 'w') as outfile2:\n",
    "    for key, value in tqdm(disease_dict.items()):\n",
    "        for i in value:\n",
    "            # get diseases\n",
    "            if i.startswith('DOID_'): \n",
    "                outfile1.write(key.split(':')[-1] + '\\t' + i + '\\n')\n",
    "\n",
    "            # get phenotypes\n",
    "            if i.startswith('HP_'): \n",
    "                outfile2.write(key.split(':')[-1] + '\\t' + i + '\\n')\n",
    "\n",
    "outfile1.close()\n",
    "outfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Human Disease Ontology Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_data = pandas.read_csv(processed_data_location + 'DISEASE_DOID_MAP.txt',\n",
    "                           header = None,\n",
    "                           names=['Disease_IDs', 'DOID_IDs'],\n",
    "                           delimiter = '\\t')\n",
    "\n",
    "print('There are {} disease-DOID edges'.format(len(dis_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Human Phenotype Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_data = pandas.read_csv(processed_data_location + 'PHENOTYPE_HPO_MAP.txt',\n",
    "                          header = None,\n",
    "                          names=['Disease_IDs', 'HP_IDs'],\n",
    "                          delimiter = '\\t')\n",
    "\n",
    "print('There are {} phenotype-HPO edges'.format(len(hp_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Protein Atlas/GTEx Tissue/Cells - UBERON + Cell Ontology + Cell Line Ontology <a class=\"anchor\" id=\"hpa-uberon\"></a>\n",
    "\n",
    "**Data Source Wiki Page:**  \n",
    "- [human-protein-atlas](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#human-protein-atlas) \n",
    "- [genotype-tissue-expression-project](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#the-genotype-tissue-expression-gtex-project)  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Purpose:** Downloads a query for cell, tissue, and blood types with overexpressed protein-coding genes in the human proteome ([`proteinatlas_search.tsv`](https://www.proteinatlas.org/api/search_download.php?search=&columns=g,eg,up,pe,rnatsm,rnaclsm,rnacasm,rnabrsm,rnabcsm,rnablsm,scl,t_RNA_adipose_tissue,t_RNA_adrenal_gland,t_RNA_amygdala,t_RNA_appendix,t_RNA_basal_ganglia,t_RNA_bone_marrow,t_RNA_breast,t_RNA_cerebellum,t_RNA_cerebral_cortex,t_RNA_cervix,_uterine,t_RNA_colon,t_RNA_corpus_callosum,t_RNA_ductus_deferens,t_RNA_duodenum,t_RNA_endometrium_1,t_RNA_epididymis,t_RNA_esophagus,t_RNA_fallopian_tube,t_RNA_gallbladder,t_RNA_heart_muscle,t_RNA_hippocampal_formation,t_RNA_hypothalamus,t_RNA_kidney,t_RNA_liver,t_RNA_lung,t_RNA_lymph_node,t_RNA_midbrain,t_RNA_olfactory_region,t_RNA_ovary,t_RNA_pancreas,t_RNA_parathyroid_gland,t_RNA_pituitary_gland,t_RNA_placenta,t_RNA_pons_and_medulla,t_RNA_prostate,t_RNA_rectum,t_RNA_retina,t_RNA_salivary_gland,t_RNA_seminal_vesicle,t_RNA_skeletal_muscle,t_RNA_skin_1,t_RNA_small_intestine,t_RNA_smooth_muscle,t_RNA_spinal_cord,t_RNA_spleen,t_RNA_stomach_1,t_RNA_testis,t_RNA_thalamus,t_RNA_thymus,t_RNA_thyroid_gland,t_RNA_tongue,t_RNA_tonsil,t_RNA_urinary_bladder,t_RNA_vagina,t_RNA_B-cells,t_RNA_dendritic_cells,t_RNA_granulocytes,t_RNA_monocytes,t_RNA_NK-cells,t_RNA_T-cells,t_RNA_total_PBMC,cell_RNA_A-431,cell_RNA_A549,cell_RNA_AF22,cell_RNA_AN3-CA,cell_RNA_ASC_diff,cell_RNA_ASC_TERT1,cell_RNA_BEWO,cell_RNA_BJ,cell_RNA_BJ_hTERT+,cell_RNA_BJ_hTERT+_SV40_Large_T+,cell_RNA_BJ_hTERT+_SV40_Large_T+_RasG12V,cell_RNA_CACO-2,cell_RNA_CAPAN-2,cell_RNA_Daudi,cell_RNA_EFO-21,cell_RNA_fHDF/TERT166,cell_RNA_HaCaT,cell_RNA_HAP1,cell_RNA_HBEC3-KT,cell_RNA_HBF_TERT88,cell_RNA_HDLM-2,cell_RNA_HEK_293,cell_RNA_HEL,cell_RNA_HeLa,cell_RNA_Hep_G2,cell_RNA_HHSteC,cell_RNA_HL-60,cell_RNA_HMC-1,cell_RNA_HSkMC,cell_RNA_hTCEpi,cell_RNA_hTEC/SVTERT24-B,cell_RNA_hTERT-HME1,cell_RNA_HUVEC_TERT2,cell_RNA_K-562,cell_RNA_Karpas-707,cell_RNA_LHCN-M2,cell_RNA_MCF7,cell_RNA_MOLT-4,cell_RNA_NB-4,cell_RNA_NTERA-2,cell_RNA_PC-3,cell_RNA_REH,cell_RNA_RH-30,cell_RNA_RPMI-8226,cell_RNA_RPTEC_TERT1,cell_RNA_RT4,cell_RNA_SCLC-21H,cell_RNA_SH-SY5Y,cell_RNA_SiHa,cell_RNA_SK-BR-3,cell_RNA_SK-MEL-30,cell_RNA_T-47d,cell_RNA_THP-1,cell_RNA_TIME,cell_RNA_U-138_MG,cell_RNA_U-2_OS,cell_RNA_U-2197,cell_RNA_U-251_MG,cell_RNA_U-266/70,cell_RNA_U-266/84,cell_RNA_U-698,cell_RNA_U-87_MG,cell_RNA_U-937,cell_RNA_WM-115,blood_RNA_basophil,blood_RNA_classical_monocyte,blood_RNA_eosinophil,blood_RNA_gdT-cell,blood_RNA_intermediate_monocyte,blood_RNA_MAIT_T-cell,blood_RNA_memory_B-cell,blood_RNA_memory_CD4_T-cell,blood_RNA_memory_CD8_T-cell,blood_RNA_myeloid_DC,blood_RNA_naive_B-cell,blood_RNA_naive_CD4_T-cell,blood_RNA_naive_CD8_T-cell,blood_RNA_neutrophil,blood_RNA_NK-cell,blood_RNA_non-classical_monocyte,blood_RNA_plasmacytoid_DC,blood_RNA_T-reg,blood_RNA_total_PBMC,brain_RNA_amygdala,brain_RNA_basal_ganglia,brain_RNA_cerebellum,brain_RNA_cerebral_cortex,brain_RNA_hippocampal_formation,brain_RNA_hypothalamus,brain_RNA_midbrain,brain_RNA_olfactory_region,brain_RNA_pons_and_medulla,brain_RNA_thalamus&format=tsv)) and median gene-level TPM by tissue for all genes that are not protein-coding ([`GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct`](https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz)) in order to create mappings between cell and tissue type strings to the Uber-Anatomy, Cell Ontology, and Cell Line Ontology concepts (see [human-protein-atlas](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-protein-atlas) for details on the mapping process). The mappings are then used to create the following edge types:  \n",
    "- rna-cell line  \n",
    "- rna-tissue type   \n",
    "- protein-cell line  \n",
    "- protein-tissue type  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Output:**  \n",
    "- All HPA tissue and cell type strings ➞ [`HPA_tissues.txt`](https://www.dropbox.com/s/m0spn8h1l8kxb61/HPA_tissues.txt?dl=1)  \n",
    "- Mapping HPA strings to ontology concepts (documentation) ➞ [`zooma_tissue_cell_mapping_04JAN2020.xlsx`](https://www.dropbox.com/s/u7elnc056zxypc6/HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt?dl=1)  \n",
    "- Final HPA-ontology mappings ➞ [`HPA_GTEx_TISSUE_CELL_MAP.txt`](https://www.dropbox.com/s/snzdwv1cvs0v9pp/HPA_GTEx_TISSUE_CELL_MAP.txt?dl=1)\n",
    "- HPA Edges ➞ [`HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt`](https://www.dropbox.com/s/u7elnc056zxypc6/HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human Protein Atlas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Download Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.proteinatlas.org/api/search_download.php?search=&columns=g,eg,up,pe,rnatsm,rnaclsm,rnacasm,rnabrsm,rnabcsm,rnablsm,scl,t_RNA_adipose_tissue,t_RNA_adrenal_gland,t_RNA_amygdala,t_RNA_appendix,t_RNA_basal_ganglia,t_RNA_bone_marrow,t_RNA_breast,t_RNA_cerebellum,t_RNA_cerebral_cortex,t_RNA_cervix,_uterine,t_RNA_colon,t_RNA_corpus_callosum,t_RNA_ductus_deferens,t_RNA_duodenum,t_RNA_endometrium_1,t_RNA_epididymis,t_RNA_esophagus,t_RNA_fallopian_tube,t_RNA_gallbladder,t_RNA_heart_muscle,t_RNA_hippocampal_formation,t_RNA_hypothalamus,t_RNA_kidney,t_RNA_liver,t_RNA_lung,t_RNA_lymph_node,t_RNA_midbrain,t_RNA_olfactory_region,t_RNA_ovary,t_RNA_pancreas,t_RNA_parathyroid_gland,t_RNA_pituitary_gland,t_RNA_placenta,t_RNA_pons_and_medulla,t_RNA_prostate,t_RNA_rectum,t_RNA_retina,t_RNA_salivary_gland,t_RNA_seminal_vesicle,t_RNA_skeletal_muscle,t_RNA_skin_1,t_RNA_small_intestine,t_RNA_smooth_muscle,t_RNA_spinal_cord,t_RNA_spleen,t_RNA_stomach_1,t_RNA_testis,t_RNA_thalamus,t_RNA_thymus,t_RNA_thyroid_gland,t_RNA_tongue,t_RNA_tonsil,t_RNA_urinary_bladder,t_RNA_vagina,t_RNA_B-cells,t_RNA_dendritic_cells,t_RNA_granulocytes,t_RNA_monocytes,t_RNA_NK-cells,t_RNA_T-cells,t_RNA_total_PBMC,cell_RNA_A-431,cell_RNA_A549,cell_RNA_AF22,cell_RNA_AN3-CA,cell_RNA_ASC_diff,cell_RNA_ASC_TERT1,cell_RNA_BEWO,cell_RNA_BJ,cell_RNA_BJ_hTERT+,cell_RNA_BJ_hTERT+_SV40_Large_T+,cell_RNA_BJ_hTERT+_SV40_Large_T+_RasG12V,cell_RNA_CACO-2,cell_RNA_CAPAN-2,cell_RNA_Daudi,cell_RNA_EFO-21,cell_RNA_fHDF/TERT166,cell_RNA_HaCaT,cell_RNA_HAP1,cell_RNA_HBEC3-KT,cell_RNA_HBF_TERT88,cell_RNA_HDLM-2,cell_RNA_HEK_293,cell_RNA_HEL,cell_RNA_HeLa,cell_RNA_Hep_G2,cell_RNA_HHSteC,cell_RNA_HL-60,cell_RNA_HMC-1,cell_RNA_HSkMC,cell_RNA_hTCEpi,cell_RNA_hTEC/SVTERT24-B,cell_RNA_hTERT-HME1,cell_RNA_HUVEC_TERT2,cell_RNA_K-562,cell_RNA_Karpas-707,cell_RNA_LHCN-M2,cell_RNA_MCF7,cell_RNA_MOLT-4,cell_RNA_NB-4,cell_RNA_NTERA-2,cell_RNA_PC-3,cell_RNA_REH,cell_RNA_RH-30,cell_RNA_RPMI-8226,cell_RNA_RPTEC_TERT1,cell_RNA_RT4,cell_RNA_SCLC-21H,cell_RNA_SH-SY5Y,cell_RNA_SiHa,cell_RNA_SK-BR-3,cell_RNA_SK-MEL-30,cell_RNA_T-47d,cell_RNA_THP-1,cell_RNA_TIME,cell_RNA_U-138_MG,cell_RNA_U-2_OS,cell_RNA_U-2197,cell_RNA_U-251_MG,cell_RNA_U-266/70,cell_RNA_U-266/84,cell_RNA_U-698,cell_RNA_U-87_MG,cell_RNA_U-937,cell_RNA_WM-115,blood_RNA_basophil,blood_RNA_classical_monocyte,blood_RNA_eosinophil,blood_RNA_gdT-cell,blood_RNA_intermediate_monocyte,blood_RNA_MAIT_T-cell,blood_RNA_memory_B-cell,blood_RNA_memory_CD4_T-cell,blood_RNA_memory_CD8_T-cell,blood_RNA_myeloid_DC,blood_RNA_naive_B-cell,blood_RNA_naive_CD4_T-cell,blood_RNA_naive_CD8_T-cell,blood_RNA_neutrophil,blood_RNA_NK-cell,blood_RNA_non-classical_monocyte,blood_RNA_plasmacytoid_DC,blood_RNA_T-reg,blood_RNA_total_PBMC,brain_RNA_amygdala,brain_RNA_basal_ganglia,brain_RNA_cerebellum,brain_RNA_cerebral_cortex,brain_RNA_hippocampal_formation,brain_RNA_hypothalamus,brain_RNA_midbrain,brain_RNA_olfactory_region,brain_RNA_pons_and_medulla,brain_RNA_thalamus&format=tsv'\n",
    "data_downloader(url, unprocessed_data_location, 'proteinatlas_search.tsv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Load Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa = pandas.read_csv(unprocessed_data_location + 'proteinatlas_search.tsv',\n",
    "                      header = 0,\n",
    "                      delimiter = '\\t')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "hpa.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Identify HPA Terms Needing Mapping_  \n",
    "To expedite the mapping process, all HPA tissues, cells, cell lines, and fluid types are extracted from the HPA data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve terms to map\n",
    "terms_to_map = list(hpa.columns)\n",
    "\n",
    "# write results\n",
    "with open(unprocessed_data_location + 'HPA_tissues.txt', 'w') as outfile:\n",
    "    for x in tqdm(terms_to_map):\n",
    "        if x.endswith('[NX]'):\n",
    "            term = x.split('RNA - ')[-1].split(' [NX]')[:-1][0]\n",
    "            outfile.write(term + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genotype-Tissue Expression Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Download Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Load Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtex = pandas.read_csv(unprocessed_data_location + 'GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct',\n",
    "                      header = 0,\n",
    "                      skiprows=2,\n",
    "                      delimiter = '\\t')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "gtex.fillna('None', inplace=True)\n",
    "\n",
    "# # set index of ensembl ids as column 'Ensembl'\n",
    "# gtex['Ensembl'] = gtex.index\n",
    "\n",
    "# remove identifier type, which appears after '.'\n",
    "gtex['Name'].replace('(\\..*)','', inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Mapping Data**   \n",
    "Import the tissues, cells, cell lines, and fluids that we externally mapped from HPA and GTEx data to [UBERON](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#uber-anatomy-ontology), the [Cell Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#cell-ontology), and the [Cell Line Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#cell-line-ontology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data = pandas.read_excel(open(unprocessed_data_location + 'zooma_tissue_cell_mapping_04JAN2020.xlsx', 'rb'),\n",
    "                                 sheet_name='Concept_Mapping - 04JAN2020',\n",
    "                                 header=0)\n",
    "\n",
    "# convert NaN to None\n",
    "mapping_data.fillna('None', inplace=True)\n",
    "\n",
    "# preview data\n",
    "mapping_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write HPA and GTEx Mapping Data_  \n",
    "The HPA and GTEx mapping data is written locally so that it can be used by the `PheKnowLator` algorithm when creating the knowledge graph edge lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'HPA_GTEx_TISSUE_CELL_MAP.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(mapping_data.iterrows(), total=mapping_data.shape[0]):\n",
    "        if row['UBERON ID'] != 'None':\n",
    "            outfile.write(str(row['ORIGINAL TERM']).strip() + '\\t' + str(row['UBERON ID']).strip() + '\\n')\n",
    "        if row['CL ID'] != 'None':\n",
    "            outfile.write(str(row['ORIGINAL TERM']).strip() + '\\t' + str(row['CL ID']).strip() + '\\n')\n",
    "        if row['CLO ID'] != 'None':\n",
    "            outfile.write(str(row['ORIGINAL TERM']).strip() + '\\t' + str(row['CLO ID']).strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data = pandas.read_csv(processed_data_location + 'HPA_GTEx_TISSUE_CELL_MAP.txt',\n",
    "                               header = None,\n",
    "                               names=['TISSUE_CELL_TERM', 'ONTOLOGY_IDs'],\n",
    "                               delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Edge Data Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Human Protein Atlas_  \n",
    "The `HPA` data looped over and reformatted such all all tissue, cell, cell lines, and fluid types are stored as a nested list. As shown in the code chunk, you will see that the anatomy type is specified as an item in the list according to its type. This is done in order to make mapping more efficient while building the knowledge graph edge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_results = []\n",
    "\n",
    "for idx, row in tqdm(hpa.iterrows(), total=hpa.shape[0]):\n",
    "    if row['RNA tissue specific NX'] != 'None':\n",
    "        for x in row['RNA tissue specific NX'].split(';'):\n",
    "            hpa_results += [[row['Ensembl'], row['Gene'], row['Uniprot'], row['Evidence'], 'anatomy', x.split(':')[0]]]\n",
    "\n",
    "    if row['RNA cell line specific NX'] != 'None':\n",
    "        for x in row['RNA cell line specific NX'].split(';'):\n",
    "            hpa_results += [[row['Ensembl'], row['Gene'], row['Uniprot'], row['Evidence'], 'cell line', x.split(':')[0]]]\n",
    "\n",
    "    if row['RNA brain regional specific NX'] != 'None':\n",
    "        for x in row['RNA brain regional specific NX'].split(';'):\n",
    "            hpa_results += [[row['Ensembl'], row['Gene'], row['Uniprot'], row['Evidence'], 'anatomy', x.split(':')[0]]]\n",
    "\n",
    "    if row['RNA blood cell specific NX'] != 'None':\n",
    "        for x in row['RNA blood cell specific NX'].split(';'):\n",
    "            hpa_results += [[row['Ensembl'], row['Gene'], row['Uniprot'], row['Evidence'], 'anatomy', x.split(':')[0]]]\n",
    "\n",
    "    if row['RNA blood lineage specific NX'] != 'None':\n",
    "        for x in row['RNA blood lineage specific NX'].split(';'):\n",
    "            hpa_results += [[row['Ensembl'], row['Gene'], row['Uniprot'], row['Evidence'], 'anatomy', x.split(':')[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Genotype-Tissue Expression Project_  \n",
    "The `GTEx` edge data is created by first filtering out all _protein-coding_ genes that appear in the `HPA` cell transcriptome data set. Once filter so that we are only left noncoding genes, we perform an additional filtering step to only add genes and their corresponding tissue, cell, or fluid, if the median expression is `>= 1.0`. The `GTEx` is formatted such all all tissue, cell, and fluid types occur as their own column and all unique genes occur as a row, thus the expression filtering step is performed while also reformatting the file. The genes and tissues/cells/fluids that meet criteria are stored as a nested list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of hpa protein-coding genes\n",
    "hpa_genes = list(hpa['Ensembl'].drop_duplicates(keep='first', inplace=False))\n",
    "\n",
    "# remove rows that contain protein coding genes\n",
    "gtex = gtex.loc[gtex['Name'].apply(lambda x: x not in hpa_genes)]\n",
    "\n",
    "# loop over data and re-organize - only keep results with tpm >= 1 and if gene symbol is not a protein-coding gene\n",
    "gtex_results = []\n",
    "\n",
    "for idx, row in tqdm(gtex.iterrows(), total=gtex.shape[0]):    \n",
    "    for col in list(gtex.columns)[2:]:\n",
    "        if row[col] >= 1.0:           \n",
    "            gtex_results += [[row['Name'], row['Description'], 'None', 'Evidence at transcript level', 'cell line' if 'Cells' in col else 'anatomy', col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write Results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt', 'w') as outfile:\n",
    "    for res in tqdm(hpa_results + gtex_results):\n",
    "        outfile.write(str(res[0]) + '\\t' + str(res[1]) + '\\t' + str(res[2]) + '\\t' + str(res[3]) + '\\t' + str(res[4]) + '\\t' + str(res[5]) + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_edges = pandas.read_csv(processed_data_location + 'HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt',\n",
    "                           header = None,\n",
    "                           names=['Ensembl_IDs', 'Gene_Symbols', 'Uniport_IDs', 'Evidence', 'Anatomy_Type', 'Anatomy'],\n",
    "                           low_memory=False,\n",
    "                           delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} edges'.format(edge_count=len(hpa_edges)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_edges.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### CREATE EDGE DATASETS  <a class=\"anchor\" id=\"create-edge-datasets\"></a>\n",
    "***\n",
    "***\n",
    "\n",
    "### Ontologies  <a class=\"anchor\" id=\"ontologies\"></a>\n",
    "***\n",
    "- [Protein Ontology](#protein-ontology)  \n",
    "- [Relations Ontology](#relations-ontology)  \n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein Ontology <a class=\"anchor\" id=\"protein-ontology\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [protein-ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-phenotype-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the [pr.owl](http://purl.obolibrary.org/obo/pr.owl) file from [ProConsortium.org](https://proconsortium.org/) in order to create a version of the ontology that contains only human proteins. This is achieved by performing forward and reverse breadth first search over all proteins which are `owl:subClassOf` [Homo sapiens protein](https://proconsortium.org/app/entry/PR%3A000029067/).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Output:**  \n",
    "- Human Protein Ontology ➞ [`human_pro.owl`](https://www.dropbox.com/s/jw8jksgnqbcz9sm/human_pro.owl?dl=1)\n",
    "- Classified Human Protein Ontology (Hermit) ➞ [`human_pro_closed.owl`](https://www.dropbox.com/s/6ux85agl95ja3wx/human_pro_closed.owl?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://purl.obolibrary.org/obo/pr.owl'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ontology as graph (the ontology is large so this takes ~60 minutes) - 11,757,623 edges on 12/18/2019\n",
    "graph = Graph()\n",
    "graph.parse(unprocessed_data_location + 'pr.owl')\n",
    "\n",
    "print('There are {} edges in the ontology'.format(len(graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Convert Ontology to Directed MulitGraph_  \n",
    "In order to create a version of the ontology which includes all relevant human edges, we need to first convert the KG to a [directed multigraph](https://networkx.github.io/documentation/stable/reference/classes/multidigraph.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert RDF graph to multidigraph (the ontology is large so this takes ~45 minutes)\n",
    "networkx_mdg = rdflib_to_networkx_multidigraph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Identify Human Proteins_   \n",
    "A list of human proteins is obtained by querying the ontology to return all ontology classes `only_in_taxon some Homo sapiens`. To expedite the query time, the following SPARQL query is run from the [ProConsortium](https://proconsortium.org/pro_sparql.shtml) SPARQL endpoint: \n",
    "\n",
    "```SPARQL\n",
    "PREFIX obo: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT ?PRO_term\n",
    "FROM <http://purl.obolibrary.org/obo/pr>\n",
    "WHERE {\n",
    "       ?PRO_term rdf:type owl:Class .\n",
    "       ?PRO_term rdfs:subClassOf ?restriction .\n",
    "       ?restriction owl:onProperty obo:RO_0002160 .\n",
    "       ?restriction owl:someValuesFrom obo:NCBITaxon_9606 .\n",
    "\n",
    "       # use this to filter-out things like hgnc ids\n",
    "       FILTER (regex(?PRO_term,\"http://purl.obolibrary.org/obo/*\")) .\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data - pro classes only_in_taxon some Homo sapiens (61,064 classes on 12/18/2019)\n",
    "url = 'http://sparql.proconsortium.org/virtuoso/sparql?query=PREFIX+obo%3A+%3Chttp%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F%3E%0D%0ASELECT+%3FPRO_term%0D%0AFROM+%3Chttp%3A%2F%2Fpurl.obolibrary.org%2Fobo%2Fpr%3E%0D%0AWHERE%0D%0A%7B%0D%0A+++%3FPRO_term+rdf%3Atype+owl%3AClass+.%0D%0A+++%3FPRO_term+rdfs%3AsubClassOf+%3Frestriction+.%0D%0A+++%3Frestriction+owl%3AonProperty+obo%3ARO_0002160+.%0D%0A+++%3Frestriction+owl%3AsomeValuesFrom+obo%3ANCBITaxon_9606+.%0D%0A%0D%0A+++FILTER+%28regex%28%3FPRO_term%2C%22http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F*%22%29%29+.%0D%0A%0D%0A%7D%0D%0A&format=text%2Fhtml&debug='\n",
    "html = requests.get(url, allow_redirects=True).content\n",
    "\n",
    "# extract data from html table\n",
    "df_list = pandas.read_html(html)\n",
    "human_pro_classes = list(df_list[-1]['PRO_term'])\n",
    "\n",
    "print('There are {protein_count} human classes in the PRO ontology'.format(protein_count=len(human_pro_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Construct Human PRO_   \n",
    "Now that we have all of the paths from the original graph that are relevant to humans, we can construct a human-only version of the PRotein ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new graph using bfs paths\n",
    "human_pro_graph = Graph()\n",
    "human_networkx_mdg = networkx.MultiDiGraph()\n",
    "\n",
    "for node in tqdm(human_pro_classes):\n",
    "    forward = list(networkx.edge_bfs(networkx_mdg, URIRef(node), orientation='original'))\n",
    "    reverse = list(networkx.edge_bfs(networkx_mdg, URIRef(node), orientation='reverse'))\n",
    "    \n",
    "    # add edges from forward and reverse bfs paths\n",
    "    for path in forward + reverse:\n",
    "        human_pro_graph.add((path[0], path[2], path[1]))\n",
    "        human_networkx_mdg.add_edge(path[0], path[1], path[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the constructed ontology only has 1 component\n",
    "networkx.number_connected_components(human_networkx_mdg.to_undirected())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered ontology\n",
    "human_pro_graph.serialize(destination=unprocessed_data_location + 'human_pro.owl', format='xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Classify Ontology_  \n",
    "To ensure that we have correclty built the new ontology, we run the hermit reasoner over it to ensure that there are no incomplete triples or inconsistent classes. In order to do this, we will call the reasoner using [OWLTools](https://github.com/owlcollab/owltools), which this script assumes has already been downloaded to the `../resources/lib` directory. The following arguments are then called to run the reasoner (from the command line):  \n",
    "\n",
    "```bash\n",
    "./resources/lib/owltools ./resources/unprocessed_data/human_pro_filtered.owl --reasoner hermit --run-reasoner --assert-implied -o ./resources/processed_data/human_pro_closed.owl\n",
    "```\n",
    "\n",
    "_**Note.** This step takes around 30-45 minutes to run. When run from the command line the reasoner determined that the ontology was consistent and 174 new axioms were inferrred (12/18/2019)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run reasoner -- RUN FROM COMMAND LINE NOT HERE\n",
    "# subprocess.run(['../../resources/lib/owltools',\n",
    "#                 '../../resources/unprocessed_data/human_pro_filtered.owl',\n",
    "#                 '--reasoner hermit',\n",
    "#                 '--run-reasoner',\n",
    "#                 '--assert-implied',\n",
    "#                 '--list-unsatisfiable',\n",
    "#                 '-o ./resources/processed_data/human_pro_closed.owl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Examine Cleaned Human PRO_  \n",
    "Once we have cleaned the ontology we can get counts of components, nodes, edges, and then write the cleaned graph to the `../../resources/processed_data` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of connected components\n",
    "pro_human_graph = Graph()\n",
    "pro_human_graph.parse(processed_data_location + 'human_pro_closed.owl')\n",
    "\n",
    "# get node and edge count\n",
    "edge_count = len(human_pro_graph)\n",
    "node_count = len(set([str(node) for edge in list(human_pro_graph) for node in edge[0::2]]))\n",
    "\n",
    "print('\\n The classified, filtered Human version of PRO contains {node} nodes and {edge} edges\\n'.format(node=node_count, edge=edge_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations Ontology <a class=\"anchor\" id=\"relations-ontology\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [RO](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#relation-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the [ro.owl](http://purl.obolibrary.org/obo/ro.owl) file from [obofoundry.org](http://www.obofoundry.org/) in order to obtain all `ObjectProperties` and their inverse relations.  \n",
    "\n",
    "**Output:** \n",
    "- Relations and Inverse Relations ➞ [`INVERSE_RELATIONS.txt`](https://www.dropbox.com/s/sd8qlib8f6gqyz4/INVERSE_RELATIONS.txt?dl=1)\n",
    "- Relations and Labels ➞ [`RELATIONS_LABELS.txt`](https://www.dropbox.com/s/k2hm9p0r8l9ecj3/RELATIONS_LABELS.txt?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Download Ontology_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ontology using subprocess and OWLTOOLS in order to get the ontology and its imported ontologies\n",
    "subprocess.check_call(['./resources/lib/owltools',\n",
    "                       'http://purl.obolibrary.org/obo/ro.owl',\n",
    "                       '--merge-import-closure',\n",
    "                       '-o',\n",
    "                       unprocessed_data_location + 'ro_with_imports.owl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Load Ontology to RDFLib Graph_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_graph = Graph()\n",
    "ro_graph.parse(unprocessed_data_location + 'ro_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology (date:{})'.format(len(ro_graph), datetime.datetime.now().strftime('%m/%d/%Y')))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Relations and Inverse Relations**  \n",
    "Identify all relations and their inverse relations using the `owl:inverseOf` property. To make it easier to look up the inverse relations, each pair is listed twice, for example:  \n",
    "- [location of](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001015) `owl:inverseOf` [located in](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001025)  \n",
    "- [located in](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001025) `owl:inverseOf` [location of](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(relations_data_location + 'INVERSE_RELATIONS.txt', 'w') as outfile:\n",
    "    \n",
    "    # write column names\n",
    "    outfile.write('Relation' + '\\t' + 'Inverse_Relation' + '\\n')\n",
    "\n",
    "    # find inverse relations\n",
    "    for s, p, o in tqdm(ro_graph):\n",
    "        if 'owl#inverseOf' in str(p):\n",
    "            if 'RO' in str(s) and 'RO' in str(o):\n",
    "                outfile.write(str(s.split('/')[-1]) + '\\t' + str(o.split('/')[-1]) + '\\n')\n",
    "                outfile.write(str(o.split('/')[-1]) + '\\t' + str(s.split('/')[-1]) + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data = pandas.read_csv(relations_data_location + 'INVERSE_RELATIONS.txt',\n",
    "                          header = 0,\n",
    "                          delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} RO Relations and Inverse Relations'.format(edge_count=len(ro_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Relations Labels**  \n",
    "Identify all relations and their labels for use when building the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ro_graph.query(\n",
    "    \"\"\"SELECT DISTINCT ?p ?p_label\n",
    "           WHERE {\n",
    "              ?p rdf:type owl:ObjectProperty .\n",
    "              ?p rdfs:label ?p_label . }\n",
    "           \"\"\", initNs={\"rdf\": 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "                        \"rdfs\": 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "                        \"owl\": 'http://www.w3.org/2002/07/owl#'})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to file\n",
    "with open(relations_data_location + 'RELATIONS_LABELS.txt', 'w') as outfile:\n",
    "    \n",
    "    # write column names\n",
    "    outfile.write('Relation' + '\\t' + 'Label' + '\\n')\n",
    "\n",
    "    for p, p_label in list(results):\n",
    "        outfile.write(str(p).split('/')[-1] + '\\t' + str(p_label) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data_label = pandas.read_csv(relations_data_location + 'RELATIONS_LABELS.txt',\n",
    "                                header = 0,\n",
    "                                delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} RO Relations and Labels'.format(edge_count=len(ro_data_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data_label.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Linked Data <a class=\"anchor\" id=\"linked-data\"></a>\n",
    "***\n",
    "* [Clinvar Variant-Diseases and Phenotypes](#clinvar-variant)\n",
    "* [Reactome Chemical-Complex Data](#reactome-chemical-complex)  \n",
    "* [Reactome Complex-Complex Data](#reactome-complex-complex)  \n",
    "* [Reactome Protein-Complex Data](#reactome-protein-complex)  \n",
    "* [Uniprot Protein-Cofactor and Protein-Catalyst](#uniprot-protein-cofactorcatalyst)  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinvar Variant-Diseases and Phenotypes <a class=\"anchor\" id=\"clinvar-variant\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Clinvar](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#clinvar)  \n",
    "\n",
    "**Purpose:** This script downloads the [variant_summary.txt](ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz) file from [CLinVar](https://www.ncbi.nlm.nih.gov/clinvar/) in order to create the following edges:  \n",
    "- gene-variant  \n",
    "- variant-disease  \n",
    "- variant-phenotype  \n",
    "\n",
    "**Output:** [`CLINVAR_VARIANT_GENE_DISEASE_PHENOTYPE_EDGES.txt`](https://www.dropbox.com/s/1doj3lj46ufgdpd/CLINVAR_VARIANT_GENE_DISEASE_PHENOTYPE_EDGES.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinvar_data = pandas.read_csv(unprocessed_data_location + 'variant_summary.txt',\n",
    "                               header = 0,\n",
    "                               delimiter = '\\t',\n",
    "                               low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with 'None'\n",
    "clinvar_data.fillna('None', inplace=True)\n",
    "\n",
    "# explode nested data\n",
    "explode_df_clinvar = explode(clinvar_data.copy(), ['PhenotypeIDS'], ';')\n",
    "explode_df_clinvar = explode(explode_df_clinvar.copy(), ['PhenotypeIDS'], ',')\n",
    "\n",
    "# edit column formatting\n",
    "explode_df_clinvar['PhenotypeIDS'].replace('Orphanet:ORPHA','ORPHA:', inplace=True, regex=True)\n",
    "explode_df_clinvar['PhenotypeIDS'].replace('Human Phenotype Ontology:HP:','HP_', inplace=True, regex=True)\n",
    "\n",
    "# write data\n",
    "explode_df_clinvar.to_csv(processed_data_location + 'CLINVAR_VARIANT_GENE_DISEASE_PHENOTYPE_EDGES.txt',\n",
    "                          header = True,\n",
    "                          sep='\\t',\n",
    "                          encoding='utf-8',\n",
    "                          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {edge_count} variant edges'.format(edge_count=len(explode_df_clinvar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview data\n",
    "explode_df_clinvar.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reactome Chemical-Complex Data <a class=\"anchor\" id=\"reactome-chemical-complex\"></a>\n",
    "\n",
    "**Data Souurce Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#reactome-pathway-database)  \n",
    "\n",
    "**Purpose:** This script downloads the [ComplexParticipantsPubMedIdentifiers_human.txt](https://reactome.org/download/current/ComplexParticipantsPubMedIdentifiers_human.txt) file from [Reactome](https://reactome.orgt) in order to create the following edges:  \n",
    "- chemical-complex  \n",
    "\n",
    "**Output:** [`REACTOME_CHEMICAL_COMPLEX.txt`](https://www.dropbox.com/s/qoetjt0vfy6qb3y/REACTOME_CHEMICAL_COMPLEX.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(unprocessed_data_location + 'ComplexParticipantsPubMedIdentifiers_human.txt').readlines()\n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'REACTOME_CHEMICAL_COMPLEX.txt', 'w') as outfile:\n",
    "    for line in tqdm(data[1:]):\n",
    "        row = line.split('\\t')\n",
    "        \n",
    "        if (row[0].strip().startswith('R-HSA') or row[0].strip().startswith('R-ALL')):\n",
    "            # find all proteins in a complex\n",
    "            for x in row[2].split('|'):\n",
    "                if x.startswith('chebi:'):            \n",
    "                    outfile.write(x.replace('chebi:', 'CHEBI_') + '\\t' + row[0].strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1_data = pandas.read_csv(processed_data_location + 'REACTOME_CHEMICAL_COMPLEX.txt',\n",
    "                           header = None,\n",
    "                           names=['CHEBI_IDs', 'Reactome_IDs'],\n",
    "                           delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} chemical-complex edges'.format(edge_count=len(cc1_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reactome Complex-Complex Data <a class=\"anchor\" id=\"reactome-complex-complex\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#reactome-pathway-database)  \n",
    "\n",
    "**Purpose:** This script downloads the [ComplexParticipantsPubMedIdentifiers_human.txt](https://reactome.org/download/current/ComplexParticipantsPubMedIdentifiers_human.txt) file from [Reactome](https://reactome.orgt) in order to create the following edges:  \n",
    "- complex-complex  \n",
    "\n",
    "**Output:** [`REACTOME_COMPLEX_COMPLEX.txt`](https://www.dropbox.com/s/sojaq8u3hwfw4jz/REACTOME_COMPLEX_COMPLEX.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(unprocessed_data_location + 'ComplexParticipantsPubMedIdentifiers_human.txt').readlines()\n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'REACTOME_COMPLEX_COMPLEX.txt', 'w') as outfile:\n",
    "    for line in tqdm(data[1:]):\n",
    "        row = line.split('\\t')\n",
    "        \n",
    "        if row[0].strip().startswith('R-HSA'):\n",
    "            for x in row[3].split('|'):\n",
    "                if x.startswith('R-HSA-') or x.startswith('R-ALL-'):            \n",
    "                    outfile.write(row[0].strip() + '\\t' + x.strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data = pandas.read_csv(processed_data_location + 'REACTOME_COMPLEX_COMPLEX.txt',\n",
    "                          header = None,\n",
    "                          names=['Reactome_Complex_u', 'Reactome_Complex_v'],\n",
    "                          delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} complex-complex edges'.format(edge_count=len(cc_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reactome Complex-Pathway Data <a class=\"anchor\" id=\"reactome-complex-pathway\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#reactome-pathway-database)  \n",
    "\n",
    "**Purpose:** This script downloads the [Complex_2_Pathway_human.txt](https://reactome.org/download/current/Complex_2_Pathway_human.txt) file from [Reactome](https://reactome.orgt) in order to create the following edges:  \n",
    "- complex-pathway  \n",
    "\n",
    "**Output:** [`REACTOME_COMPLEX_PATHWAY.txt`](https://www.dropbox.com/s/my03w16fjw7bt20/REACTOME_COMPLEX_PATHWAY.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://reactome.org/download/current/Complex_2_Pathway_human.txt'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "data = open(unprocessed_data_location + 'Complex_2_Pathway_human.txt').readlines()\n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'REACTOME_COMPLEX_PATHWAY.txt', 'w') as outfile:\n",
    "    for line in tqdm(data[1:]):\n",
    "        row = line.split('\\t')\n",
    "        if row[0].startswith('R-HSA-'):            \n",
    "            outfile.write(row[0].strip() + '\\t' + row[1].strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data = pandas.read_csv(processed_data_location + 'REACTOME_COMPLEX_PATHWAY.txt',\n",
    "                          header = None,\n",
    "                          names=['Reactome_Complex', 'Reactome_Pathway'],\n",
    "                          delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} complex-pathway edges'.format(edge_count=len(cp_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reactome Protein-Complex Data <a class=\"anchor\" id=\"reactome-protein-complex\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#reactome-pathway-database)  \n",
    "\n",
    "**Purpose:** This script downloads the [ComplexParticipantsPubMedIdentifiers_human.txt](https://reactome.org/download/current/ComplexParticipantsPubMedIdentifiers_human.txt) file from [Reactome](https://reactome.org) in order to create the following edges:  \n",
    "- protein-complex\n",
    "\n",
    "**Output:** [`REACTOME_PROTEIN_COMPLEX.txt`](https://www.dropbox.com/s/7meu0cdz1mrnsz7/REACTOME_PROTEIN_COMPLEX.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://reactome.org/download/current/ComplexParticipantsPubMedIdentifiers_human.txt'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "data = open(unprocessed_data_location + 'ComplexParticipantsPubMedIdentifiers_human.txt').readlines()\n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'REACTOME_PROTEIN_COMPLEX.txt', 'w') as outfile:\n",
    "    for line in tqdm(data):\n",
    "        row = line.split('\\t')\n",
    "        \n",
    "        if row[0].strip().startswith('R-HSA'):\n",
    "            # find all proteins in a complex\n",
    "            for x in row[2].split('|'):\n",
    "                if x.startswith('uniprot:'):            \n",
    "                    outfile.write(x.split(':')[-1].strip() + '\\t' + row[0].strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_data = pandas.read_csv(processed_data_location + 'REACTOME_PROTEIN_COMPLEX.txt',\n",
    "                       header = None,\n",
    "                       names=['Uniprot_Protein', 'Reactome_Complex'],\n",
    "                       delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} protein-complex edges'.format(edge_count=len(pc_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniprot  Protein-Cofactor and Protein-Catalyst <a class=\"anchor\" id=\"uniprot-protein-cofactorcatalyst\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Uniprot](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#uniprot-knowledgebase)  \n",
    "\n",
    "**Purpose:** This script downloads the [uniprot-cofactor-catalyst.tab](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#uniprot-knowledgebase) file from the [Uniprot Knowledge Base](https://www.uniprot.org) in order to create the following edges:  \n",
    "- protein-cofactor  \n",
    "- protein-catalyst  \n",
    "\n",
    "**Output:**  \n",
    "- protein-cofactor ➞ [`UNIPROT_PROTEIN_COFACTOR.txt`](https://www.dropbox.com/s/ij9t89botd8nmmj/UNIPROT_PROTEIN_COFACTOR.txt?dl=1)\n",
    "- protein-catalyst ➞ [`UNIPROT_PROTEIN_CATALYST.txt`](https://www.dropbox.com/s/pvopvs0iq8x3oq2/UNIPROT_PROTEIN_CATALYST.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.uniprot.org/uniprot/?query=&fil=organism%3A%22Homo%20sapiens%20(Human)%20%5B9606%5D%22&columns=id%2Centry%20name%2Creviewed%2Cdatabase(PRO)%2Cchebi(Cofactor)%2Cchebi(Catalytic%20activity)&format=tab'\n",
    "data_downloader(url, unprocessed_data_location, 'uniprot-cofactor-catalyst.tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(unprocessed_data_location + 'uniprot-cofactor-catalyst.tab').readlines()\n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'UNIPROT_PROTEIN_COFACTOR.txt', 'w') as outfile1, open(processed_data_location + 'UNIPROT_PROTEIN_CATALYST.txt', 'w') as outfile2:\n",
    "    for line in tqdm(data):\n",
    "\n",
    "        # get cofactors\n",
    "        if 'CHEBI' in line.split('\\t')[4]: \n",
    "            for i in line.split('\\t')[4].split(';'):\n",
    "                chebi = i.split('[')[-1].replace(']', '').replace(':', '_')\n",
    "                outfile1.write('PR_' + line.split('\\t')[3].strip(';') + '\\t' + chebi + '\\n')\n",
    "        \n",
    "        # get catalysts\n",
    "        if 'CHEBI' in line.split('\\t')[5]:       \n",
    "            for i in line.split('\\t')[5].split(';'):\n",
    "                chebi = i.split('[')[-1].replace(']', '').replace(':', '_')\n",
    "                outfile2.write('PR_' + line.split('\\t')[3].strip(';') + '\\t' + chebi + '\\n')\n",
    "\n",
    "outfile1.close()\n",
    "outfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Cofactor Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp1_data = pandas.read_csv(processed_data_location + 'UNIPROT_PROTEIN_COFACTOR.txt',\n",
    "                            header = None,\n",
    "                            names=['Protein_Ontology_IDs', 'CHEBI_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} protein-cofactor edges'.format(edge_count=len(pcp1_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp1_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Catalyst Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp2_data = pandas.read_csv(processed_data_location + 'UNIPROT_PROTEIN_CATALYST.txt',\n",
    "                            header = None,\n",
    "                            names=['Protein_Ontology_IDs', 'CHEBI_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} protein-catalyst edges'.format(edge_count=len(pcp2_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp2_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### INSTANCE METADATA <a class=\"anchor\" id=\"create-instance-metadata\"></a>\n",
    "***\n",
    "\n",
    "**Data Source Wiki Page:** [Dependencies](https://github.com/callahantiff/PheKnowLator/wiki/Dependencies/#node-metadata) \n",
    "\n",
    "<br>\n",
    "\n",
    "**Purpose:** The goal of this section is to obtain metadata for each instance data source used in the knowledge graph. To determine which of the edges contains instance data, the [`Master_Edge_List_Dict.json`](https://www.dropbox.com/s/4j0vrwx26dh8hd1/Master_Edge_List_Dict.json?dl=1) file is parsed and saved to a nested dictionary (see example below). \n",
    "\n",
    "```python\n",
    "{\n",
    "  'complex': {\n",
    "              'chemical-complex': [[node_1, node_2]...[node_n, node_m]],\n",
    "              'complex-complex':  [[node_1, node_2]...[node_n, node_m]],\n",
    "              'complex-pathway':  [[node_1, node_2]...[node_n, node_m]],\n",
    "              },\n",
    "     'gene': {\n",
    "                'chemical-gene':  [[node_1, node_2]...[node_n, node_m]],\n",
    "                 'gene-disease':  [[node_1, node_2]...[node_n, node_m]],\n",
    "              }\n",
    "}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Once this dictionary is created, each major data type (examples shown in the list below) will be processed. For **[`Release V2.0.0`](https://github.com/callahantiff/PheKnowLator/wiki/v2.0.0)**, the following are instance data and require the compiling of metadata:\n",
    "- [Genes](#gene-metadata)\n",
    "- [RNA](#rna-metadata)\n",
    "- [Pathways](#pathway-metadata)\n",
    "- [Complexes](#complex-metadata)\n",
    "- [Reactions](#reaction-metadata)\n",
    "- [Variants](#variant-metadata)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "____\n",
    "\n",
    "**Metadata:** The <u>metadata</u> we will gather includes:  \n",
    "\n",
    "| **Metadata Type** | **Definition** | **Example Node**  | **Example Node Metadata** | \n",
    "| :---: | :---: | :---: | :---: | \n",
    "| Label | The primary label or name for the node | `R-HSA-1006173` | \"CFH:Host cell surface\" |       \n",
    "| Description | A definition or other useful details about the node | `rs794727058` | This `germline` `single nucleotide variant (allele alteration: C➞T)` located on chromosome `5 (GRCh38: NC_000005.10, start/stop positions (126555930/126555930))` with `pathogenic` clinical significance and a last review date of `2/23/2015` (review status: `criteria provided, single submitter`). |        \n",
    "| Synonym | Alternative terms used for a node | `81399` | \"OR1-1, OR7-21\" |           \n",
    "\n",
    "<br>\n",
    "\n",
    "The metadata information will be used to create the following edges in the knowledge graph:  \n",
    "- **Label** ➞ node `rdfs:label`  \n",
    "- **Description** ➞ node `obo:IAO_0000115` description \n",
    "- **Synonyms** ➞ node `oboInOwl:hasExactSynonym` synonym \n",
    "\n",
    "<br>\n",
    "\n",
    "*<b>NOTE.</b> All node metadata datasets are written to the `node_data` directory. The algorithm will look for data in this directory and if it is not there, then no node metadata will be created.*\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Metadata Dictionaries\n",
    "***\n",
    "\n",
    "**Purpose:** To create the resources needed in order to create metadata dictionaries, which are in turn used to obtain metadata for instance data nodes. This process has the following steps:\n",
    "\n",
    "**1. [Identify Instance Data Nodes](#identify-instance-data-nodes):** In order to automatically obtain the list of edges that include an instance data source and their corresponding edge lists, the `Master_Edge_List_Dict.json` is read in and processed.  \n",
    "  - <u>Input Data</u>: [`Master_Edge_List_Dict.json`](./resources/Master_Edge_List_Dict.json)  \n",
    "\n",
    "<br>\n",
    "\n",
    "**2. [Generate Metadata Dictionaries](#generate-metadata-dictionaries):** In order to efficiently obtain metadata for the instance data nodes identified in _Step 1_, we first read in the data for each node type (i.e. genes, rna, complexes, pathways, reactions, and variants) and convert it into a dictionary. Then, each metadata dictionary is saved to a `master_metadata_dictionary`, keyed by node type.\n",
    "  - <u>Input Datasets</u>:  \n",
    "    - Genes ➞ [`Homo_sapiens.gene_info`](https://www.dropbox.com/s/f2nz5q6g46u0tth/Merged_gene_rna_protein_identifiers.json?dl=1)    \n",
    "    - RNA ➞ [`Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt`](https://www.dropbox.com/s/fiek6h5rowi7dh0/Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt?dl=1) \n",
    "    - Complexes ➞ [`reactome2py API`](https://github.com/reactome/reactome2py)  \n",
    "    - Pathways ➞ [`reactome2py API`](https://github.com/reactome/reactome2py)  \n",
    "    - Reactions ➞ [`reactome2py API`](https://github.com/reactome/reactome2py)  \n",
    "    - Variants ➞ [`variant_summary.txt.gz`](ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz)  \n",
    "\n",
    "<br>\n",
    "\n",
    "**3. [Write Metadata Files](#write-metadata-files):** The Instance data node dictionary from _Step 1_ and metadata dictionaries from _Step 2_ are used to write `.txt` files for all `edge-type` data included in the instance node dictionary.\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Instance Data Nodes  <a class=\"anchor\" id=\"identify-instance-data-nodes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data files for each edge type\n",
    "edge_data = json.load(open('./resources/Master_Edge_List_Dict.json', 'r'))\n",
    "edge_dict = {key:[edge_data[key]['data_type'], edge_data[key]['edge_list']] for key in edge_data.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort Data**  \n",
    "For all edges in the `edge_dict()` that include instance data, we create a new dictionary where each edge type is further organized by node from the edge type that references the instance data (e.g. from the `chemical-gene` edge type, the `gene` node references instance data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data files\n",
    "metadata_file_info = {}\n",
    "\n",
    "for edge in tqdm(edge_dict.keys()): \n",
    "    if 'instance' in edge_dict[edge][0]:\n",
    "        \n",
    "        # get instance type\n",
    "        inst_type = edge.split('-')[edge_dict[edge][0].split('-').index('instance')]\n",
    "        \n",
    "        # read in data\n",
    "        if inst_type in metadata_file_info.keys(): \n",
    "            metadata_file_info[inst_type][edge] = {}\n",
    "            metadata_file_info[inst_type][edge]['data'] = edge_dict[edge][1]\n",
    "            metadata_file_info[inst_type][edge]['instance_idx'] = edge_dict[edge][0]\n",
    "            \n",
    "        else:\n",
    "            metadata_file_info[inst_type] = {}\n",
    "            metadata_file_info[inst_type][edge] =  {}\n",
    "            metadata_file_info[inst_type][edge]['data'] =  edge_dict[edge][1]\n",
    "            metadata_file_info[inst_type][edge]['instance_idx'] = edge_dict[edge][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Metadata Dictionaries  <a class=\"anchor\" id=\"generate-metadata-dictionaries\"></a>\n",
    "In this step, the goal is to create a metadata dictionary for each node type that does not rely on API data. In this case, only the **Gene**, **RNA**, and **Variant** nodes require data that is not from an API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genes Metadata Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrez gene data\n",
    "entrez_gene_data = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.gene_info',\n",
    "                                   header = 0,\n",
    "                                   delimiter = '\\t',\n",
    "                                   low_memory=False)\n",
    "\n",
    "# remove all rows that are not human\n",
    "entrez_gene_data = entrez_gene_data.loc[entrez_gene_data['#tax_id'].apply(lambda x: x == 9606)]\n",
    "\n",
    "# replace NaN and '-' with 'None'\n",
    "entrez_gene_data.fillna('None', inplace=True)\n",
    "entrez_gene_data.replace('-','None', inplace=True, regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Create Gene Metadata Dictionary_  \n",
    "The nested dictionary of gene metadata is created by looping over the merged data described in the prior column. The `keys` of the dictionary are `Entrez gene identifiers` and the `values` are dictionaries for each metadata type: `symbol`, `description`, and `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata\n",
    "genes, label, description, synonym = [], [], [], []\n",
    "\n",
    "for idx, row in tqdm(entrez_gene_data.iterrows(), total=entrez_gene_data.shape[0]):\n",
    "    # node \n",
    "    if row['GeneID'] != 'None':\n",
    "        genes.append(row['GeneID'])\n",
    "    \n",
    "    # label -- only want metadata if there is a label\n",
    "    if row['Symbol'] != 'None' or row['Symbol'] != '':       \n",
    "        label.append(row['Symbol'])\n",
    "\n",
    "        # description        \n",
    "        if row['Full_name_from_nomenclature_authority'] != 'None' and row['type_of_gene'] != 'None' and row['chromosome'] != 'None' and row['map_location'] != 'None':\n",
    "        \n",
    "            description.append('{desc} is a {gene} gene that is located on chromosome {chrom} (map_location: {map_loc}).'.format(desc=row['Symbol'],\n",
    "                                                                                                                                 gene=row['type_of_gene'],\n",
    "                                                                                                                                 chrom=row['chromosome'],\n",
    "                                                                                                                                 map_loc=row['map_location']))\n",
    "            \n",
    "        else:\n",
    "            description.append('{desc} is a {gene} gene.'.format(desc=row['Symbol'], gene=row['type_of_gene']))\n",
    "        \n",
    "        # synonym        \n",
    "        if row['Synonyms'] != 'None' or row['Other_designations'] != 'None':\n",
    "            syns = '|'.join(set([x for x in (row['Synonyms'] + row['Other_designations']).split('|') if x != 'None' or x != '']))\n",
    "            synonym.append(syns)\n",
    "        else:\n",
    "            synonym.append('None')\n",
    "            \n",
    "    \n",
    "# combine into new data frame        \n",
    "gene_metadata_final = pandas.DataFrame(list(zip(genes, label, description, synonym)), columns =['ID', 'Label', 'Description', 'Synonym'])\n",
    "\n",
    "# make all variables string\n",
    "gene_metadata_final = gene_metadata_final.astype(str)\n",
    "\n",
    "# dedup\n",
    "gene_metadata_final.drop_duplicates(subset='ID', keep='first', inplace=True)\n",
    "\n",
    "# convert df to dictionary\n",
    "gene_metadata_final.set_index('ID', inplace=True)\n",
    "gene_metadata_dict = gene_metadata_final.to_dict('index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNA Metadata Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_gene_data = pandas.read_csv(processed_data_location + 'Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt',\n",
    "                                header = 0,\n",
    "                                delimiter = '\\t',\n",
    "                                low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Normal data preprocess and filtering steps are performed in order to prepare the data for the next step, which converts it to a metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows without identifiers\n",
    "rna_gene_data = rna_gene_data.loc[rna_gene_data['transcript_stable_id'].apply(lambda x: x != 'None')]\n",
    "\n",
    "# remove unneede columns\n",
    "rna_gene_data.drop(['ensembl_gene_id', 'symbol', 'protein_stable_id', 'uniprot_id',\n",
    "                    'entrez_id', 'hgnc_id', 'pro_id'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "rna_gene_data.drop_duplicates(subset=['transcript_stable_id', 'transcript_name', 'transcript_type_update'], keep='first', inplace=True)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "rna_gene_data.fillna('None', inplace=True)\n",
    "\n",
    "# get transcript_names\n",
    "trans_name = {k: g['transcript_name'].tolist() for k,g in rna_gene_data.groupby('transcript_stable_id')}\n",
    "\n",
    "# get trans type\n",
    "trans_type = {k: g['transcript_type_update'].tolist() for k,g in rna_gene_data.groupby('transcript_stable_id')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Create RNA Metadata Dictionary_  \n",
    "The nested dictionary of rna metadata is created by looping over the human merged gene, RNA, protein identifier data set ([`Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt`](https://www.dropbox.com/s/fiek6h5rowi7dh0/Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt?dl=1)). The `keys` of the dictionary are `Ensembl transcript identifiers` and the `values` are dictionaries for each metadata type: `symbol`, `description`, and `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata\n",
    "rna, label, description, synonym = [], [], [], []\n",
    "\n",
    "for node in tqdm(set(list(rna_gene_data['transcript_stable_id']))):\n",
    "    # node\n",
    "    rna.append(node)\n",
    "    \n",
    "    # label info\n",
    "    label_list = [x for x in trans_name[node] if x != 'None']\n",
    "    labels = label_list[0] if len(label_list) > 0 else 'None'\n",
    "    label.append(labels)\n",
    "    \n",
    "    # rna type info\n",
    "    types = set(trans_type[node])\n",
    "    rna_type = ['/'.join(x) if len(types) > 1 else x for x in types][0]\n",
    "\n",
    "    if rna_type != 'None':\n",
    "        # description\n",
    "        description.append('Transcript {desc} is classified as type {typ}'.format(desc=node, typ=rna_type))\n",
    "    else:\n",
    "        # description\n",
    "        description.append('None')\n",
    "\n",
    "    # synonym\n",
    "    synonym.append('None')\n",
    "    \n",
    "# combine into new data frame\n",
    "rna_metadata_final = pandas.DataFrame(list(zip(rna, label, description, synonym)),\n",
    "                                      columns =['ID', 'Label', 'Description', 'Synonym'])\n",
    "\n",
    "# convert df to dictionary\n",
    "rna_metadata_final.set_index('ID', inplace=True)\n",
    "rna_metadata_dict = rna_metadata_final.to_dict('index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variant Metadata Dictionary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Download Data_  \n",
    "Only run this code block if the `variant_summary.txt` has not already been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_data = pandas.read_csv(unprocessed_data_location + 'variant_summary.txt',\n",
    "                           header = 0,\n",
    "                           delimiter = '\\t',\n",
    "                           low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Normal data preprocess and filtering steps are performed in order to prepare the data for the next step, which converts it to a metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows without identifiers\n",
    "var_data = var_data.loc[var_data['Assembly'].apply(lambda x: x == 'GRCh38')]\n",
    "var_data = var_data.loc[var_data['RS# (dbSNP)'].apply(lambda x: x != -1)]\n",
    "\n",
    "# de-dup data\n",
    "var_metadata = var_data[['#AlleleID', 'Type', 'Name', 'ClinicalSignificance', 'RS# (dbSNP)', 'Origin',\n",
    "                         'ChromosomeAccession', 'Chromosome', 'Start', 'Stop', 'ReferenceAllele',\n",
    "                         'Assembly', 'AlternateAllele','Cytogenetic', 'ReviewStatus', 'LastEvaluated']] \n",
    "\n",
    "# replace NaN with 'None'\n",
    "var_metadata.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicate dbSNP ids by choosing the most recent reviewed variant\n",
    "var_metadata.sort_values('LastEvaluated', ascending=False, inplace=True)\n",
    "var_metadata.drop_duplicates(subset='RS# (dbSNP)', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Create Variant Metadata Dictionary_  \n",
    "The nested dictionary of rna metadata is created by looping over the human [ClinVar Variant](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#clinvar) identifier data set ([`variant_summary.txt`](https://www.dropbox.com/s/g58b0oduv7sj3ja/variant_summary.txt?dl=1)). The `keys` of the dictionary are `dbSNP identifiers` and the `values` are dictionaries for each metadata type: `symbol`, `description`, and `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata\n",
    "variant, label, description = [], [], []\n",
    "\n",
    "for idx, row in tqdm(var_metadata.iterrows(), total=var_metadata.shape[0]):\n",
    "    \n",
    "    # node\n",
    "    if row['RS# (dbSNP)'] != 'None':\n",
    "        variant.append('rs' + str(row['RS# (dbSNP)']))\n",
    "    \n",
    "    # label -- only want metadata if there is a label\n",
    "    if row['Name'] != 'None':\n",
    "        label.append(row['Name'])\n",
    "    \n",
    "        # description\n",
    "        sent = 'This variant is a {Origin} {Type} that results when a {ReferenceAllele} allele is changed to {AlternateAllele} on chromosome {Chromosome} ({ChromosomeAccession}, start:{Start}/stop:{Stop} positions, cytogenetic location:{Cytogenetic}) and has clinical significance {ClinicalSignificance}. This entry is for the {Assembly} and was last reviewed on {LastEvaluated} with review status \"{ReviewStatus}\".'\n",
    "        description.append(sent.format(Origin=row['Origin'], Type=row['Type'], ReferenceAllele=row['ReferenceAllele'],\n",
    "                                       AlternateAllele=row['AlternateAllele'], Chromosome=row['Chromosome'],\n",
    "                                       ChromosomeAccession=row['ChromosomeAccession'], Start=row['Start'],\n",
    "                                       Stop=row['Stop'], Cytogenetic=row['Cytogenetic'], ClinicalSignificance=row['ClinicalSignificance'],\n",
    "                                       Assembly=row['Assembly'], LastEvaluated=row['LastEvaluated'], ReviewStatus=row['ReviewStatus']))\n",
    "\n",
    "# combine into new data frame\n",
    "var_metadata_final = pandas.DataFrame(list(zip(variant, label, description)), columns =['ID', 'Label', 'Description'])\n",
    "\n",
    "# drop duplicates\n",
    "var_metadata_final.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# make all variables string\n",
    "var_metadata_final = var_metadata_final.astype(str)\n",
    "\n",
    "# convert df to dictionary\n",
    "var_metadata_final.set_index('ID', inplace=True)\n",
    "var_metadata_dict = var_metadata_final.to_dict('index')                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Master Metadata Dictionary**  \n",
    "To make it easier to navigate the mapping of each instance node in an edge, a master dictionary is created and keyed by node type. This is most useful when both nodes in an edge are instances, but of different data types (e.g. `gene-rna`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_metadata_dictionary = {'gene': gene_metadata_dict, 'rna': rna_metadata_dict, 'variant': var_metadata_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Metadata Files  <a class=\"anchor\" id=\"write-metadata-files\"></a>   \n",
    "using the `Master Metadata Dictionary` created in the prior step, all of the `edge-type` data is processed and the resulting data written out `.txt` file to the `./resource/node_data` repository.\n",
    "\n",
    "- [Genes](#gene-metadata)\n",
    "- [RNA](#rna-metadata)\n",
    "- [Pathways](#pathway-metadata)\n",
    "- [Complexes](#complex-metadata)\n",
    "- [Reactions](#reaction-metadata)\n",
    "- [Variants](#variant-metadata)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genes <a class=\"anchor\" id=\"gene-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Pages:** [NCBI Gene](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#ncbi-gene) \n",
    "\n",
    "**Output:**  \n",
    "- chemical-gene ➞ [`chemical-gene_GENE_METADATA.txt`](https://www.dropbox.com/s/fvkqnuk5xhs0huh/chemical-gene_GENE_METADATA.txt?dl=1) \n",
    "- gene-disease ➞ [`gene-disease_GENE_METADATA.txt`](https://www.dropbox.com/s/o0y21rx3b829q6d/gene-disease_GENE_METADATA.txt?dl=1) \n",
    "- gene-gene ➞ [`gene-gene_GENE_METADATA.txt`](https://www.dropbox.com/s/i4gznnct7rzh7pn/gene-gene_GENE_METADATA.txt?dl=1) \n",
    "- gene-pathway ➞ [`gene-pathway_GENE_METADATA.txt`](https://www.dropbox.com/s/yncd95vanhkp0ey/gene-pathway_GENE_METADATA.txt?dl=1) \n",
    "- gene-phenotype ➞ [`gene-phenotype_GENE_METADATA.txt`](https://www.dropbox.com/s/jghcoc5xzada011/gene-phenotype_GENE_METADATA.txt?dl=1) \n",
    "- gene-protein ➞ [`gene-protein_GENE_METADATA.txt`](https://www.dropbox.com/s/6vu961lna08qn08/gene-protein_GENE_METADATA.txt?dl=1) \n",
    "- gene-rna ➞ [`gene-rna_GENE_METADATA.txt`](https://www.dropbox.com/s/vs0kirmugdo9zkd/gene-rna_GENE_METADATA.txt?dl=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'gene'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_data_location + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNA<a class=\"anchor\" id=\"rna-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Pages:**  \n",
    "- [Ensembl](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#clinvar)  \n",
    "- [NCBI Gene](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#ncbi-gene) \n",
    "\n",
    "**Output:**  \n",
    "- chemical-rna ➞ [`chemical-rna_RNA_METADATA.txt`](https://www.dropbox.com/s/sm0orl0waq5iqhd/chemical-rna_RNA_METADATA.txt?dl=1) \n",
    "- rna-anatomy ➞ [`rna-anatomy_RNA_METADATA.txt`](https://www.dropbox.com/s/plkrunhhusx6mez/rna-anatomy_RNA_METADATA.txt?dl=1) \n",
    "- rna-cell ➞ [`rna-cell_RNA_METADATA.txt`](https://www.dropbox.com/s/dld0eadxyyzr44y/rna-cell_RNA_METADATA.txt?dl=1) \n",
    "- rna-protein ➞ [`rna-protein_RNA_METADATA.txt`](https://www.dropbox.com/s/3g72sb2e685rptn/rna-protein_RNA_METADATA.txt?dl=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'rna'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_data_location + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pathways<a class=\"anchor\" id=\"pathway-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#reactome-pathway-database)  \n",
    "\n",
    "**Output:**    \n",
    "- chemical-pathway ➞ [`chemical-pathway_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/2txg2ui4e6y7rnm/chemical-pathway_PATHWAY_METADATA.txt?dl=1)\n",
    "- gobp-pathway ➞ [`gobp-pathway_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/bq0g1g4ef40vwxj/gobp-pathway_PATHWAY_METADATA.txt?dl=1)\n",
    "- pathway-gocc ➞ [`pathway-gocc_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/6fzkzjxj08u6jfi/pathway-gocc_PATHWAY_METADATA.txt?dl=1)\n",
    "- pathway-gomf ➞ [`pathway-gomf_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/gfqt86vujnoo7j5/pathway-gomf_PATHWAY_METADATA.txt?dl=1)\n",
    "- protein-pathway ➞ [`protein-pathway_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/xadtz4c0ab4a7p9/protein-pathway_PATHWAY_METADATA.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'pathway'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_data_location + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexes<a class=\"anchor\" id=\"complex-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#reactome-pathway-database)    \n",
    "\n",
    "**Output:**    \n",
    "- chemical-complex ➞ [`chemical-complex_COMPLEX_METADATA.txt`](https://www.dropbox.com/s/mu53u8fv5v0epvf/chemical-complex_COMPLEX_METADATA.txt?dl=1) \n",
    "- complex-complex ➞ [`complex-complex_COMPLEX_METADATA.txt`](https://www.dropbox.com/s/y4qt0ne47ix1tqb/complex-complex_COMPLEX_METADATA.txt?dl=1) \n",
    "- complex-pathway ➞ [`complex-pathway_COMPLEX_METADATA.txt`](https://www.dropbox.com/s/6n9w0vvxabi7efl/complex-pathway_COMPLEX_METADATA.txt?dl=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'complex'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_data_location + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reactions<a class=\"anchor\" id=\"reaction-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#reactome-pathway-database)   \n",
    "\n",
    "**Output:**    \n",
    "- chemical-reaction ➞ [`chemical-reaction_REACTION_METADATA.txt`](https://www.dropbox.com/s/6iztwaxrhrp1f7h/chemical-reaction_REACTION_METADATA.txt?dl=1)\n",
    "- protein-reaction ➞ [`protein-reaction_REACTION_METADATA.txt`](https://www.dropbox.com/s/92vsuon54w4uq4j/protein-reaction_REACTION_METADATA.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'reaction'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_data_location + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants<a class=\"anchor\" id=\"variant-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [ClinVar](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#clinvar)  \n",
    "\n",
    "**Output:**  \n",
    "- variant-disease ➞ [`variant-disease_VARIANT_METADATA.txt`](https://www.dropbox.com/s/vj440u5efwdwibl/variant-disease_VARIANT_METADATA.txt?dl=1)  \n",
    "- variant-gene ➞ [`variant-gene_VARIANT_METADATA.txt`](https://www.dropbox.com/s/geui7nby9h055bc/variant-gene_VARIANT_METADATA.txt?dl=1)  \n",
    "- variant-phenotype ➞ [`variant-phenotype_VARIANT_METADATA.txt`](https://www.dropbox.com/s/hnocd802detivdd/variant-phenotype_VARIANT_METADATA.txt?dl=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'variant'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_data_location + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "```\n",
    "@misc{callahan_tj_2019_3401437,\n",
    "  author       = {Callahan, TJ},\n",
    "  title        = {PheKnowLator},\n",
    "  month        = mar,\n",
    "  year         = 2019,\n",
    "  doi          = {10.5281/zenodo.3401437},\n",
    "  url          = {https://doi.org/10.5281/zenodo.3401437}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
