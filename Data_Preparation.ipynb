{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# PheKnowLator - Data Preparation\n",
    "***\n",
    "***\n",
    "\n",
    "**Author:** [TJCallahan](https://mail.google.com/mail/u/0/?view=cm&fs=1&tf=1&to=callahantiff@gmail.com)  \n",
    "**GitHub Repository:** [PheKnowLator](https://github.com/callahantiff/PheKnowLator/wiki)  \n",
    "**Release:** **[v2.0.0](https://github.com/callahantiff/PheKnowLator/wiki/v2.0.0)**\n",
    "\n",
    "**Purpose:** This notebook serves as a script to download and process data in order to generate mapping and filtering data needed to build edges for the PheKnowLator knowledge graph. For more information on the data sources utilize within this script, please see the [Data Sources](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources) Wiki page.\n",
    "\n",
    "**Assumptions:**   \n",
    "- Raw data downloads ➞ `./resources/processed_data/unprocessed_data`    \n",
    "- Processed data write location ➞ `./resources/processed_data`   \n",
    "\n",
    "**Dependencies:** This notebook utilizes several helper functions, which are stored in the [`data_preparation_helper_functions.py`](https://github.com/callahantiff/PheKnowLator/blob/master/scripts/python/data_preparation_helper_functions.py) script. Hyperlinks to all downloaded and generated data sources are provided on the [Data Sources](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources) Wiki page as well as within each source subsection of this notebook. All generated data is freely available for download from DropBox. \n",
    "\n",
    "_____\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "***\n",
    "\n",
    "### [Create Identifier Maps ](#create-identifier-maps)  \n",
    "- [HUMAN TRANSCRIPT, GENE, AND PROTEIN IDENTIFIER MAPPING](#human-transcript,-gene,-and-protein-identifier-mapping)\n",
    "  - [Ensembl Gene-Ensembl Transcript](#ensemblgene-ensembltranscript)  \n",
    "  - [Ensembl Gene-Entrez Gene](#ensemblgene-entrezgene)\n",
    "  - [Ensembl Transcript-Protein Ontology](#ensembltranscript-proteinontology)\n",
    "  - [Entrez Gene-Ensembl Transcript](#entrezgene-ensembltranscript)\n",
    "  - [Entrez Gene-Protein Ontology](#entrezgene-proteinontology)  \n",
    "  - [STRING-Protein Ontology](#string-proteinontology)  \n",
    "  - [Uniprot Accession-Protein Ontology](#uniprotaccession-proteinontology)\n",
    "\n",
    "\n",
    "- [OTHER IDENTIFIER MAPPING](#other-identifier-mapping) \n",
    "  - [ChEBI Identifiers](#mesh-chebi) \n",
    "  - [Human Disease and Phenotype Identifiers](#disease-identifiers)\n",
    "  - [Human Protein Atlas Tissue and Cell Types](#hpa-uberon)  \n",
    "\n",
    "<br>\n",
    "\n",
    "### [Create Edge Datasets](#create-edge-datasets)\n",
    "- [ONTOLOGIES](#ontologies)  \n",
    "  - [Protein Ontology](#protein-ontology)  \n",
    "  - [Relations Ontology](#relations-ontology)  \n",
    "\n",
    "\n",
    "- [LINKED DATA](#linked-data)  \n",
    "  - [Clinvar Variant-Diseases and Phenotypes](#clinvar-variant)\n",
    "  - [NCBI Gene Protein-Coding Genes and Proteins](#ncbi-protein-coding-genes)  \n",
    "  - [Reactome Chemical-Complex Data](#reactome-chemical-complex)\n",
    "  - [Reactome Complex-Complex Data](#reactome-complex-complex)\n",
    "  - [Reactome Complex-Pathway Data](#reactome-complex-pathway)\n",
    "  - [Reactome Protein-Complex Data](#reactome-protein-complex)\n",
    "  - [Uniprot Protein-Cofactor and Protein-Catalyst](#uniprot-protein-cofactorcatalyst)  \n",
    "\n",
    "<br>\n",
    "\n",
    "### [Gather Instance Data Metadata](#create-instance-metadata)  \n",
    "- [Genes/RNA](#gene-and-rna-metadata)\n",
    "- [Pathways](#pathway-metadata)\n",
    "- [Complexes](#complex-metadata)\n",
    "- [Reactions](#reaction-metadata)\n",
    "- [Variants](#variant-metadata) \n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-Up Environment\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import glob\n",
    "import networkx\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "from functools import reduce\n",
    "from owlready2 import subprocess\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, extras, Literal\n",
    "from rdflib.extras.external_graph_libs import *\n",
    "from reactome2py import content\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import script containing helper functions\n",
    "from scripts.python.data_preparation_helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to read unprocessed data files from\n",
    "unprocessed_data_location = 'resources/processed_data/unprocessed_data/'\n",
    "\n",
    "# directory to write processed data files to\n",
    "processed_data_location = 'resources/processed_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### CREATE MAPPING DATASETS  <a class=\"anchor\" id=\"create-identifier-maps\"></a>\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Transcript, Gene, and Protein Identifier Mapping  <a class=\"anchor\" id=\"human-transcript,-gene,-and-protein-identifier-mapping\"></a>\n",
    "***\n",
    "\n",
    "**Data Source Wiki Pages:**   \n",
    "- [Ensembl](https://uswest.ensembl.org/)\n",
    "- [Uniprot Knowledgebase](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#uniprot-knowledgebase)   \n",
    "- [NCBI Gene](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#ncbi-gene) \n",
    "- [Protein Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#protein-ontology)\n",
    "\n",
    "**Purpose:** To map create protein-coding gene-protein relations and mappings between the identifiers listed below. The edges types produced from each of these mappings will be further described within each identifier mapping section:  \n",
    "- [Ensembl Gene-Ensembl Transcript](#ensemblgene-ensembltranscript)  \n",
    "- [Entrez Gene-Ensembl Transcript](#entrezgene-ensembltranscript)  \n",
    "- [Entrez Gene-Protein Ontology](#entrezgene-proteinontology)  \n",
    "- [Ensembl Gene-Entrez Gene](#ensemblgene-entrezgene)\n",
    "- [Uniprot Accession-Protein Ontology](#uniprotaccession-proteinontology)\n",
    "- [STRING-Protein Ontology](#string-proteinontology)\n",
    "\n",
    "**Output:** This script downloads and saves the following data:  \n",
    "\n",
    "- Human Ensembl-UniProt Identifiers: [Homo_sapiens.GRCh38.98.uniprot.tsv.gz](ftp://ftp.ensembl.org/pub/release-98/tsv/homo_sapiens/Homo_sapiens.GRCh38.98.uniprot.tsv.gz) ➞ [`Homo_sapiens.GRCh38.98.uniprot.tsv`](https://www.dropbox.com/s/cesjvqz1b8c7ami/Homo_sapiens.GRCh38.98.uniprot.tsv?dl=1) \n",
    "- Human Ensembl-Entrez Identifiers: [Homo_sapiens.GRCh38.98.entrez.tsv.gz](ftp://ftp.ensembl.org/pub/release-98/tsv/homo_sapiens/Homo_sapiens.GRCh38.98.entrez.tsv.gz) ➞ [`Homo_sapiens.GRCh38.98.entrez.tsv`](https://www.dropbox.com/s/5kstw70py0azvws/Homo_sapiens.GRCh38.98.entrez.tsv?dl=1) \n",
    "- Human Gene Identifiers: [Homo_sapiens.gene_info.gz](ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz) ➞ [`Homo_sapiens.gene_info`](https://www.dropbox.com/s/vazlmzxydgv6xzz/Homo_sapiens.gene_info?dl=1)  \n",
    "- Human Protein Identifiers: [promapping.txt](https://proconsortium.org/download/current/promapping.txt) ➞ [`promapping.txt`](https://www.dropbox.com/s/x7wdimv6ph6bl8k/promapping.txt?dl=1) \n",
    "\n",
    "_All Merged Data Sets:_ [`Merged_Human_Ensembl_Entrez_Uniprot_Identifiers.txt`](https://www.dropbox.com/s/l79166x1fx6vc4l/Merged_Human_Ensembl_Entrez_Uniprot_Identifiers.txt?dl=1)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Process Data:** `Homo_sapiens.GRCh38.98.uniprot.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'ftp://ftp.ensembl.org/pub/release-98/tsv/homo_sapiens/Homo_sapiens.GRCh38.98.uniprot.tsv.gz'\n",
    "data_downloader(url1, unprocessed_data_location)\n",
    "\n",
    "url2 = 'ftp://ftp.ensembl.org/pub/release-98/tsv/homo_sapiens/Homo_sapiens.GRCh38.98.entrez.tsv.gz'\n",
    "data_downloader(url2, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ensembl-uniprot data\n",
    "ensembl1 = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.98.uniprot.tsv',\n",
    "                          header = 0,\n",
    "                          delimiter = '\\t',\n",
    "                          low_memory=False)\n",
    "# replace \"-\"\n",
    "ensembl1.replace('-','None', inplace=True)\n",
    "\n",
    "# read in entrez-uniprot data\n",
    "ensembl2 = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.98.entrez.tsv',\n",
    "                          header = 0,\n",
    "                          delimiter = '\\t',\n",
    "                          low_memory=False)\n",
    "\n",
    "# replace \"-\"\n",
    "ensembl2.replace('-','None', inplace=True)\n",
    "\n",
    "# merge datasets\n",
    "ensembl = pandas.merge(ensembl1[['gene_stable_id', 'transcript_stable_id', 'protein_stable_id', 'xref']],\n",
    "                       ensembl2[['gene_stable_id', 'transcript_stable_id', 'protein_stable_id', 'xref']],\n",
    "                       left_on=['gene_stable_id', 'transcript_stable_id', 'protein_stable_id'],\n",
    "                       right_on=['gene_stable_id', 'transcript_stable_id', 'protein_stable_id'],\n",
    "                       how='outer')\n",
    "\n",
    "# rename columns\n",
    "ensembl.rename(columns={'xref_x': 'xref_uniprot', 'xref_y': 'xref_entrez'}, inplace=True)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl.fillna('None', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Process Data:** `Homo_sapiens.gene_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi_gene = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.gene_info', header = 0, delimiter = '\\t')\n",
    "\n",
    "# replace \"-\" with \"None\"\n",
    "ncbi_gene.replace('-','None', inplace=True)\n",
    "\n",
    "# explode nested data\n",
    "explode_df_ncbi_gene = explode(ncbi_gene.copy(), ['dbXrefs'], '|')\n",
    "\n",
    "# remove identifier type, which appears before ':'\n",
    "explode_df_ncbi_gene['dbXrefs'].replace('(^\\w*\\:)','', inplace=True, regex=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_ncbi_gene.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for quick look-up of ensembl gene ids by entrez gene ids\n",
    "# EXPLANATION: an incorrect number of indentifers is returned when attempting to merge columns, this strategy is more effective\n",
    "ensembl_map = {}\n",
    "\n",
    "# loop over data and fill in missing values\n",
    "for idx, row in tqdm(explode_df_ncbi_gene.iterrows(), total=explode_df_ncbi_gene.shape[0]):\n",
    "    if (row['GeneID'] != 'None' and row['dbXrefs'] != 'None') and row['dbXrefs'].startswith('ENSG'):\n",
    "        if row['GeneID'] in ensembl_map.keys():\n",
    "            ensembl_map[row['GeneID']].append(row['dbXrefs'])\n",
    "        else:\n",
    "            ensembl_map[row['GeneID']] = [row['dbXrefs']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Process Data:** `promapping.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://proconsortium.org/download/current/promapping.txt'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pro_mapping = pandas.read_csv(unprocessed_data_location + 'promapping.txt',\n",
    "                              header = None,\n",
    "                              names = ['pro_id', 'Entry', 'pro_mapping'],\n",
    "                              delimiter = '\\t')\n",
    "\n",
    "# remove rows without 'UniProtKB'\n",
    "pro_mapping = pro_mapping.loc[pro_mapping['Entry'].apply(lambda x: x.startswith('UniProtKB:'))] \n",
    "\n",
    "# remove identifier type, which appears before ':'\n",
    "pro_mapping['Entry'].replace('(^\\w*\\:)','', inplace=True, regex=True)\n",
    "\n",
    "# preview data\n",
    "pro_mapping.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Merge Processed Data:** `ensembl` + `Homo_sapiens.gene_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# make sure that merge columns are of same type\n",
    "ncbi_gene['GeneID'] = ncbi_gene['GeneID'].astype(str)\n",
    "\n",
    "# merge uniprot and ncbi data\n",
    "ensembl_ncbi_merged_data = pandas.merge(ensembl,\n",
    "                                        ncbi_gene,\n",
    "                                        left_on=['xref_entrez'],\n",
    "                                        right_on=['GeneID'],\n",
    "                                        how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_ncbi_merged_data.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Clean Merged Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up merged data by combining columns of same type and removing un-needed columns\n",
    "gene_ids, ensemble_genes = [], []\n",
    "\n",
    "# loop over data and fill in missing values\n",
    "for idx, row in tqdm(ensembl_ncbi_merged_data.iterrows(), total=ensembl_ncbi_merged_data.shape[0]):\n",
    "    \n",
    "    # gene ids\n",
    "    if row['xref_entrez'] != 'None' or row['GeneID'] != 'None':\n",
    "        gene_ids.append(re.sub('None', '', ''.join(set([row['xref_entrez'], row['GeneID']]))))\n",
    "    else:\n",
    "        gene_ids.append('None')\n",
    "    \n",
    "    # fill in missing ensembl gene ids\n",
    "    if row['gene_stable_id'] == 'None' and row['GeneID'] != 'None':\n",
    "        if int(row['GeneID']) in ensembl_map.keys():\n",
    "            ensemble_genes.append(ensembl_map[int(row['GeneID'])][0])\n",
    "        else:\n",
    "            ensemble_genes.append('None')\n",
    "    else:\n",
    "        ensemble_genes.append(row['gene_stable_id'])\n",
    "        \n",
    "# reduce columns\n",
    "ensembl_ncbi_merged_data_clean = ensembl_ncbi_merged_data.copy()\n",
    "ensembl_ncbi_merged_data_clean = ensembl_ncbi_merged_data[['transcript_stable_id', 'protein_stable_id', 'xref_uniprot', 'Symbol',\n",
    "                                                           'Synonyms', 'description', 'type_of_gene', 'chromosome', 'map_location', 'Other_designations']]\n",
    "\n",
    "# add cleaned columns\n",
    "ensembl_ncbi_merged_data_clean['GeneID_Cleaned'] = gene_ids\n",
    "ensembl_ncbi_merged_data_clean['EnsemblGenes_Cleaned'] = ensemble_genes\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_ncbi_merged_data_clean.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "    \n",
    "# preview data\n",
    "ensembl_ncbi_merged_data_clean.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Merge Processed Data:** `ensembl_ncbi_merged_data_clean` + `promapping.txt`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge uniprot and ncbi data\n",
    "merged_data = pandas.merge(ensembl_ncbi_merged_data_clean,\n",
    "                           pro_mapping,\n",
    "                           left_on='xref_uniprot',\n",
    "                           right_on='Entry',\n",
    "                           how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "merged_data.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean Full Merged Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up merged data by combining columns of same type and removing un-needed columns\n",
    "uniprot_ids = []\n",
    "\n",
    "# loop over data and fill in missing values\n",
    "for idx, row in tqdm(merged_data.iterrows(), total=merged_data.shape[0]):\n",
    "    \n",
    "    # uniprot ids\n",
    "    if row['xref_uniprot'] != 'None' or row['Entry'] != 'None':\n",
    "        uniprot_ids.append(re.sub('None', '', ''.join(set([row['xref_uniprot'], row['Entry']]))))\n",
    "    else:\n",
    "        uniprot_ids.append('None')\n",
    "        \n",
    "# reduce columns\n",
    "merged_data_clean = merged_data.copy()\n",
    "merged_data_clean = merged_data[['EnsemblGenes_Cleaned', 'transcript_stable_id', 'protein_stable_id',\n",
    "                                 'GeneID_Cleaned', 'type_of_gene', 'Symbol', 'Synonyms', 'description',\n",
    "                                 'chromosome', 'map_location', 'Other_designations','pro_id', 'pro_mapping']]\n",
    "\n",
    "# add cleaned columns\n",
    "merged_data_clean['UniprotAccessionID_Cleaned'] = uniprot_ids\n",
    "\n",
    "# remove duplicates\n",
    "merged_data_clean.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "merged_data_clean.fillna('None', inplace=True)\n",
    "\n",
    "# write data\n",
    "merged_data_clean.to_csv(processed_data_location + 'Merged_Human_Ensembl_Entrez_Uniprot_Identifiers.txt',\n",
    "                         header = True,\n",
    "                         sep = '\\t',\n",
    "                         index = False)\n",
    "    \n",
    "# preview data\n",
    "merged_data_clean.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembl Gene-Ensembl Transcript <a class=\"anchor\" id=\"ensemblgene-ensembltranscript\"></a>\n",
    "\n",
    "**Purpose:** To map Ensembl gene identifiers to Ensembl transcript identifiers when creating the following edges: \n",
    "- RNA-Cell   \n",
    "- RNA-Tissue Types  \n",
    "\n",
    "**Output:** [`ENSEMBL_GENE_ENSEMBL_TRANSCRIPT_MAP.txt`](https://www.dropbox.com/s/8n1isqytlz2z1g6/ENSEMBL_GENE_ENSEMBL_TRANSCRIPT_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-dup data\n",
    "df_ens = merged_data_clean.drop_duplicates(subset=['EnsemblGenes_Cleaned', 'transcript_stable_id'], keep='first', inplace=False) \n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'ENSEMBL_GENE_ENSEMBL_TRANSCRIPT_MAP.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(df_ens.iterrows(), total=df_ens.shape[0]):\n",
    "        if row['EnsemblGenes_Cleaned'] != 'None' and row['transcript_stable_id'] != 'None': \n",
    "            outfile.write(row['EnsemblGenes_Cleaned'].strip() + '\\t' + row['transcript_stable_id'].strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eget_data = pandas.read_csv(processed_data_location + 'ENSEMBL_GENE_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['Ensembl_Gene_IDs', 'Ensembl_Transcript_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} ensembl gene-ensembl transcript edges'.format(edge_count=len(eget_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eget_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembl Gene-Entrez Gene <a class=\"anchor\" id=\"ensemblgene-entrezgene\"></a>\n",
    "\n",
    "\n",
    "**Purpose:** To map Ensembl gene identifiers to Entrez gene identifiers when creating the following edges:   \n",
    "- gene-gene\n",
    "\n",
    "**Output:** [`ENSEMBL_GENE_ENTREZ_GENE_MAP.txt`](https://www.dropbox.com/s/crghjh2we5v7pws/ENSEMBL_GENE_ENTREZ_GENE_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-dup data\n",
    "df_ens = merged_data_clean.drop_duplicates(subset=['EnsemblGenes_Cleaned', 'GeneID_Cleaned'], keep='first', inplace=False) \n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'ENSEMBL_GENE_ENTREZ_GENE_MAP.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(df_ens.iterrows(), total=df_ens.shape[0]):\n",
    "        if row['EnsemblGenes_Cleaned'] != 'None' and row['GeneID_Cleaned'] != 'None': \n",
    "            outfile.write(row['EnsemblGenes_Cleaned'].strip() + '\\t' + row['GeneID_Cleaned'].strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egeg_data = pandas.read_csv(processed_data_location + 'ENSEMBL_GENE_ENTREZ_GENE_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['Ensembl_Gene_IDs', 'Entrez_Gene_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} ensembl gene-entrez gene edges'.format(edge_count=len(egeg_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egeg_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembl Transcript-Protein Ontology <a class=\"anchor\" id=\"ensembltranscript-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Ensembl transcript identifiers to Protein Ontology identifiers when creating the following edges: \n",
    "- RNA-Protein  \n",
    "\n",
    "**Output:** [`ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/ckrw11nfyu6a08c/ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-dup data\n",
    "df_po = merged_data_clean.drop_duplicates(subset=['transcript_stable_id', 'pro_id'], keep='first', inplace=False) \n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(df_po.iterrows(), total=df_po.shape[0]):\n",
    "        if row['transcript_stable_id'] != 'None' and row['pro_id'] != 'None': \n",
    "            outfile.write(row['transcript_stable_id'].strip() + '\\t' + row['pro_id'].replace('PR:', 'PR_').strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etpr_data = pandas.read_csv(processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['Ensembl_Transcript_IDs', 'Protein_Ontology_IDs'],\n",
    "                            delimiter = '\\t',\n",
    "                            low_memory=False)\n",
    "\n",
    "print('There are {edge_count} ensembl transcript-protein ontology edges'.format(edge_count=len(etpr_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrez Gene-Ensembl Transcript <a class=\"anchor\" id=\"entrezgene-ensembltranscript\"></a>\n",
    "\n",
    "**Purpose:** To map Entrez gene identifiers to Ensembl transcript identifiers when creating the following edges: \n",
    "- gene-RNA \n",
    "- chemical-rna\n",
    "\n",
    "**Output:** [`ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt`](https://www.dropbox.com/s/yqnofd8h90luygu/ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-dup data\n",
    "df_ens = merged_data_clean.drop_duplicates(subset=['GeneID_Cleaned', 'transcript_stable_id'], keep='first', inplace=False) \n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(df_ens.iterrows(), total=df_ens.shape[0]):\n",
    "        if row['GeneID_Cleaned'] != 'None' and row['transcript_stable_id'] != 'None': \n",
    "            outfile.write(row['GeneID_Cleaned'].strip() + '\\t' + row['transcript_stable_id'].strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eet_data = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['Entrez_Gene_IDs', 'Ensembl_Transcript_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} entrez gene-ensembl transcript edges'.format(edge_count=len(eet_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eet_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrez Gene-Protein Ontology <a class=\"anchor\" id=\"entrezgene-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Protein Ontology identifiers to Ensembl transcript identifiers when creating the following edges:   \n",
    "- chemical-protein  \n",
    "\n",
    "**Output:** [`ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/ufbp5o6zgagriw7/ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-dup data\n",
    "df_egpr = merged_data_clean.drop_duplicates(subset=['GeneID_Cleaned', 'pro_id'], keep='first', inplace=False) \n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(df_egpr.iterrows(), total=df_egpr.shape[0]):\n",
    "        if row['GeneID_Cleaned'] != 'None' and row['pro_id'] != 'None': \n",
    "            outfile.write(row['GeneID_Cleaned'].strip() + '\\t' + row['pro_id'].replace(':', '_').strip() +  '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egpr_data = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['Gene_IDs', 'Protein_Ontology_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} entrez gene-protein ontology edges'.format(edge_count=len(egpr_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STRING-Protein Ontology <a class=\"anchor\" id=\"string-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map STRING identifiers to Protein Ontology identifiers when creating the following edges:   \n",
    "- protein-protein  \n",
    "\n",
    "**Output:** [`STRING_PRO_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/mekh5lr3bxp7gvu/STRING_PRO_ONTOLOGY_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-dup data\n",
    "df_ens = merged_data_clean.drop_duplicates(subset=['protein_stable_id', 'pro_id'], keep='first', inplace=False) \n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(df_ens.iterrows(), total=df_ens.shape[0]):\n",
    "        if row['protein_stable_id'] != 'None' and row['pro_id'] != 'None':\n",
    "            outfile.write('9606.' + row['protein_stable_id'].strip() + '\\t' + row['pro_id'].replace(':', '_').strip() +  '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpr_data = pandas.read_csv(processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['STRING_IDs', 'Protein_Ontology_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} string-protein ontology edges'.format(edge_count=len(stpr_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniprot Accession-Protein Ontology <a class=\"anchor\" id=\"uniprotaccession-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Uniprot accession identifiers to Protein Ontology identifiers when creating the following edges:  \n",
    "- protein-gobp  \n",
    "- protein-gomf  \n",
    "- protein-gocc  \n",
    "- protein-complex  \n",
    "- protein-cofactor  \n",
    "- protein-catalyst \n",
    "- protein-reaction  \n",
    "- protein-pathway\n",
    "\n",
    "**Output:** [`UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/txp8tqdipzwus9p/UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-dup data\n",
    "df_ens = merged_data_clean.drop_duplicates(subset=['UniprotAccessionID_Cleaned', 'pro_id'], keep='first', inplace=False) \n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(df_ens.iterrows(), total=df_ens.shape[0]):\n",
    "        if row['UniprotAccessionID_Cleaned'] != 'None' and row['pro_id'] != 'None': \n",
    "            outfile.write(row['UniprotAccessionID_Cleaned'].strip() + '\\t' + row['pro_id'].replace(':', '_').strip() +  '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uapr_data = pandas.read_csv(processed_data_location + 'UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header = None,\n",
    "                            names=['Uniprot_Accession_IDs', 'Protein_Ontology_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} uniprot accession-protein ontology edges'.format(edge_count=len(uapr_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uapr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Other Identifier Mapping <a class=\"anchor\" id=\"other-identifier-mapping\"></a>\n",
    "***\n",
    "* [ChEBI Identifiers](#mesh-chebi)  \n",
    "* [Human Protein Atlas Tissue and Cell Types](#hpa-uberon) \n",
    "* [Human Disease and Phenotype Identifiers](#disease-identifiers) \n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChEBI-MeSH Identifiers <a class=\"anchor\" id=\"mesh-chebi\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [mapping-mesh-to-chebi](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#mapping-mesh-identifiers-to-chebi-identifiers)  \n",
    "\n",
    "**Purpose:** Map MeSH identifiers to ChEBI identifiers when creating the following edges:  \n",
    "- Chemical-Gene  \n",
    "- Chemical-Disease\n",
    "\n",
    "**Dependencies:** This script assumes that the [`ncbo_rest_api.py`](https://github.com/callahantiff/PheKnowLator/blob/development/scripts/python/ncbo_rest_api.py) script was run and the data generated from this file was written to `./resources/processed_data/temp`. \n",
    "\n",
    "**Output:** [`MESH_CHEBI_MAP.txt`](https://www.dropbox.com/s/5nr87v5h6x8oc1b/MESH_CHEBI_MAP.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'MESH_CHEBI_MAP.txt', 'w') as out:\n",
    "    for filename in tqdm(glob.glob(processed_data_location + 'temp/*.txt')):\n",
    "        for row in list(filter(None, open(filename, 'r').read().split('\\n'))):\n",
    "            mesh = '_'.join(row.split('\\t')[0].split('/')[-2:])\n",
    "            chebi = row.split('\\t')[1].split('/')[-1]\n",
    "            out.write(mesh + '\\t' + chebi + '\\n')\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_data = pandas.read_csv(processed_data_location + 'MESH_CHEBI_MAP.txt',\n",
    "                          delimiter = '\\t',\n",
    "                          header=None,\n",
    "                          names=['MeSH_IDs', 'ChEBI_IDs'])\n",
    "\n",
    "print('There are {edge_count} MeSH-ChEBI edges'.format(edge_count=len(mc_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disease and Phenotype Identifiers <a class=\"anchor\" id=\"disease-identifiers\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [disgenet](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#disgenet)  \n",
    "\n",
    "**Purpose:** This script downloads the [disease_mappings.tsv](https://www.disgenet.org/static/disgenet_ap1/files/downloads/disease_mappings.tsv.gz) to map UMLS identifiers to Human Disease and Human Phenotype identifiers when creating the following edges:  \n",
    "- chemical-disease  \n",
    "- disease-phenotype\n",
    "\n",
    "**Output:**   \n",
    "- Human Disease Ontology Mappings ➞ [`DISEASE_DOID_MAP.txt`](https://www.dropbox.com/s/q30ferujl7k574j/DISEASE_DOID_MAP.txt?dl=1)  \n",
    "- Human Phenotype Ontology Mappings ➞ [`PHENOTYPE_HPO_MAP.txt`](https://www.dropbox.com/s/5ayl0c5qm7r4tdm/PHENOTYPE_HPO_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.disgenet.org/static/disgenet_ap1/files/downloads/disease_mappings.tsv.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_data = pandas.read_csv(unprocessed_data_location + 'disease_mappings.tsv',\n",
    "                               header = 0,\n",
    "                               delimiter = '|')\n",
    "\n",
    "disease_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dictionary\n",
    "disease_dict = {}\n",
    "\n",
    "for idx, row in tqdm(disease_data.iterrows(), total=disease_data.shape[0]):\n",
    "    \n",
    "    if row['vocabulary'] == 'MSH':\n",
    "        mesh_finder(disease_data, row['code'], 'MESH:', disease_dict)\n",
    "        \n",
    "    elif row['vocabulary'] == 'OMIM':\n",
    "        mesh_finder(disease_data, row['code'], 'OMIM:', disease_dict)\n",
    "        \n",
    "    elif row['vocabulary'] == 'ORDO':\n",
    "        mesh_finder(disease_data, row['code'], 'ORPHA:', disease_dict)\n",
    "    \n",
    "    elif row['diseaseId'] in disease_dict.keys():\n",
    "        if row['vocabulary'] == 'DO':\n",
    "            disease_dict[row['diseaseId']].append('DOID_' + row['code']) \n",
    "        \n",
    "        if row['vocabulary'] == 'HPO':\n",
    "            disease_dict[row['diseaseId']].append(row['code'].replace('HP:', 'HP_'))\n",
    "    \n",
    "    else:\n",
    "        if row['vocabulary'] == 'DO':\n",
    "            disease_dict[row['diseaseId']] = ['DOID_' + row['code']] \n",
    "        \n",
    "        if row['vocabulary'] == 'HPO':\n",
    "            disease_dict[row['diseaseId']] = [row['code'].replace('HP:', 'HP_')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'DISEASE_DOID_MAP.txt', 'w') as outfile1,open(processed_data_location + 'PHENOTYPE_HPO_MAP.txt', 'w') as outfile2:\n",
    "    for key, value in tqdm(disease_dict.items()):\n",
    "        for i in value:\n",
    "            # get diseases\n",
    "            if i.startswith('DOID_'): \n",
    "                outfile1.write(key.split(':')[-1] + '\\t' + i + '\\n')\n",
    "\n",
    "            # get phenotypes\n",
    "            if i.startswith('HP_'): \n",
    "                outfile2.write(key.split(':')[-1] + '\\t' + i + '\\n')\n",
    "\n",
    "outfile1.close()\n",
    "outfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Disease (DOID) Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_data = pandas.read_csv(processed_data_location + 'DISEASE_DOID_MAP.txt',\n",
    "                           header = None,\n",
    "                           names=['Disease_IDs', 'DOID_IDs'],\n",
    "                           delimiter = '\\t')\n",
    "\n",
    "print('There are {} disease-DOID edges'.format(len(dis_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Phenotype (HP) Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_data = pandas.read_csv(processed_data_location + 'PHENOTYPE_HPO_MAP.txt',\n",
    "                          header = None,\n",
    "                          names=['Disease_IDs', 'HP_IDs'],\n",
    "                          delimiter = '\\t')\n",
    "\n",
    "print('There are {} phenotype-HPO edges'.format(len(hp_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Protein Atlas Tissue/Cells - UBERON + Cell Ontology + Cell Line Ontology <a class=\"anchor\" id=\"hpa-uberon\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [human-protein-atlas](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#human-protein-atlas)  \n",
    "\n",
    "**Purpose:** Downloads a query for cell, tissue, and blood types with overexpressed protein-coding genes in the human proteome ([`proteinatlas_search.tsv`](https://www.proteinatlas.org/api/search_download.php?search=&columns=g,eg,up,pe,rnatsm,rnaclsm,rnacasm,rnabrsm,rnabcsm,rnablsm,scl,t_RNA_adipose_tissue,t_RNA_adrenal_gland,t_RNA_amygdala,t_RNA_appendix,t_RNA_basal_ganglia,t_RNA_bone_marrow,t_RNA_breast,t_RNA_cerebellum,t_RNA_cerebral_cortex,t_RNA_cervix,_uterine,t_RNA_colon,t_RNA_corpus_callosum,t_RNA_ductus_deferens,t_RNA_duodenum,t_RNA_endometrium_1,t_RNA_epididymis,t_RNA_esophagus,t_RNA_fallopian_tube,t_RNA_gallbladder,t_RNA_heart_muscle,t_RNA_hippocampal_formation,t_RNA_hypothalamus,t_RNA_kidney,t_RNA_liver,t_RNA_lung,t_RNA_lymph_node,t_RNA_midbrain,t_RNA_olfactory_region,t_RNA_ovary,t_RNA_pancreas,t_RNA_parathyroid_gland,t_RNA_pituitary_gland,t_RNA_placenta,t_RNA_pons_and_medulla,t_RNA_prostate,t_RNA_rectum,t_RNA_retina,t_RNA_salivary_gland,t_RNA_seminal_vesicle,t_RNA_skeletal_muscle,t_RNA_skin_1,t_RNA_small_intestine,t_RNA_smooth_muscle,t_RNA_spinal_cord,t_RNA_spleen,t_RNA_stomach_1,t_RNA_testis,t_RNA_thalamus,t_RNA_thymus,t_RNA_thyroid_gland,t_RNA_tongue,t_RNA_tonsil,t_RNA_urinary_bladder,t_RNA_vagina,t_RNA_B-cells,t_RNA_dendritic_cells,t_RNA_granulocytes,t_RNA_monocytes,t_RNA_NK-cells,t_RNA_T-cells,t_RNA_total_PBMC,cell_RNA_A-431,cell_RNA_A549,cell_RNA_AF22,cell_RNA_AN3-CA,cell_RNA_ASC_diff,cell_RNA_ASC_TERT1,cell_RNA_BEWO,cell_RNA_BJ,cell_RNA_BJ_hTERT+,cell_RNA_BJ_hTERT+_SV40_Large_T+,cell_RNA_BJ_hTERT+_SV40_Large_T+_RasG12V,cell_RNA_CACO-2,cell_RNA_CAPAN-2,cell_RNA_Daudi,cell_RNA_EFO-21,cell_RNA_fHDF/TERT166,cell_RNA_HaCaT,cell_RNA_HAP1,cell_RNA_HBEC3-KT,cell_RNA_HBF_TERT88,cell_RNA_HDLM-2,cell_RNA_HEK_293,cell_RNA_HEL,cell_RNA_HeLa,cell_RNA_Hep_G2,cell_RNA_HHSteC,cell_RNA_HL-60,cell_RNA_HMC-1,cell_RNA_HSkMC,cell_RNA_hTCEpi,cell_RNA_hTEC/SVTERT24-B,cell_RNA_hTERT-HME1,cell_RNA_HUVEC_TERT2,cell_RNA_K-562,cell_RNA_Karpas-707,cell_RNA_LHCN-M2,cell_RNA_MCF7,cell_RNA_MOLT-4,cell_RNA_NB-4,cell_RNA_NTERA-2,cell_RNA_PC-3,cell_RNA_REH,cell_RNA_RH-30,cell_RNA_RPMI-8226,cell_RNA_RPTEC_TERT1,cell_RNA_RT4,cell_RNA_SCLC-21H,cell_RNA_SH-SY5Y,cell_RNA_SiHa,cell_RNA_SK-BR-3,cell_RNA_SK-MEL-30,cell_RNA_T-47d,cell_RNA_THP-1,cell_RNA_TIME,cell_RNA_U-138_MG,cell_RNA_U-2_OS,cell_RNA_U-2197,cell_RNA_U-251_MG,cell_RNA_U-266/70,cell_RNA_U-266/84,cell_RNA_U-698,cell_RNA_U-87_MG,cell_RNA_U-937,cell_RNA_WM-115,blood_RNA_basophil,blood_RNA_classical_monocyte,blood_RNA_eosinophil,blood_RNA_gdT-cell,blood_RNA_intermediate_monocyte,blood_RNA_MAIT_T-cell,blood_RNA_memory_B-cell,blood_RNA_memory_CD4_T-cell,blood_RNA_memory_CD8_T-cell,blood_RNA_myeloid_DC,blood_RNA_naive_B-cell,blood_RNA_naive_CD4_T-cell,blood_RNA_naive_CD8_T-cell,blood_RNA_neutrophil,blood_RNA_NK-cell,blood_RNA_non-classical_monocyte,blood_RNA_plasmacytoid_DC,blood_RNA_T-reg,blood_RNA_total_PBMC,brain_RNA_amygdala,brain_RNA_basal_ganglia,brain_RNA_cerebellum,brain_RNA_cerebral_cortex,brain_RNA_hippocampal_formation,brain_RNA_hypothalamus,brain_RNA_midbrain,brain_RNA_olfactory_region,brain_RNA_pons_and_medulla,brain_RNA_thalamus&format=tsv)) in order to create mappings between HPA cell and tissue type strings to Uber-Anatomy, Cell Ontology, and Cell Line Ontology concepts (see [human-protein-atlas](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-protein-atlas) for details on the mapping process). This mapping is then used to create the following edge types:  \n",
    "- RNA-cell Line  \n",
    "- RNA-tissue type   \n",
    "- Protein-cell Line  \n",
    "- Protein-tissue type  \n",
    "\n",
    "\n",
    "**Output:**  \n",
    "- All HPA tissue and cell type strings ➞ [`HPA_tissues.txt`](https://www.dropbox.com/s/m0spn8h1l8kxb61/HPA_tissues.txt?dl=1)  \n",
    "- Mapping HPA strings to ontology concepts (documentation) ➞ [`zooma_tissue_cell_mapping_04JAN2020.xlsx`](https://www.dropbox.com/s/lxp8vxj39eumvcn/zooma_tissue_cell_mapping_04JAN2020.xlsx?dl=1)  \n",
    "- Final HPA-ontology mappings ➞ [`HPA_TISSUE_CELL_MAP.txt`](https://www.dropbox.com/s/dsh1x88u6251w76/HPA_TISSUE_CELL_MAP.txt?dl=1)\n",
    "- HPA Edges ➞ [`HPA_RNA_GENE_PROTEIN_EDGES.txt`](https://www.dropbox.com/s/vww5mif0i8h7bhj/HPA_RNA_GENE_PROTEIN_EDGES.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.proteinatlas.org/api/search_download.php?search=&columns=g,eg,up,pe,rnatsm,rnaclsm,rnacasm,rnabrsm,rnabcsm,rnablsm,scl,t_RNA_adipose_tissue,t_RNA_adrenal_gland,t_RNA_amygdala,t_RNA_appendix,t_RNA_basal_ganglia,t_RNA_bone_marrow,t_RNA_breast,t_RNA_cerebellum,t_RNA_cerebral_cortex,t_RNA_cervix,_uterine,t_RNA_colon,t_RNA_corpus_callosum,t_RNA_ductus_deferens,t_RNA_duodenum,t_RNA_endometrium_1,t_RNA_epididymis,t_RNA_esophagus,t_RNA_fallopian_tube,t_RNA_gallbladder,t_RNA_heart_muscle,t_RNA_hippocampal_formation,t_RNA_hypothalamus,t_RNA_kidney,t_RNA_liver,t_RNA_lung,t_RNA_lymph_node,t_RNA_midbrain,t_RNA_olfactory_region,t_RNA_ovary,t_RNA_pancreas,t_RNA_parathyroid_gland,t_RNA_pituitary_gland,t_RNA_placenta,t_RNA_pons_and_medulla,t_RNA_prostate,t_RNA_rectum,t_RNA_retina,t_RNA_salivary_gland,t_RNA_seminal_vesicle,t_RNA_skeletal_muscle,t_RNA_skin_1,t_RNA_small_intestine,t_RNA_smooth_muscle,t_RNA_spinal_cord,t_RNA_spleen,t_RNA_stomach_1,t_RNA_testis,t_RNA_thalamus,t_RNA_thymus,t_RNA_thyroid_gland,t_RNA_tongue,t_RNA_tonsil,t_RNA_urinary_bladder,t_RNA_vagina,t_RNA_B-cells,t_RNA_dendritic_cells,t_RNA_granulocytes,t_RNA_monocytes,t_RNA_NK-cells,t_RNA_T-cells,t_RNA_total_PBMC,cell_RNA_A-431,cell_RNA_A549,cell_RNA_AF22,cell_RNA_AN3-CA,cell_RNA_ASC_diff,cell_RNA_ASC_TERT1,cell_RNA_BEWO,cell_RNA_BJ,cell_RNA_BJ_hTERT+,cell_RNA_BJ_hTERT+_SV40_Large_T+,cell_RNA_BJ_hTERT+_SV40_Large_T+_RasG12V,cell_RNA_CACO-2,cell_RNA_CAPAN-2,cell_RNA_Daudi,cell_RNA_EFO-21,cell_RNA_fHDF/TERT166,cell_RNA_HaCaT,cell_RNA_HAP1,cell_RNA_HBEC3-KT,cell_RNA_HBF_TERT88,cell_RNA_HDLM-2,cell_RNA_HEK_293,cell_RNA_HEL,cell_RNA_HeLa,cell_RNA_Hep_G2,cell_RNA_HHSteC,cell_RNA_HL-60,cell_RNA_HMC-1,cell_RNA_HSkMC,cell_RNA_hTCEpi,cell_RNA_hTEC/SVTERT24-B,cell_RNA_hTERT-HME1,cell_RNA_HUVEC_TERT2,cell_RNA_K-562,cell_RNA_Karpas-707,cell_RNA_LHCN-M2,cell_RNA_MCF7,cell_RNA_MOLT-4,cell_RNA_NB-4,cell_RNA_NTERA-2,cell_RNA_PC-3,cell_RNA_REH,cell_RNA_RH-30,cell_RNA_RPMI-8226,cell_RNA_RPTEC_TERT1,cell_RNA_RT4,cell_RNA_SCLC-21H,cell_RNA_SH-SY5Y,cell_RNA_SiHa,cell_RNA_SK-BR-3,cell_RNA_SK-MEL-30,cell_RNA_T-47d,cell_RNA_THP-1,cell_RNA_TIME,cell_RNA_U-138_MG,cell_RNA_U-2_OS,cell_RNA_U-2197,cell_RNA_U-251_MG,cell_RNA_U-266/70,cell_RNA_U-266/84,cell_RNA_U-698,cell_RNA_U-87_MG,cell_RNA_U-937,cell_RNA_WM-115,blood_RNA_basophil,blood_RNA_classical_monocyte,blood_RNA_eosinophil,blood_RNA_gdT-cell,blood_RNA_intermediate_monocyte,blood_RNA_MAIT_T-cell,blood_RNA_memory_B-cell,blood_RNA_memory_CD4_T-cell,blood_RNA_memory_CD8_T-cell,blood_RNA_myeloid_DC,blood_RNA_naive_B-cell,blood_RNA_naive_CD4_T-cell,blood_RNA_naive_CD8_T-cell,blood_RNA_neutrophil,blood_RNA_NK-cell,blood_RNA_non-classical_monocyte,blood_RNA_plasmacytoid_DC,blood_RNA_T-reg,blood_RNA_total_PBMC,brain_RNA_amygdala,brain_RNA_basal_ganglia,brain_RNA_cerebellum,brain_RNA_cerebral_cortex,brain_RNA_hippocampal_formation,brain_RNA_hypothalamus,brain_RNA_midbrain,brain_RNA_olfactory_region,brain_RNA_pons_and_medulla,brain_RNA_thalamus&format=tsv'\n",
    "data_downloader(url, unprocessed_data_location, 'proteinatlas_search.tsv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Read in Data Files_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa = pandas.read_csv(unprocessed_data_location + 'proteinatlas_search.tsv',\n",
    "                      header = 0,\n",
    "                      delimiter = '\\t')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "hpa.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Mapping Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve terms to map\n",
    "terms_to_map = list(hpa.columns)\n",
    "\n",
    "# write results\n",
    "with open(unprocessed_data_location + 'HPA_tissues.txt', 'w') as outfile:\n",
    "    for x in tqdm(terms_to_map):\n",
    "        if x.endswith('[NX]'):\n",
    "            term = x.split('RNA - ')[-1].split(' [NX]')[:-1][0]\n",
    "            outfile.write(term + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back in mapped tissue/cell data\n",
    "hpa_mapping_data = pandas.read_excel(open(unprocessed_data_location + 'zooma_tissue_cell_mapping_04JAN2020.xlsx', 'rb'),\n",
    "                                     sheet_name='Concept_Mapping - 04JAN2020',\n",
    "                                     header=0)\n",
    "\n",
    "hpa_mapping_data.fillna('None', inplace=True)\n",
    "\n",
    "# preview data\n",
    "hpa_mapping_data.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'HPA_TISSUE_CELL_MAP.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(hpa_mapping_data.iterrows(), total=hpa_mapping_data.shape[0]):\n",
    "        if row['UBERON ID'] != 'None':\n",
    "            outfile.write(str(row['ORIGINAL TERM']).strip() + '\\t' + str(row['UBERON ID']).strip() + '\\n')\n",
    "\n",
    "        if row['CL ID'] != 'None':\n",
    "            outfile.write(str(row['ORIGINAL TERM']).strip() + '\\t' + str(row['CL ID']).strip() + '\\n')\n",
    "        \n",
    "        if row['CLO ID'] != 'None':\n",
    "            outfile.write(str(row['ORIGINAL TERM']).strip() + '\\t' + str(row['CLO ID']).strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_data = pandas.read_csv(processed_data_location + 'HPA_TISSUE_CELL_MAP.txt',\n",
    "                           header = None,\n",
    "                           names=['Tissue/Cell TERM', 'ONTOLOGY_IDs'],\n",
    "                           delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} edges'.format(edge_count=len(hpa_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Edge Data Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'HPA_RNA_GENE_PROTEIN_EDGES.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(hpa.iterrows(), total=hpa.shape[0]):\n",
    "        if row['RNA tissue specific NX'] != 'None':\n",
    "            for x in row['RNA tissue specific NX'].split(';'):\n",
    "                outfile.write(row['Ensembl'] + '\\t' + row['Gene'] + '\\t' + row['Uniprot'] + '\\t' + row['Evidence'] + '\\t' + 'anatomy' + '\\t' + x.split(':')[0] + '\\n')\n",
    "\n",
    "        if row['RNA cell line specific NX'] != 'None':\n",
    "            for x in row['RNA cell line specific NX'].split(';'):\n",
    "                outfile.write(row['Ensembl'] + '\\t' + row['Gene'] + '\\t' + row['Uniprot'] + '\\t' + row['Evidence'] + '\\t' + 'cell line' + '\\t' + x.split(':')[0] + '\\n')\n",
    "\n",
    "        if row['RNA brain regional specific NX'] != 'None':\n",
    "            for x in row['RNA brain regional specific NX'].split(';'):\n",
    "                outfile.write(row['Ensembl'] + '\\t' + row['Gene'] + '\\t' + row['Uniprot'] + '\\t' + row['Evidence'] + '\\t' + 'anatomy' + '\\t' + x.split(':')[0] + '\\n')\n",
    "\n",
    "        if row['RNA blood cell specific NX'] != 'None':\n",
    "            for x in row['RNA blood cell specific NX'].split(';'):\n",
    "                outfile.write(row['Ensembl'] + '\\t' + row['Gene'] + '\\t' + row['Uniprot'] + '\\t' + row['Evidence'] + '\\t' + 'anatomy' + '\\t' + x.split(':')[0] + '\\n')\n",
    "\n",
    "        if row['RNA blood lineage specific NX'] != 'None':\n",
    "            for x in row['RNA blood lineage specific NX'].split(';'):\n",
    "                outfile.write(row['Ensembl'] + '\\t' + row['Gene'] + '\\t' + row['Uniprot'] + '\\t' + row['Evidence'] + '\\t' + 'anatomy' + '\\t' + x.split(':')[0] + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_edges = pandas.read_csv(processed_data_location + 'HPA_RNA_GENE_PROTEIN_EDGES.txt',\n",
    "                           header = None,\n",
    "                           names=['Ensembl_IDs', 'Gene_Symbols', 'Uniport_IDs', 'Evidence', 'Anatomy_Type', 'Anatomy'],\n",
    "                           delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} edges'.format(edge_count=len(hpa_edges)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_edges.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### CREATE EDGE DATASETS  <a class=\"anchor\" id=\"create-edge-datasets\"></a>\n",
    "***\n",
    "***\n",
    "\n",
    "### Ontologies  <a class=\"anchor\" id=\"ontologies\"></a>\n",
    "***\n",
    "- [Protein Ontology](#protein-ontology)  \n",
    "- [Relations Ontology](#relations-ontology)  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein Ontology <a class=\"anchor\" id=\"protein-ontology\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [protein-ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-phenotype-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the [pr.owl](http://purl.obolibrary.org/obo/pr.owl) file from [ProConsortium.org](https://proconsortium.org/) in order to create a version of the ontology that contains only human proteins. This is achieved by performing forward and reverse breadth first search over all proteins which are `owl:subClassOf` [Homo sapiens protein](https://proconsortium.org/app/entry/PR%3A000029067/).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Output:**  \n",
    "- Human Protein Ontology ➞ [`human_pro.owl`](https://www.dropbox.com/s/jw8jksgnqbcz9sm/human_pro.owl?dl=1)\n",
    "- Classified Human Protein Ontology (Hermit) ➞ [`human_pro_closed.owl`](https://www.dropbox.com/s/6ux85agl95ja3wx/human_pro_closed.owl?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://purl.obolibrary.org/obo/pr.owl'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ontology as graph (the ontology is large so this takes ~60 minutes) - 11,757,623 edges on 12/18/2019\n",
    "graph = Graph()\n",
    "graph.parse(unprocessed_data_location + 'pr.owl')\n",
    "\n",
    "print('There are {} edges in the ontology'.format(len(graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Ontology to Directed MulitGraph:**  \n",
    "In order to create a version of the ontology which includes all relevant human edges, we need to first convert the KG to a [directed multigraph](https://networkx.github.io/documentation/stable/reference/classes/multidigraph.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert RDF graph to multidigraph (the ontology is large so this takes ~45 minutes)\n",
    "networkx_mdg = rdflib_to_networkx_multidigraph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Human Proteins:**   \n",
    "A list of human proteins is obtained by querying the ontology to return all ontology classes `only_in_taxon some Homo sapiens`. To expedite the query time, the following SPARQL query is run from the [ProConsortium](https://proconsortium.org/pro_sparql.shtml) SPARQL endpoint: \n",
    "\n",
    "```SPARQL\n",
    "PREFIX obo: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT ?PRO_term\n",
    "FROM <http://purl.obolibrary.org/obo/pr>\n",
    "WHERE {\n",
    "       ?PRO_term rdf:type owl:Class .\n",
    "       ?PRO_term rdfs:subClassOf ?restriction .\n",
    "       ?restriction owl:onProperty obo:RO_0002160 .\n",
    "       ?restriction owl:someValuesFrom obo:NCBITaxon_9606 .\n",
    "\n",
    "       # use this to filter-out things like hgnc ids\n",
    "       FILTER (regex(?PRO_term,\"http://purl.obolibrary.org/obo/*\")) .\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data - pro classes only_in_taxon some Homo sapiens (61,064 classes on 12/18/2019)\n",
    "url = 'http://sparql.proconsortium.org/virtuoso/sparql?query=PREFIX+obo%3A+%3Chttp%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F%3E%0D%0ASELECT+%3FPRO_term%0D%0AFROM+%3Chttp%3A%2F%2Fpurl.obolibrary.org%2Fobo%2Fpr%3E%0D%0AWHERE%0D%0A%7B%0D%0A+++%3FPRO_term+rdf%3Atype+owl%3AClass+.%0D%0A+++%3FPRO_term+rdfs%3AsubClassOf+%3Frestriction+.%0D%0A+++%3Frestriction+owl%3AonProperty+obo%3ARO_0002160+.%0D%0A+++%3Frestriction+owl%3AsomeValuesFrom+obo%3ANCBITaxon_9606+.%0D%0A%0D%0A+++FILTER+%28regex%28%3FPRO_term%2C%22http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F*%22%29%29+.%0D%0A%0D%0A%7D%0D%0A&format=text%2Fhtml&debug='\n",
    "html = requests.get(url, allow_redirects=True).content\n",
    "\n",
    "# extract data from html table\n",
    "df_list = pandas.read_html(html)\n",
    "human_pro_classes = list(df_list[-1]['PRO_term'])\n",
    "\n",
    "print('There are {protein_count} human classes in the PRO ontology'.format(protein_count=len(human_pro_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct Human PRO:**   \n",
    "Now that we have all of the paths from the original graph that are relevant to humans, we can construct a human-only version of the PRotein ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new graph using bfs paths\n",
    "human_pro_graph = Graph()\n",
    "human_networkx_mdg = networkx.MultiDiGraph()\n",
    "\n",
    "for node in tqdm(human_pro_classes):\n",
    "    forward = list(networkx.edge_bfs(networkx_mdg, URIRef(node), orientation='original'))\n",
    "    reverse = list(networkx.edge_bfs(networkx_mdg, URIRef(node), orientation='reverse'))\n",
    "    \n",
    "    # add edges from forward and reverse bfs paths\n",
    "    for path in forward + reverse:\n",
    "        human_pro_graph.add((path[0], path[2], path[1]))\n",
    "        human_networkx_mdg.add_edge(path[0], path[1], path[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the constructed ontology only has 1 component\n",
    "networkx.number_connected_components(human_networkx_mdg.to_undirected())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered ontology\n",
    "human_pro_graph.serialize(destination=unprocessed_data_location + 'human_pro.owl', format='xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify Ontology:**  \n",
    "To ensure that we have correclty built the new ontology, we run the hermit reasoner over it to ensure that there are no incomplete triples or inconsistent classes. In order to do this, we will call the reasoner using [OWLTools](https://github.com/owlcollab/owltools), which this script assumes has already been downloaded to the `../resources/lib` directory. The following arguments are then called to run the reasoner (from the command line):  \n",
    "\n",
    "```bash\n",
    "./resources/lib/owltools ./resources/unprocessed_data/human_pro_filtered.owl --reasoner hermit --run-reasoner --assert-implied -o ./resources/processed_data/human_pro_closed.owl\n",
    "```\n",
    "\n",
    "_**Note.** This step takes around 30-45 minutes to run. When run from the command line the reasoner determined that the ontology was consistent and 174 new axioms were inferrred (12/18/2019)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run reasoner -- RUN FROM COMMAND LINE NOT HERE\n",
    "# subprocess.run(['../../resources/lib/owltools',\n",
    "#                 '../../resources/unprocessed_data/human_pro_filtered.owl',\n",
    "#                 '--reasoner hermit',\n",
    "#                 '--run-reasoner',\n",
    "#                 '--assert-implied',\n",
    "#                 '--list-unsatisfiable',\n",
    "#                 '-o ./resources/processed_data/human_pro_closed.owl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine Cleaned Human PRO:**  \n",
    "Once we have cleaned the ontology we can get counts of components, nodes, edges, and then write the cleaned graph to the `../../resources/processed_data` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of connected components\n",
    "pro_human_graph = Graph()\n",
    "pro_human_graph.parse(processed_data_location + 'human_pro_closed.owl')\n",
    "\n",
    "# get node and edge count\n",
    "edge_count = len(human_pro_graph)\n",
    "node_count = len(set([str(node) for edge in list(human_pro_graph) for node in edge[0::2]]))\n",
    "\n",
    "print('\\n The classified, filtered Human version of PRO contains {node} nodes and {edge} edges\\n'.format(node=node_count, edge=edge_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations Ontology <a class=\"anchor\" id=\"relations-ontology\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [RO](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#relation-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the [ro.owl](http://purl.obolibrary.org/obo/ro.owl) file from [obofoundry.org](http://www.obofoundry.org/) in order to obtain all `ObjectProperties` and their inverse relations.  \n",
    "\n",
    "**Output:** \n",
    "- Relations and Inverse Relations ➞ [`INVERSE_RELATIONS.txt`](https://www.dropbox.com/s/sd8qlib8f6gqyz4/INVERSE_RELATIONS.txt?dl=1)\n",
    "- Relations and Labels ➞ [`RELATIONS_LABELS.txt`](https://www.dropbox.com/s/k2hm9p0r8l9ecj3/RELATIONS_LABELS.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://purl.obolibrary.org/obo/ro.owl'\n",
    "data_downloader(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_graph = Graph()\n",
    "ro_graph.parse(unprocessed_data_location + 'ro.owl')\n",
    "\n",
    "print('There are {} edges in the ontology'.format(len(ro_graph))) #5,669 edges on 12/15/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "**Identify Relations and Inverse Relations:**  \n",
    "Identify all relations and their inverse relations using the `owl:inverseOf` property. To make it easier to look up the inverse relations, each pair is listed twice, for example:  \n",
    "- [location of](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001015) `owl:inverseOf` [located in](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001025)  \n",
    "- [located in](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001025) `owl:inverseOf` [location of](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/relations_data/INVERSE_RELATIONS.txt', 'w') as outfile:\n",
    "    \n",
    "    # write column names\n",
    "    outfile.write('Relation' + '\\t' + 'Inverse_Relation' + '\\n')\n",
    "    \n",
    "    # manually add missing relations\n",
    "    outfile.write('RO_0000056' + '\\t' + 'RO_0000057' + '\\n') #participates_in/has_participant\n",
    "    outfile.write('RO_0000057' + '\\t' + 'RO_0000056' + '\\n') #participates_in/has_participant\n",
    "    outfile.write('RO_0000085' + '\\t' + 'RO_0000079' + '\\n') #has_function/function_of\n",
    "    outfile.write('RO_0000079' + '\\t' + 'RO_0000085' + '\\n') #has_function/function_of\n",
    "    outfile.write('RO_0001025' + '\\t' + 'RO_0001015' + '\\n') #located_in/has_location\n",
    "    outfile.write('RO_0001015' + '\\t' + 'RO_0001025' + '\\n') #located_in/has_location\n",
    "\n",
    "    # find inverse relations\n",
    "    for s, p, o in tqdm(ro_graph):\n",
    "        if 'owl#inverseOf' in str(p):\n",
    "            if 'RO' in str(s) and 'RO' in str(o):\n",
    "                outfile.write(str(s.split('/')[-1]) + '\\t' + str(o.split('/')[-1]) + '\\n')\n",
    "                outfile.write(str(o.split('/')[-1]) + '\\t' + str(s.split('/')[-1]) + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data = pandas.read_csv('./resources/relations_data/INVERSE_RELATIONS.txt',\n",
    "                          header = 0,\n",
    "                          delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} RO Relations and Inverse Relations'.format(edge_count=len(ro_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Get Relations Labels:**  \n",
    "Identify all relations and their labels for use when building the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ro_graph.query(\n",
    "    \"\"\"SELECT DISTINCT ?p ?p_label\n",
    "           WHERE {\n",
    "              ?p rdf:type owl:ObjectProperty .\n",
    "              ?p rdfs:label ?p_label . }\n",
    "           \"\"\", initNs={\"rdf\": 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "                        \"rdfs\": 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "                        \"owl\": 'http://www.w3.org/2002/07/owl#'})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to file\n",
    "with open('./resources/relations_data/RELATIONS_LABELS.txt', 'w') as outfile:\n",
    "    \n",
    "    # write column names\n",
    "    outfile.write('Relation' + '\\t' + 'Label' + '\\n')\n",
    "\n",
    "    for p, p_label in list(results):\n",
    "        outfile.write(str(p).split('/')[-1] + '\\t' + str(p_label) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data_label = pandas.read_csv('./resources/relations_data/RELATIONS_LABELS.txt',\n",
    "                                header = 0,\n",
    "                                delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} RO Relations and Labels'.format(edge_count=len(ro_data_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data_label.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Linked Data <a class=\"anchor\" id=\"linked-data\"></a>\n",
    "***\n",
    "* [Clinvar Variant-Diseases and Phenotypes](#clinvar-variant)\n",
    "* [NCBI Gene Protein-Coding Genes and Proteins](#ncbi-protein-coding-genes)  \n",
    "* [Reactome Chemical-Complex Data](#reactome-chemical-complex)  \n",
    "* [Reactome Complex-Complex Data](#reactome-complex-complex)  \n",
    "* [Reactome Protein-Complex Data](#reactome-protein-complex)  \n",
    "* [Uniprot Protein-Cofactor and Protein-Catalyst](#uniprot-protein-cofactorcatalyst)  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinvar Variant-Diseases and Phenotypes <a class=\"anchor\" id=\"clinvar-variant\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Clinvar](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#clinvar)  \n",
    "\n",
    "**Purpose:** This script downloads the [variant_summary.txt](ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz) file from [CLinVar](https://www.ncbi.nlm.nih.gov/clinvar/) in order to create the following edges:  \n",
    "- gene-variant  \n",
    "- variant-disease  \n",
    "- variant-phenotype  \n",
    "\n",
    "**Output:** [`CLINVAR_VARIANT_GENE_DISEASE_PHENOTYPE_EDGES.txt`](https://www.dropbox.com/s/1doj3lj46ufgdpd/CLINVAR_VARIANT_GENE_DISEASE_PHENOTYPE_EDGES.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data and provided labels (needed to unnest data)\n",
    "clinvar_data = pandas.read_csv(unprocessed_data_location + 'variant_summary.txt',\n",
    "                               header = 0,\n",
    "                               delimiter = '\\t',\n",
    "                               low_memory=False)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "clinvar_data.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode nested data\n",
    "explode_df_clinvar = explode(clinvar_data.copy(), ['PhenotypeIDS'], ';')\n",
    "explode_df_clinvar = explode(explode_df_clinvar.copy(), ['PhenotypeIDS'], ',')\n",
    "\n",
    "# edit column formatting\n",
    "explode_df_clinvar['PhenotypeIDS'].replace('Orphanet:ORPHA','ORPHA:', inplace=True, regex=True)\n",
    "explode_df_clinvar['PhenotypeIDS'].replace('Human Phenotype Ontology:HP:','HP_', inplace=True, regex=True)\n",
    "\n",
    "# write data\n",
    "explode_df_clinvar.to_csv(processed_data_location + 'CLINVAR_VARIANT_GENE_DISEASE_PHENOTYPE_EDGES.txt', header = True, sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {edge_count} variant edges'.format(edge_count=len(explode_df_clinvar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview data\n",
    "explode_df_clinvar.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCBI Gene Protein-Coding Gene-Protein <a class=\"anchor\" id=\"ncbi-protein-coding-genes\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Uniprot](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#uniprot-knowledgebase) \n",
    "\n",
    "**Purpose:** This script utilizes the merged data created in the [Human-Transcript, Gene, and Protein Identifier Mapping](#Human-Transcript,-Gene,-and-Protein-Identifier-Mapping) subsection in order to create the following edges:  \n",
    "- gene-protein\n",
    "\n",
    "**Output:** [`PROTEIN_CODING_GENES_PROTEINS.txt`](https://www.dropbox.com/s/79ce6oe68jt72ph/PROTEIN_CODING_GENES_PROTEINS.txt?dl=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-dup data\n",
    "df_ens = merged_data_clean.drop_duplicates(subset=['GeneID_Cleaned', 'pro_id'], keep='first', inplace=False) \n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'PROTEIN_CODING_GENES_PROTEINS.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(df_ens.iterrows(), total=df_ens.shape[0]):\n",
    "        if (row['GeneID_Cleaned'] != 'None' and row['pro_id'] != 'None') and row['type_of_gene'] == 'protein-coding': \n",
    "            outfile.write(row['GeneID_Cleaned'].strip() + '\\t' + row['pro_id'].replace('PR:', 'PR_').strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpe_data = pandas.read_csv(processed_data_location + 'PROTEIN_CODING_GENES_PROTEINS.txt',\n",
    "                           header = None,\n",
    "                           names=['Entrez_Gene_IDs', 'Protein_Ontology_IDs'],\n",
    "                           delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} protein-coding gene edges'.format(edge_count=len(hpe_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(list(hpe_data['Entrez_Gene_IDs'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpe_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reactome Chemical-Complex Data <a class=\"anchor\" id=\"reactome-chemical-complex\"></a>\n",
    "\n",
    "**Data Souurce Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#reactome-pathway-database)  \n",
    "\n",
    "**Purpose:** This script downloads the [ComplexParticipantsPubMedIdentifiers_human.txt](https://reactome.org/download/current/ComplexParticipantsPubMedIdentifiers_human.txt) file from [Reactome](https://reactome.orgt) in order to create the following edges:  \n",
    "- chemical-complex  \n",
    "\n",
    "**Output:** [`REACTOME_CHEMICAL_COMPLEX.txt`](https://www.dropbox.com/s/qoetjt0vfy6qb3y/REACTOME_CHEMICAL_COMPLEX.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "data = open(unprocessed_data_location + 'ComplexParticipantsPubMedIdentifiers_human.txt').readlines()\n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'REACTOME_CHEMICAL_COMPLEX.txt', 'w') as outfile:\n",
    "    for line in tqdm(data[1:]):\n",
    "        row = line.split('\\t')\n",
    "        \n",
    "        if (row[0].strip().startswith('R-HSA') or row[0].strip().startswith('R-ALL')):\n",
    "            # find all proteins in a complex\n",
    "            for x in row[2].split('|'):\n",
    "                if x.startswith('chebi:'):            \n",
    "                    outfile.write(x.replace('chebi:', 'CHEBI_') + '\\t' + row[0].strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1_data = pandas.read_csv(processed_data_location + 'REACTOME_CHEMICAL_COMPLEX.txt',\n",
    "                           header = None,\n",
    "                           names=['CHEBI_IDs', 'Reactome_IDs'],\n",
    "                           delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} chemical-complex edges'.format(edge_count=len(cc1_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reactome Complex-Complex Data <a class=\"anchor\" id=\"reactome-complex-complex\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#reactome-pathway-database)  \n",
    "\n",
    "**Purpose:** This script downloads the [ComplexParticipantsPubMedIdentifiers_human.txt](https://reactome.org/download/current/ComplexParticipantsPubMedIdentifiers_human.txt) file from [Reactome](https://reactome.orgt) in order to create the following edges:  \n",
    "- complex-complex  \n",
    "\n",
    "**Output:** [`REACTOME_COMPLEX_COMPLEX.txt`](https://www.dropbox.com/s/sojaq8u3hwfw4jz/REACTOME_COMPLEX_COMPLEX.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label dictionary\n",
    "labels = pandas.read_csv(unprocessed_data_location + 'ComplexParticipantsPubMedIdentifiers_human.txt',\n",
    "                         header = 0,\n",
    "                         delimiter = '\\t')\n",
    "\n",
    "# convert to dictionary\n",
    "label_dict = {row[0]:row[1] for idx, row in labels.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "data = open(unprocessed_data_location + 'ComplexParticipantsPubMedIdentifiers_human.txt').readlines()\n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'REACTOME_COMPLEX_COMPLEX.txt', 'w') as outfile:\n",
    "    for line in tqdm(data[1:]):\n",
    "        row = line.split('\\t')\n",
    "        \n",
    "        if row[0].strip().startswith('R-HSA'):\n",
    "            # find all complexes\n",
    "            for x in row[3].split('|'):\n",
    "                if (x.startswith('R-HSA-') or x.startswith('R-ALL-')) and x.strip() in label_dict.keys():            \n",
    "                    outfile.write(row[0].strip() + '\\t' + x.strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data = pandas.read_csv(processed_data_location + 'REACTOME_COMPLEX_COMPLEX.txt',\n",
    "                          header = None,\n",
    "                          names=['Reactome_Complex_u', 'Reactome_Complex_v'],\n",
    "                          delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} complex-complex edges'.format(edge_count=len(cc_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reactome Complex-Pathway Data <a class=\"anchor\" id=\"reactome-complex-pathway\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#reactome-pathway-database)  \n",
    "\n",
    "**Purpose:** This script downloads the [Complex_2_Pathway_human.txt](https://reactome.org/download/current/Complex_2_Pathway_human.txt) file from [Reactome](https://reactome.orgt) in order to create the following edges:  \n",
    "- complex-pathway  \n",
    "\n",
    "**Output:** [`REACTOME_COMPLEX_PATHWAY.txt`](https://www.dropbox.com/s/my03w16fjw7bt20/REACTOME_COMPLEX_PATHWAY.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://reactome.org/download/current/Complex_2_Pathway_human.txt'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "data = open(unprocessed_data_location + 'Complex_2_Pathway_human.txt').readlines()\n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'REACTOME_COMPLEX_PATHWAY.txt', 'w') as outfile:\n",
    "    for line in tqdm(data[1:]):\n",
    "        row = line.split('\\t')\n",
    "        if row[0].startswith('R-HSA-'):            \n",
    "            outfile.write(row[0].strip() + '\\t' + row[1].strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previewed Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data = pandas.read_csv(processed_data_location + 'REACTOME_COMPLEX_PATHWAY.txt',\n",
    "                          header = None,\n",
    "                          names=['Reactome_Complex', 'Reactome_Pathway'],\n",
    "                          delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} complex-pathway edges'.format(edge_count=len(cp_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reactome Protein-Complex Data <a class=\"anchor\" id=\"reactome-protein-complex\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#reactome-pathway-database)  \n",
    "\n",
    "**Purpose:** This script downloads the [ComplexParticipantsPubMedIdentifiers_human.txt](https://reactome.org/download/current/ComplexParticipantsPubMedIdentifiers_human.txt) file from [Reactome](https://reactome.org) in order to create the following edges:  \n",
    "- protein-complex\n",
    "\n",
    "**Output:** [`REACTOME_PROTEIN_COMPLEX.txt`](https://www.dropbox.com/s/7meu0cdz1mrnsz7/REACTOME_PROTEIN_COMPLEX.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://reactome.org/download/current/ComplexParticipantsPubMedIdentifiers_human.txt'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "data = open(unprocessed_data_location + 'ComplexParticipantsPubMedIdentifiers_human.txt').readlines()\n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'REACTOME_PROTEIN_COMPLEX.txt', 'w') as outfile:\n",
    "    for line in tqdm(data):\n",
    "        row = line.split('\\t')\n",
    "        \n",
    "        if row[0].strip().startswith('R-HSA'):\n",
    "            # find all proteins in a complex\n",
    "            for x in row[2].split('|'):\n",
    "                if x.startswith('uniprot:'):            \n",
    "                    outfile.write(x.split(':')[-1].strip() + '\\t' + row[0].strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_data = pandas.read_csv(processed_data_location + 'REACTOME_PROTEIN_COMPLEX.txt',\n",
    "                       header = None,\n",
    "                       names=['Uniprot_Protein', 'Reactome_Complex'],\n",
    "                       delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} protein-complex edges'.format(edge_count=len(pc_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniprot  Protein-Cofactor and Protein-Catalyst <a class=\"anchor\" id=\"uniprot-protein-cofactorcatalyst\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Uniprot](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#uniprot-knowledgebase)  \n",
    "\n",
    "**Purpose:** This script downloads the [uniprot-cofactor-catalyst.tab](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#uniprot-knowledgebase) file from the [Uniprot Knowledge Base](https://www.uniprot.org) in order to create the following edges:  \n",
    "- protein-cofactor  \n",
    "- protein-catalyst  \n",
    "\n",
    "**Output:**  \n",
    "- protein-cofactor ➞ [`UNIPROT_PROTEIN_COFACTOR.txt`](https://www.dropbox.com/s/ij9t89botd8nmmj/UNIPROT_PROTEIN_COFACTOR.txt?dl=1)\n",
    "- protein-catalyst ➞ [`UNIPROT_PROTEIN_CATALYST.txt`](https://www.dropbox.com/s/pvopvs0iq8x3oq2/UNIPROT_PROTEIN_CATALYST.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.uniprot.org/uniprot/?query=&fil=organism%3A%22Homo%20sapiens%20(Human)%20%5B9606%5D%22&columns=id%2Centry%20name%2Creviewed%2Cdatabase(PRO)%2Cchebi(Cofactor)%2Cchebi(Catalytic%20activity)&format=tab'\n",
    "data_downloader(url, unprocessed_data_location, 'uniprot-cofactor-catalyst.tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(unprocessed_data_location + 'uniprot-cofactor-catalyst.tab').readlines()\n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'UNIPROT_PROTEIN_COFACTOR.txt', 'w') as outfile1, open(processed_data_location + 'UNIPROT_PROTEIN_CATALYST.txt', 'w') as outfile2:\n",
    "    for line in tqdm(data):\n",
    "\n",
    "        # get cofactors\n",
    "        if 'CHEBI' in line.split('\\t')[4]: \n",
    "            for i in line.split('\\t')[4].split(';'):\n",
    "                chebi = i.split('[')[-1].replace(']', '').replace(':', '_')\n",
    "                outfile1.write('PR_' + line.split('\\t')[3].strip(';') + '\\t' + chebi + '\\n')\n",
    "        \n",
    "        # get catalysts\n",
    "        if 'CHEBI' in line.split('\\t')[5]:       \n",
    "            for i in line.split('\\t')[5].split(';'):\n",
    "                chebi = i.split('[')[-1].replace(']', '').replace(':', '_')\n",
    "                outfile2.write('PR_' + line.split('\\t')[3].strip(';') + '\\t' + chebi + '\\n')\n",
    "\n",
    "outfile1.close()\n",
    "outfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Processed Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Cofactor Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp1_data = pandas.read_csv(processed_data_location + 'UNIPROT_PROTEIN_COFACTOR.txt',\n",
    "                            header = None,\n",
    "                            names=['Protein_Ontology_IDs', 'CHEBI_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} protein-cofactor edges'.format(edge_count=len(pcp1_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp1_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Catalyst Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp2_data = pandas.read_csv(processed_data_location + 'UNIPROT_PROTEIN_CATALYST.txt',\n",
    "                            header = None,\n",
    "                            names=['Protein_Ontology_IDs', 'CHEBI_IDs'],\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "print('There are {edge_count} protein-catalyst edges'.format(edge_count=len(pcp2_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp2_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### INSTANCE METADATA <a class=\"anchor\" id=\"create-instance-metadata\"></a>\n",
    "***\n",
    "\n",
    "**Data Source Wiki Page:** [Dependencies](https://github.com/callahantiff/PheKnowLator/wiki/Dependencies/#node-metadata) \n",
    "\n",
    "**Purpose:** The goal of this section is to obtain metadata for each instance data source used in the knowledge graph. To determine which of the edges contains instance data, the [`Master_Edge_List_Dict.json`](https://www.dropbox.com/s/4j0vrwx26dh8hd1/Master_Edge_List_Dict.json?dl=1) file is parsed and saved to a nested dictionary (see example below). \n",
    "\n",
    "```python\n",
    "{\n",
    "  'complex': {\n",
    "              'chemical-complex': [[node_1, node_2]...[node_n, node_m]],\n",
    "              'complex-complex':  [[node_1, node_2]...[node_n, node_m]],\n",
    "              'complex-pathway':  [[node_1, node_2]...[node_n, node_m]],\n",
    "              },\n",
    "     'gene': {\n",
    "                'chemical-gene':  [[node_1, node_2]...[node_n, node_m]],\n",
    "                 'gene-disease':  [[node_1, node_2]...[node_n, node_m]],\n",
    "              }\n",
    "}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Once this dictionary is created, each major data type (examples shown in the list below) will be processed. For **[`Release V2.0.0`](https://github.com/callahantiff/PheKnowLator/wiki/v2.0.0)**, the following are instance data and require the compiling of metadata:\n",
    "- [Genes](#gene-metadata)\n",
    "- [RNA](#rna-metadata)\n",
    "- [Pathways](#pathway-metadata)\n",
    "- [Complexes](#complex-metadata)\n",
    "- [Reactions](#reaction-metadata)\n",
    "- [Variants](#variant-metadata)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "____\n",
    "\n",
    "**Metadata:** The <u>metadata</u> we will gather includes:  \n",
    "\n",
    "| **Metadata Type** | **Definition** | **Example Node**  | **Example Node Metadata** | \n",
    "| :---: | :---: | :---: | :---: | \n",
    "| Label | The primary label or name for the node | `R-HSA-1006173` | \"CFH:Host cell surface\" |       \n",
    "| Description | A definition or other useful details about the node | `rs794727058` | This `germline` `single nucleotide variant (allele alteration: C➞T)` located on chromosome `5 (GRCh38: NC_000005.10, start/stop positions (126555930/126555930))` with `pathogenic` clinical significance and a last review date of `2/23/2015` (review status: `criteria provided, single submitter`). |        \n",
    "| Synonym | Alternative terms used for a node | `81399` | \"OR1-1, OR7-21\" |           \n",
    "\n",
    "<br>\n",
    "\n",
    "The metadata information will be used to create the following edges in the knowledge graph:  \n",
    "- **Label** ➞ node `rdfs:label`  \n",
    "- **Description** ➞ node `obo:IAO_0000115` description \n",
    "- **Synonyms** ➞ node `oboInOwl:hasExactSynonym` synonym \n",
    "\n",
    "<br>\n",
    "\n",
    "*<b>NOTE.</b> All node metadata datasets are written to the `node_data` directory. The algorithm will look for data in this directory and if it is not there, then no node metadata will be created.*\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Metadata Dictionaries\n",
    "***\n",
    "\n",
    "**Purpose:** To create the resources needed in order to create metadata dictionaries, which are in turn used to obtain metadata for instance data nodes. This process has the following steps:\n",
    "\n",
    "**1. [Identify Instance Data Nodes](#identify-instance-data-nodes):** In order to automatically obtain the list of edges that include an instance data source and their corresponding edge lists, the `Master_Edge_List_Dict.json` is read in and processed.  \n",
    "  - <u>Input Data</u>: [`Master_Edge_List_Dict.json`](./resources/Master_Edge_List_Dict.json)  \n",
    "\n",
    "<br>\n",
    "\n",
    "**2. [Generate Metadata Dictionaries](#generate-metadata-dictionaries):** In order to efficiently obtain metadata for the instance data nodes identified in _Step 1_, we first read in the data for each node type (i.e. genes, rna, complexes, pathways, reactions, and variants) and convert it into a dictionary. Then, each metadata dictionary is saved to a `master_metadata_dictioanry`, keyed by node type.\n",
    "  - <u>Input Datasets</u>:  \n",
    "    - Genes ➞ [`Homo_sapiens.gene_info`](https://www.dropbox.com/s/vazlmzxydgv6xzz/Homo_sapiens.gene_info?dl=1)    \n",
    "    - RNA ➞ [`Merged_Human_Ensembl_Entrez_Uniprot_Identifiers.txt`](https://www.dropbox.com/s/l79166x1fx6vc4l/Merged_Human_Ensembl_Entrez_Uniprot_Identifiers.txt?dl=1) \n",
    "    - Complexes ➞ [`reactome2py API`](https://github.com/reactome/reactome2py)  \n",
    "    - Pathways ➞ [`reactome2py API`](https://github.com/reactome/reactome2py)  \n",
    "    - Reactions ➞ [`reactome2py API`](https://github.com/reactome/reactome2py)  \n",
    "    - Variants ➞ [`variant_summary.txt.gz`](ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz)  \n",
    "\n",
    "<br>\n",
    "\n",
    "**3. [Write Metadata Files](#write-metadata-files):** The Instance data node dictionary from _Step 1_ and metadata dictionaries from _Step 2_ are used to write `.txt` files for all `edge-type` data included in the instance node dictionary.\n",
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Instance Data Nodes  <a class=\"anchor\" id=\"identify-instance-data-nodes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data files for each edge type\n",
    "edge_data = json.load(open('./resources/Master_Edge_List_Dict.json', 'r'))\n",
    "edge_dict = {key:[edge_data[key]['data_type'], edge_data[key]['edge_list']] for key in edge_data.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort Data:** For all edges in the `edge_dict()` that include instance data, we create a new dictionary where each edge type is further organized by node from the edge type that references the instance data (e.g. from the `chemical-gene` edge type, the `gene` node references instance data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 51797.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# sort data files\n",
    "metadata_file_info = {}\n",
    "\n",
    "for edge in tqdm(edge_dict.keys()): \n",
    "    if 'instance' in edge_dict[edge][0]:\n",
    "        \n",
    "        # get instance type\n",
    "        inst_type = edge.split('-')[edge_dict[edge][0].split('-').index('instance')]\n",
    "        \n",
    "        # read in data\n",
    "        if inst_type in metadata_file_info.keys(): \n",
    "            metadata_file_info[inst_type][edge] = {}\n",
    "            metadata_file_info[inst_type][edge]['data'] = edge_dict[edge][1]\n",
    "            metadata_file_info[inst_type][edge]['instance_idx'] = edge_dict[edge][0]\n",
    "            \n",
    "        else:\n",
    "            metadata_file_info[inst_type] = {}\n",
    "            metadata_file_info[inst_type][edge] =  {}\n",
    "            metadata_file_info[inst_type][edge]['data'] =  edge_dict[edge][1]\n",
    "            metadata_file_info[inst_type][edge]['instance_idx'] = edge_dict[edge][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory to write node data to\n",
    "node_directory = './resources/node_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Metadata Dictionaries  <a class=\"anchor\" id=\"generate-metadata-dictionaries\"></a>\n",
    "In this step, the goal is to create a metadata dictionary for each node type that does not rely on API data. In this case, only the **Gene**, **RNA**, and **Variant** nodes require data that is not from an API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genes Metadata Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ncbi gene data\n",
    "ncbi_gene = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.gene_info',\n",
    "                            header = 0,\n",
    "                            delimiter = '\\t')\n",
    "\n",
    "\n",
    "# replace \"-\" with \"None\"\n",
    "ncbi_gene.replace('-','None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61645/61645 [00:15<00:00, 3943.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# create metadata\n",
    "genes, label, description, synonym = [], [], [], []\n",
    "\n",
    "for idx, row in tqdm(ncbi_gene.iterrows(), total=ncbi_gene.shape[0]):\n",
    "    \n",
    "    # node \n",
    "    if row['GeneID'] != 'None':\n",
    "        genes.append(row['GeneID'])\n",
    "    \n",
    "    # label -- only want metadata if there is a label\n",
    "    if row['Symbol'] != 'None':\n",
    "        label.append(row['Symbol'])\n",
    "    \n",
    "        # description\n",
    "        description.append('{desc} is a {gene} gene that is located on chromosome {chrom} (map_location: {map_loc}).'.format(desc=row['description'].title(),\n",
    "                                                                                                                            gene=row['type_of_gene'],\n",
    "                                                                                                                            chrom=row['chromosome'],\n",
    "                                                                                                                            map_loc=row['map_location']))\n",
    "        # synonym\n",
    "        if row['Synonyms'] != 'None' and row['Other_designations'] != 'None':\n",
    "            synonym.append(row['Synonyms'] + '|' + row['Other_designations'])\n",
    "\n",
    "        elif row['Synonyms'] != 'None' and row['Other_designations'] == 'None':\n",
    "            synonym.append(row['Synonyms'])\n",
    "\n",
    "        elif row['Synonyms'] == 'None' and row['Other_designations'] != 'None':\n",
    "            synonym.append(row['Other_designations'])\n",
    "\n",
    "        else:\n",
    "            synonym.append('None')\n",
    "            \n",
    "    \n",
    "# combine into new data frame        \n",
    "gene_metadata_final = pandas.DataFrame(list(zip(genes, label, description, synonym)), columns =['ID', 'Label', 'Description', 'Synonym'])\n",
    "\n",
    "# make all variables string\n",
    "gene_metadata_final = gene_metadata_final.astype(str)\n",
    "\n",
    "# convert df to dictionary\n",
    "gene_metadata_final.set_index('ID', inplace=True)\n",
    "gene_metadata_dict = gene_metadata_final.to_dict('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNA Metadata Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "rna_gene_data = pandas.read_csv(processed_data_location + 'Merged_Human_Ensembl_Entrez_Uniprot_Identifiers.txt',\n",
    "                                header = 0,\n",
    "                                delimiter = '\\t',\n",
    "                                low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows without identifiers\n",
    "rna_data = rna_gene_data.loc[rna_gene_data['transcript_stable_id'].apply(lambda x: x != 'None')]\n",
    "rna_data_labels = rna_gene_data.loc[rna_gene_data['GeneID_Cleaned'].apply(lambda x: x != 'None')]\n",
    "\n",
    "# de-dup data\n",
    "rna_metadata = rna_data[['transcript_stable_id', 'type_of_gene', 'Symbol', 'Synonyms', 'description', 'chromosome', 'map_location', 'Other_designations']].drop_duplicates(subset=['transcript_stable_id', 'type_of_gene', 'Symbol', 'Synonyms', 'description', 'chromosome', 'map_location', 'Other_designations'], keep='first', inplace=False) \n",
    "\n",
    "\n",
    "# aggregate mapping identifiers\n",
    "agg_cols = []\n",
    "\n",
    "for x in [ x for x in list(rna_data_labels) if x != 'transcript_stable_id']:\n",
    "    if x == 'GeneID_Cleaned':\n",
    "        agg_cols.append(rna_data_labels[['transcript_stable_id', x]].groupby('transcript_stable_id', as_index=False).agg(lambda x: '|'.join([x for x in list(set(x)) if x != 'None'])))\n",
    "    \n",
    "    else:\n",
    "        agg_cols.append(rna_data_labels[['transcript_stable_id', x]].groupby('transcript_stable_id', as_index=False).agg(lambda x: ', '.join([x for x in list(set(x)) if x != 'None'])))\n",
    "\n",
    "# merged aggreagted columns back together\n",
    "rna_merged = reduce(lambda  left, right: pandas.merge(left, right, on=['transcript_stable_id'], how='outer'), agg_cols)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "rna_merged.replace('','None', inplace=True)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "rna_merged.fillna('None', inplace=True)\n",
    "\n",
    "# remove rows without symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171919/171919 [00:48<00:00, 3535.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# create metadata\n",
    "rna, label, description, synonym = [], [], [], []\n",
    "\n",
    "for idx, row in tqdm(rna_merged.iterrows(), total=rna_merged.shape[0]):\n",
    "    \n",
    "    # node\n",
    "    if row['transcript_stable_id'] != 'None':\n",
    "        rna.append(row['transcript_stable_id'])\n",
    "    \n",
    "    # label -- only want metadata if there is a label\n",
    "    if row['Symbol'] != 'None':\n",
    "        label.append(row['Symbol'])\n",
    "    \n",
    "        # description\n",
    "        description.append('This transcript was transcribed from {desc}, a {gene} gene that is located on chromosome {chrom} (map_location: {map_loc}).'.format(desc=row['description'].title(),\n",
    "                                                                                                                                                                 gene=row['type_of_gene'],\n",
    "                                                                                                                                                                 chrom=row['chromosome'],\n",
    "                                                                                                                                                                 map_loc=row['map_location']))\n",
    "\n",
    "        # synonym\n",
    "        if row['Synonyms'] != 'None' and row['Other_designations'] != 'None':\n",
    "            synonym.append(row['Synonyms'] + '|' + row['Other_designations'])\n",
    "\n",
    "        elif row['Synonyms'] != 'None' and row['Other_designations'] == 'None':\n",
    "            synonym.append(row['Synonyms'])\n",
    "\n",
    "        elif row['Synonyms'] == 'None' and row['Other_designations'] != 'None':\n",
    "            synonym.append(row['Other_designations'])\n",
    "\n",
    "        else:\n",
    "            synonym.append('None')\n",
    "    \n",
    "# combine into new data frame\n",
    "rna_metadata_final = pandas.DataFrame(list(zip(rna, label, description, synonym)), columns =['ID', 'Label', 'Description', 'Synonym'])\n",
    "\n",
    "# make all variables string\n",
    "rna_metadata_final = rna_metadata_final.astype(str)\n",
    "\n",
    "# convert df to dictionary\n",
    "rna_metadata_final.set_index('ID', inplace=True)\n",
    "rna_metadata_dict = rna_metadata_final.to_dict('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variant Metadata Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_data = pandas.read_csv(unprocessed_data_location + 'variant_summary.txt',\n",
    "                           header = 0,\n",
    "                           delimiter = '\\t',\n",
    "                           low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows without identifiers\n",
    "var_data = var_data.loc[var_data['Assembly'].apply(lambda x: x == 'GRCh38')]\n",
    "var_data = var_data.loc[var_data['RS# (dbSNP)'].apply(lambda x: x != -1)]\n",
    "\n",
    "# de-dup data\n",
    "var_metadata = var_data[['#AlleleID', 'Type', 'Name', 'ClinicalSignificance', 'RS# (dbSNP)', 'Origin',\n",
    "                         'ChromosomeAccession', 'Chromosome', 'Start', 'Stop', 'ReferenceAllele',\n",
    "                         'Assembly', 'AlternateAllele','Cytogenetic', 'ReviewStatus', 'LastEvaluated']] \n",
    "\n",
    "# replace NaN with 'None'\n",
    "var_metadata.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicate dbSNP ids by choosing the most recent reviewed variant\n",
    "var_metadata.sort_values('LastEvaluated', ascending=False, inplace=True)\n",
    "var_metadata.drop_duplicates(subset='RS# (dbSNP)', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 429639/429639 [02:15<00:00, 3180.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# create metadata\n",
    "variant, label, description = [], [], []\n",
    "\n",
    "for idx, row in tqdm(var_metadata.iterrows(), total=var_metadata.shape[0]):\n",
    "    \n",
    "    # node\n",
    "    if row['RS# (dbSNP)'] != 'None':\n",
    "        variant.append('rs' + str(row['RS# (dbSNP)']))\n",
    "    \n",
    "    # label -- only want metadata if there is a label\n",
    "    if row['Name'] != 'None':\n",
    "        label.append(row['Name'])\n",
    "    \n",
    "        # description\n",
    "        sent = 'This variant is a {Origin} {Type} that results when a {ReferenceAllele} allele is changed to {AlternateAllele} on chromosome {Chromosome} ({ChromosomeAccession}, start:{Start}/stop:{Stop} positions, cytogenetic location:{Cytogenetic}) and has clinical significance {ClinicalSignificance}. This entry is for the {Assembly} and was last reviewed on {LastEvaluated} with review status \"{ReviewStatus}\".'\n",
    "        description.append(sent.format(Origin=row['Origin'], Type=row['Type'], ReferenceAllele=row['ReferenceAllele'],\n",
    "                                       AlternateAllele=row['AlternateAllele'], Chromosome=row['Chromosome'],\n",
    "                                       ChromosomeAccession=row['ChromosomeAccession'], Start=row['Start'],\n",
    "                                       Stop=row['Stop'], Cytogenetic=row['Cytogenetic'], ClinicalSignificance=row['ClinicalSignificance'],\n",
    "                                       Assembly=row['Assembly'], LastEvaluated=row['LastEvaluated'], ReviewStatus=row['ReviewStatus']))\n",
    "\n",
    "# combine into new data frame\n",
    "var_metadata_final = pandas.DataFrame(list(zip(variant, label, description)), columns =['ID', 'Label', 'Description'])\n",
    "\n",
    "# drop duplicates\n",
    "var_metadata_final.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# make all variables string\n",
    "var_metadata_final = var_metadata_final.astype(str)\n",
    "\n",
    "# convert df to dictionary\n",
    "var_metadata_final.set_index('ID', inplace=True)\n",
    "var_metadata_dict = var_metadata_final.to_dict('index')                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Master Metadata Dictionary**  \n",
    "To make it easier to navigate the mapping of each instance node in an edge, a master dictionary is created and keyed by node type. This is most useful when both nodes in an edge are instances, but of different data types (e.g. `gene-rna`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_metadata_dictionary = {'gene': gene_metadata_dict, 'rna': rna_metadata_dict, 'variant': var_metadata_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Metadata Files  <a class=\"anchor\" id=\"write-metadata-files\"></a>   \n",
    "using the `Master Metadata Dictionary` created in the prior step, all of the `edge-type` data is processed and the resulting data written out `.txt` file to the `./resource/node_data` repository.\n",
    "\n",
    "- [Genes](#gene-metadata)\n",
    "- [RNA](#rna-metadata)\n",
    "- [Pathways](#pathway-metadata)\n",
    "- [Complexes](#complex-metadata)\n",
    "- [Reactions](#reaction-metadata)\n",
    "- [Variants](#variant-metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genes <a class=\"anchor\" id=\"gene-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Pages:** [NCBI Gene](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#ncbi-gene) \n",
    "\n",
    "**Output:**  \n",
    "- chemical-gene ➞ [`chemical-gene_GENE_METADATA.txt`](https://www.dropbox.com/s/fvkqnuk5xhs0huh/chemical-gene_GENE_METADATA.txt?dl=1) \n",
    "- gene-disease ➞ [`gene-disease_GENE_METADATA.txt`](https://www.dropbox.com/s/o0y21rx3b829q6d/gene-disease_GENE_METADATA.txt?dl=1) \n",
    "- gene-gene ➞ [`gene-gene_GENE_METADATA.txt`](https://www.dropbox.com/s/i4gznnct7rzh7pn/gene-gene_GENE_METADATA.txt?dl=1) \n",
    "- gene-pathway ➞ [`gene-pathway_GENE_METADATA.txt`](https://www.dropbox.com/s/yncd95vanhkp0ey/gene-pathway_GENE_METADATA.txt?dl=1) \n",
    "- gene-phenotype ➞ [`gene-phenotype_GENE_METADATA.txt`](https://www.dropbox.com/s/jghcoc5xzada011/gene-phenotype_GENE_METADATA.txt?dl=1) \n",
    "- gene-protein ➞ [`gene-protein_GENE_METADATA.txt`](https://www.dropbox.com/s/6vu961lna08qn08/gene-protein_GENE_METADATA.txt?dl=1) \n",
    "- gene-rna ➞ [`gene-rna_GENE_METADATA.txt`](https://www.dropbox.com/s/vs0kirmugdo9zkd/gene-rna_GENE_METADATA.txt?dl=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'gene'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_directory + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNA<a class=\"anchor\" id=\"rna-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Pages:**  \n",
    "- [Ensembl](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#clinvar)  \n",
    "- [NCBI Gene](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#ncbi-gene) \n",
    "\n",
    "**Output:**  \n",
    "- chemical-rna ➞ [`chemical-rna_RNA_METADATA.txt`](https://www.dropbox.com/s/sm0orl0waq5iqhd/chemical-rna_RNA_METADATA.txt?dl=1) \n",
    "- rna-anatomy ➞ [`rna-anatomy_RNA_METADATA.txt`](https://www.dropbox.com/s/plkrunhhusx6mez/rna-anatomy_RNA_METADATA.txt?dl=1) \n",
    "- rna-cell ➞ [`rna-cell_RNA_METADATA.txt`](https://www.dropbox.com/s/dld0eadxyyzr44y/rna-cell_RNA_METADATA.txt?dl=1) \n",
    "- rna-protein ➞ [`rna-protein_RNA_METADATA.txt`](https://www.dropbox.com/s/3g72sb2e685rptn/rna-protein_RNA_METADATA.txt?dl=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'rna'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_directory + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pathways<a class=\"anchor\" id=\"pathway-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#reactome-pathway-database)  \n",
    "\n",
    "**Output:**    \n",
    "- chemical-pathway ➞ [`chemical-pathway_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/2txg2ui4e6y7rnm/chemical-pathway_PATHWAY_METADATA.txt?dl=1)\n",
    "- gobp-pathway ➞ [`gobp-pathway_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/bq0g1g4ef40vwxj/gobp-pathway_PATHWAY_METADATA.txt?dl=1)\n",
    "- pathway-gocc ➞ [`pathway-gocc_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/6fzkzjxj08u6jfi/pathway-gocc_PATHWAY_METADATA.txt?dl=1)\n",
    "- pathway-gomf ➞ [`pathway-gomf_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/gfqt86vujnoo7j5/pathway-gomf_PATHWAY_METADATA.txt?dl=1)\n",
    "- protein-pathway ➞ [`protein-pathway_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/xadtz4c0ab4a7p9/protein-pathway_PATHWAY_METADATA.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'pathway'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_directory + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexes<a class=\"anchor\" id=\"complex-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#reactome-pathway-database)    \n",
    "\n",
    "**Output:**    \n",
    "- chemical-complex ➞ [`chemical-complex_COMPLEX_METADATA.txt`](https://www.dropbox.com/s/mu53u8fv5v0epvf/chemical-complex_COMPLEX_METADATA.txt?dl=1) \n",
    "- complex-complex ➞ [`complex-complex_COMPLEX_METADATA.txt`](https://www.dropbox.com/s/y4qt0ne47ix1tqb/complex-complex_COMPLEX_METADATA.txt?dl=1) \n",
    "- complex-pathway ➞ [`complex-pathway_COMPLEX_METADATA.txt`](https://www.dropbox.com/s/6n9w0vvxabi7efl/complex-pathway_COMPLEX_METADATA.txt?dl=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'complex'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_directory + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reactions<a class=\"anchor\" id=\"reaction-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#reactome-pathway-database)   \n",
    "\n",
    "**Output:**    \n",
    "- chemical-reaction ➞ [`chemical-reaction_REACTION_METADATA.txt`](https://www.dropbox.com/s/6iztwaxrhrp1f7h/chemical-reaction_REACTION_METADATA.txt?dl=1)\n",
    "- protein-reaction ➞ [`protein-reaction_REACTION_METADATA.txt`](https://www.dropbox.com/s/92vsuon54w4uq4j/protein-reaction_REACTION_METADATA.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'reaction'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_directory + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants<a class=\"anchor\" id=\"variant-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [ClinVar](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#clinvar)  \n",
    "\n",
    "**Output:**  \n",
    "- variant-disease ➞ [`variant-disease_VARIANT_METADATA.txt`](https://www.dropbox.com/s/vj440u5efwdwibl/variant-disease_VARIANT_METADATA.txt?dl=1)  \n",
    "- variant-gene ➞ [`variant-gene_VARIANT_METADATA.txt`](https://www.dropbox.com/s/geui7nby9h055bc/variant-gene_VARIANT_METADATA.txt?dl=1)  \n",
    "- variant-phenotype ➞ [`variant-phenotype_VARIANT_METADATA.txt`](https://www.dropbox.com/s/hnocd802detivdd/variant-phenotype_VARIANT_METADATA.txt?dl=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'variant'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_directory + edge_type + '_' + node_type.upper() + '_METADATA.txt', header = True, sep = '\\t', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "```\n",
    "@misc{callahan_tj_2019_3401437,\n",
    "  author       = {Callahan, TJ},\n",
    "  title        = {PheKnowLator},\n",
    "  month        = mar,\n",
    "  year         = 2019,\n",
    "  doi          = {10.5281/zenodo.3401437},\n",
    "  url          = {https://doi.org/10.5281/zenodo.3401437}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
