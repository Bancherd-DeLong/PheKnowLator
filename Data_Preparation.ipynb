{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# PheKnowLator - Data Preparation\n",
    "***\n",
    "***\n",
    "\n",
    "**Author:** [TJCallahan](https://mail.google.com/mail/u/0/?view=cm&fs=1&tf=1&to=callahantiff@gmail.com)  \n",
    "**GitHub Repository:** [PheKnowLator](https://github.com/callahantiff/PheKnowLator/wiki)  \n",
    "**Release:** **[v2.0.0](https://github.com/callahantiff/PheKnowLator/wiki/v2.0.0)**\n",
    "  \n",
    "<br>  \n",
    "  \n",
    "**Purpose:** This notebook serves as a script to download and process data in order to generate mapping and filtering data needed to build edges for the PheKnowLator knowledge graph. For more information on the data sources utilize within this script, please see the [Data Sources](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources) Wiki page.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Assumptions:**   \n",
    "- Raw data downloads ➞ `./resources/processed_data/unprocessed_data`    \n",
    "- Processed data write location ➞ `./resources/processed_data`  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Dependencies:** This notebook utilizes several helper functions, which are stored in the [`data_preparation_helper_functions.py`](https://github.com/callahantiff/PheKnowLator/blob/master/scripts/python/data_preparation_helper_functions.py) script. Hyperlinks to all downloaded and generated data sources are provided on the [Data Sources](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources) Wiki page as well as within each source subsection of this notebook. All generated data is freely available for download from DropBox. \n",
    "\n",
    "_____\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "***\n",
    "\n",
    "### [Create Identifier Maps ](#create-identifier-maps)  \n",
    "- [HUMAN TRANSCRIPT, GENE, AND PROTEIN IDENTIFIER MAPPING](#human-transcript,-gene,-and-protein-identifier-mapping)\n",
    "  - [Entrez Gene-Ensembl Transcript](#entrezgene-ensembltranscript)  \n",
    "  - [Entrez Gene-Protein Ontology](#entrezgene-proteinontology)  \n",
    "  - [Ensembl Gene-Entrez Gene](#ensemblgene-entrezgene)\n",
    "  - [Gene Symbol-Ensembl Transcript](#genesymbol-ensembltranscript)  \n",
    "  - [STRING-Protein Ontology](#string-proteinontology)  \n",
    "  - [Uniprot Accession-Protein Ontology](#uniprotaccession-proteinontology)\n",
    "  \n",
    "\n",
    "- [OTHER IDENTIFIER MAPPING](#other-identifier-mapping) \n",
    "  - [ChEBI Identifiers](#mesh-chebi) \n",
    "  - [Human Disease and Phenotype Identifiers](#disease-identifiers)\n",
    "  - [Human Protein Atlas Tissue and Cell Types](#hpa-uberon)  \n",
    "  - [Reactome Pathways - Pathway Ontology](#reactome-pw)  \n",
    "  - [Genomic Identifiers - Sequence Ontology](#genomic-so)\n",
    "\n",
    "<br>\n",
    "\n",
    "### [Create Edge Datasets](#create-edge-datasets)\n",
    "- [ONTOLOGIES](#ontologies)  \n",
    "  - [Protein Ontology](#protein-ontology)  \n",
    "  - [Relations Ontology](#relations-ontology)  \n",
    "\n",
    "\n",
    "- [LINKED DATA](#linked-data)  \n",
    "  - [Clinvar Variant-Diseases and Phenotypes](#clinvar-variant)\n",
    "  - [Uniprot Protein-Cofactor and Protein-Catalyst](#uniprot-protein-cofactorcatalyst)  \n",
    "\n",
    "<br>\n",
    "\n",
    "### [Create Instance Data Metadata](#create-instance-metadata)  \n",
    "- [Genes/RNA](#gene-and-rna-metadata)\n",
    "- [Pathways](#pathway-metadata)\n",
    "- [Variants](#variant-metadata) \n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-Up Environment\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import datetime\n",
    "import glob\n",
    "import ijson\n",
    "import itertools\n",
    "import networkx\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from owlready2 import subprocess\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, Literal\n",
    "from reactome2py import content\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import script containing helper functions\n",
    "from pkt.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to read unprocessed data files from\n",
    "unprocessed_data_location = 'resources/processed_data/unprocessed_data/'\n",
    "\n",
    "# directory to write processed data files to\n",
    "processed_data_location = 'resources/processed_data/'\n",
    "\n",
    "# directory to write relations data to\n",
    "relations_data_location = 'resources/relations_data/'\n",
    "\n",
    "# directory to write node metadata to\n",
    "node_data_location = 'resources/node_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### CREATE MAPPING DATASETS  <a class=\"anchor\" id=\"create-identifier-maps\"></a>\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Transcript, Gene, and Protein Identifier Mapping  <a class=\"anchor\" id=\"human-transcript,-gene,-and-protein-identifier-mapping\"></a>\n",
    "***\n",
    "\n",
    "**Data Source Wiki Pages:**   \n",
    "- [Ensembl](https://uswest.ensembl.org/)  \n",
    "- [Uniprot Knowledgebase](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#uniprot-knowledgebase)  \n",
    "- [HGNC](ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt) \n",
    "- [NCBI Gene](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#ncbi-gene) \n",
    "- [Protein Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#protein-ontology)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Purpose:** To map create protein-coding gene-protein relations and mappings between the identifiers listed below. The edges types produced from each of these mappings will be further described within each identifier mapping section:  \n",
    "- [Entrez Gene-Ensembl Transcript](#entrezgene-ensembltranscript)  \n",
    "- [Entrez Gene-Protein Ontology](#entrezgene-proteinontology)  \n",
    "- [Ensembl Gene-Entrez Gene](#ensemblgene-entrezgene)\n",
    "- [Gene Symbol-Ensembl Transcript](#genesymbol-ensembltranscript)  \n",
    "- [STRING-Protein Ontology](#string-proteinontology)  \n",
    "- [Uniprot Accession-Protein Ontology](#uniprotaccession-proteinontology)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Gene and Trainscript Types:** The transcript and gene/locus types were reviewed by a PhD Molecular biologist to confirm whether or not they should be treated as `protein-coding` or not, which is useful for creating `genomic-rna`, `genomic-protein`, and `rna-protein` edges in the knowledge graph. For more information on this classification, please see the table below. Definitions of concepts in the table have been taken from [HGNC](https://www.genenames.org/help/symbol-report/), [Ensembl](https://uswest.ensembl.org/info/genome/genebuild/biotypes.html), [NCBI](https://www.ncbi.nlm.nih.gov/IEB/ToolBox/CPP_DOC/lxr/source/src/objects/entrezgene/entrezgene.asn), and Wikipedia.\n",
    "\n",
    "<table>\n",
    "<th align=\"center\">Gene and Transcript Type</th>  \n",
    "<th align=\"center\">Definition</th>\n",
    "<th align=\"center\">Type</th>\n",
    "<th align=\"center\">Genomic material <i>transcribed_to</i> RNA</th>\n",
    "<th align=\"center\">RNA <i>translated_to</i> Protein</th>\n",
    "<th align=\"center\">Genomic material <i>has_gene_product</i> Protein</th>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">biological-region</td> \n",
    "  <td rowspan=\"2\">Biological_region (SO:0001411; Special note: This is a parental feature spanning all other feature annotation on each RefSeq Functional Element record. It is a 'misc_feature' in GenBank flat files but a 'Region' feature in ASN.1 and GFF3 formats</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>transcript</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_C_gene</td> \n",
    "  <td rowspan=\"2\">Constant chain immunoglobulin gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_C_pseudogene</td> \n",
    "  <td rowspan=\"2\">Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\t \t \n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_D_gene</td> \n",
    "  <td rowspan=\"2\">Diversity chain immunoglobulin gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_J_gene</td> \n",
    "  <td rowspan=\"2\">IG J gene: Joining chain immunoglobulin gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_J_pseudogene</td> \n",
    "  <td rowspan=\"2\">Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_pseudogene</td> \n",
    "  <td rowspan=\"2\">Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_V_gene</td> \n",
    "  <td rowspan=\"2\">Variable chain immunoglobulin gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_V_pseudogene</td> \n",
    "  <td rowspan=\"2\">Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">lncRNA</td> \n",
    "  <td rowspan=\"2\">RNA, long non-coding - non-protein coding genes that encode long non-coding RNAs (lncRNAs) (SO:0001877); these are at least 200 nt in length. Subtypes include intergenic (SO:0001463), intronic (SO:0001903) and antisense (SO:0001904)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">miRNA</td> \n",
    "  <td rowspan=\"2\">RNA, micro - non-protein coding genes that encode microRNAs (miRNAs) (SO:0001265)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">misc_RNA</td> \n",
    "  <td rowspan=\"2\">Non-protein coding genes that encode miscellaneous types of small ncRNAs, such as vault (SO:0000404) and Y (SO:0000405) RNA genes</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">Mt_rRNA</td> \n",
    "  <td rowspan=\"2\">Mitochondrial rRNA</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">Mt_tRNA</td> \n",
    "  <td rowspan=\"2\">Mitochondrial tRNA</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">ncRNA</td> \n",
    "  <td rowspan=\"2\">Noncoding RNA</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">non_stop_decay</td> \n",
    "  <td rowspan=\"2\">Transcripts that have polyA features (including signal) without a prior stop codon in the CDS, i.e. a non-genomic polyA tail attached directly to the CDS without 3' UTR. These transcripts are subject to degradation</td>\n",
    "  <td>gene</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">nonsense_mediated_decay</td> \n",
    "  <td rowspan=\"2\">If the coding sequence (following the appropriate reference) of a transcript finishes >50bp from a downstream splice site then it is tagged as NMD. If the variant does not cover the full reference coding sequence then it is annotated as NMD if NMD is unavoidable i.e. no matter what the exon structure of the missing portion is the transcript will be subject to NMD</td>\n",
    "  <td>gene</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">other</td> \n",
    "  <td rowspan=\"2\">other</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">phenotype</td> \n",
    "  <td rowspan=\"2\"> Mapped phenotypes where the causative gene has not been identified (SO:0001500) </td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">polymorphic_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene owing to a SNP/DIP but in other individuals/haplotypes/strains the gene is translated</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">processed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene that lack introns and is thought to arise from reverse transcription of mRNA followed by reinsertion of DNA into the genome</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">processed_transcript</td> \n",
    "  <td rowspan=\"2\">Gene/transcript that doesn't contain an open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">protein_coding</td> \n",
    "  <td rowspan=\"2\">Contains an open reading frame (ORF)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>yes</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">pseudogene</td> \n",
    "  <td rowspan=\"2\">Have homology to proteins but generally suffer from a disrupted coding sequence and an active homologous gene can be found at another locus. Sometimes these entries have an intact coding sequence or an open but truncated ORF, in which case there is other evidence used (for example genomic polyA stretches at the 3' end) to classify them as a pseudogene</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">retained_intron</td> \n",
    "  <td rowspan=\"2\">Has an alternatively spliced transcript believed to contain intronic sequence relative to other, coding, variants</td>\n",
    "  <td>gene</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">ribozyme</td> \n",
    "  <td rowspan=\"2\">Ribozymes are RNA molecules that have the ability to catalyze specific biochemical reactions, including RNA splicing in gene expression, similar to the action of protein enzymes</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">rRNA</td> \n",
    "  <td rowspan=\"2\">RNA, ribosomal - non-protein coding genes that encode ribosomal RNAs (rRNAs) (SO:0001637)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">rRNA_pseudogene</td> \n",
    "  <td rowspan=\"2\">A gene that has homology to known protein-coding genes but contain a frameshift and/or stop codon(s) which disrupts the ORF. Thought to have arisen through duplication followed by loss of function</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">scaRNA</td> \n",
    "  <td rowspan=\"2\">Small Cajal body-specific RNAs are a class of small nucleolar RNAs that specifically localise to the Cajal body, a nuclear organelle involved in the biogenesis of small nuclear ribonucleoproteins/td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">scRNA</td> \n",
    "  <td rowspan=\"2\">RNA, small cytoplasmic - non-protein coding genes that encode small cytoplasmic RNAs (scRNAs) (SO:0001266)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">snoRNA</td> \n",
    "  <td rowspan=\"2\">RNA, small nucleolar - non-protein coding genes that encode small nucleolar RNAs (snoRNAs) containing C/D or H/ACA box domains (SO:0001267)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">snRNA</td> \n",
    "  <td rowspan=\"2\">RNA, small nuclear - non-protein coding genes that encode small nuclear RNAs (snRNAs) (SO:0001268)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">sRNA</td> \n",
    "  <td rowspan=\"2\">Bacterial small RNAs (sRNA) are small RNAs produced by bacteria; they are 50- to 500-nucleotide non-coding RNA molecules, highly structured and containing several stem-loops</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TEC</td> \n",
    "  <td rowspan=\"2\">TEC (To be Experimentally Confirmed). This is used for non-spliced EST clusters that have polyA features. This category has been specifically created for the ENCODE project to highlight regions that could indicate the presence of protein coding genes that require experimental validation, either by 5' RACE or RT-PCR to extend the transcripts, or by confirming expression of the putatively-encoded peptide with specific antibodies</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>yes</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_C_gene</td> \n",
    "  <td rowspan=\"2\">Constant chain T cell receptor gene that undergoes somatic recombination before transcription/td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_D_gene</td> \n",
    "  <td rowspan=\"2\">Diversity chain T cell receptor gene that undergoes somatic recombination before transcription/td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_J_gene</td> \n",
    "  <td rowspan=\"2\">Joining chain T cell receptor gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_J_pseudogene</td> \n",
    "  <td rowspan=\"2\">T cell receptor pseudogene - T cell receptor gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_V_gene</td> \n",
    "  <td rowspan=\"2\">Variable chain T cell receptor gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_V_pseudogene</td> \n",
    "  <td rowspan=\"2\">T cell receptor pseudogene - T cell receptor gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">transcribed_processed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary'</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">transcribed_unitary_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary'</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">transcribed_unprocessed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary'</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">translated_processed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogenes that have mass spec data suggesting that they are also translated. These can be classified into 'Processed', 'Unprocessed'</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">translated_unprocessed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">tRNA</td> \n",
    "  <td rowspan=\"2\">Transfer RNA</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">unitary_pseudogene</td> \n",
    "  <td rowspan=\"2\">A species specific unprocessed pseudogene without a parent gene, as it has an active orthologue in another species</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">unknown</td> \n",
    "  <td rowspan=\"2\">Entries where the locus type is currently unknown</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">unprocessed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene that can contain introns since produced by gene duplication</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\" align=\"center\">VvaultRNA</td> \n",
    "  <td rowspan=\"2\" align=\"center\">Short non coding RNA genes that form part of the vault ribonucleoprotein complex</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "</table> \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Output:** This script downloads and saves the following data:  \n",
    "- Human Ensembl Gene Set ➞ [`Homo_sapiens.GRCh38.99.gtf`](ftp://ftp.ensembl.org/pub/release-99/gtf/homo_sapiens/Homo_sapiens.GRCh38.99.gtf.gz)\n",
    "- Human Ensembl-UniProt Identifiers ➞ [`Homo_sapiens.GRCh38.98.uniprot.tsv`](https://www.dropbox.com/s/cesjvqz1b8c7ami/Homo_sapiens.GRCh38.98.uniprot.tsv?dl=1) \n",
    "- Human Ensembl-Entrez Identifiers ➞ [`Homo_sapiens.GRCh38.98.entrez.tsv`](https://www.dropbox.com/s/5kstw70py0azvws/Homo_sapiens.GRCh38.98.entrez.tsv?dl=1) \n",
    "- Human Gene Identifiers ➞ [`Homo_sapiens.gene_info`](https://www.dropbox.com/s/vazlmzxydgv6xzz/Homo_sapiens.gene_info?dl=1), [`hgnc_complete_set.txt`](ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt)  \n",
    "- Human Protein Identifiers ➞ [`promapping.txt`](https://www.dropbox.com/s/x7wdimv6ph6bl8k/promapping.txt?dl=1)  \n",
    "- UniProt Identifiers ➞ [`uniprot_identifier_mapping.tab`](https://www.dropbox.com/s/3wbd3wq35328a2v/uniprot_identifier_mapping.tab?dl=1)\n",
    "\n",
    "_All Merged Data Sets:_ [`Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt`](https://www.dropbox.com/s/fiek6h5rowi7dh0/Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt?dl=1)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**HGNC Data** \n",
    "\n",
    "_Human Gene Set Data_ - `hgnc_complete_set.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnc = pandas.read_csv(unprocessed_data_location + 'hgnc_complete_set.txt',\n",
    "                       header=0,\n",
    "                       delimiter='\\t',\n",
    "                       low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, updating data types (i.e. making all columns type `str`), and unnesting `|` delimited data. The final step is to update the gene_type variable such that each of the variable values is re-grouped to be protein-coding, other or ncRNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hgnc_id</th>\n",
       "      <th>entrez_id</th>\n",
       "      <th>ensembl_gene_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>symbol</th>\n",
       "      <th>hgnc_gene_type</th>\n",
       "      <th>name</th>\n",
       "      <th>map_location</th>\n",
       "      <th>primary_symbol</th>\n",
       "      <th>synonyms</th>\n",
       "      <th>master_gene_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>ENSG00000121410</td>\n",
       "      <td>P04217</td>\n",
       "      <td>None</td>\n",
       "      <td>protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>19q13.43</td>\n",
       "      <td>A1BG</td>\n",
       "      <td>None</td>\n",
       "      <td>protein-coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37133</td>\n",
       "      <td>503538</td>\n",
       "      <td>ENSG00000268895</td>\n",
       "      <td>None</td>\n",
       "      <td>A1BG-AS1</td>\n",
       "      <td>lncRNA</td>\n",
       "      <td>None</td>\n",
       "      <td>19q13.43</td>\n",
       "      <td>A1BG-AS1</td>\n",
       "      <td>None</td>\n",
       "      <td>ncRNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37133</td>\n",
       "      <td>503538</td>\n",
       "      <td>ENSG00000268895</td>\n",
       "      <td>None</td>\n",
       "      <td>FLJ23569</td>\n",
       "      <td>lncRNA</td>\n",
       "      <td>None</td>\n",
       "      <td>19q13.43</td>\n",
       "      <td>A1BG-AS1</td>\n",
       "      <td>None</td>\n",
       "      <td>ncRNA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hgnc_id entrez_id  ensembl_gene_id uniprot_id    symbol  hgnc_gene_type  \\\n",
       "0       5         1  ENSG00000121410     P04217      None  protein-coding   \n",
       "1   37133    503538  ENSG00000268895       None  A1BG-AS1          lncRNA   \n",
       "2   37133    503538  ENSG00000268895       None  FLJ23569          lncRNA   \n",
       "\n",
       "   name map_location primary_symbol synonyms master_gene_type  \n",
       "0  None     19q13.43           A1BG     None   protein-coding  \n",
       "1  None     19q13.43       A1BG-AS1     None            ncRNA  \n",
       "2  None     19q13.43       A1BG-AS1     None            ncRNA  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all rows thave don't have 'approved' status\n",
    "hgnc = hgnc.loc[hgnc['status'].apply(lambda x: x == 'Approved')]\n",
    "\n",
    "# drop uneeded columns\n",
    "hgnc = hgnc[['hgnc_id', 'entrez_id', 'ensembl_gene_id', 'uniprot_ids', 'symbol', 'locus_type',\n",
    "             'alias_symbol', 'name', 'location', 'alias_name']]\n",
    "\n",
    "# rename columns\n",
    "hgnc.rename(columns={'uniprot_ids': 'uniprot_id', 'location': 'map_location', 'locus_type': 'hgnc_gene_type'}, inplace=True)\n",
    "\n",
    "# strip 'HGNC' off of the identifiers\n",
    "hgnc['hgnc_id'].replace('.*\\:','', inplace=True, regex=True)\n",
    "\n",
    "# combine certain columns into single column\n",
    "hgnc['primary_symbol'] = hgnc['symbol']\n",
    "hgnc['symbol'] = hgnc['symbol'] + '|' + hgnc['alias_symbol']\n",
    "hgnc['name'] = hgnc['name'] + '|' + hgnc['alias_name']\n",
    "hgnc['synonyms'] = hgnc['alias_symbol'] + '|' + hgnc['alias_name']\n",
    "\n",
    "# replace NaN with 'None'\n",
    "hgnc.fillna('None', inplace=True)\n",
    "\n",
    "# make data columns of type string\n",
    "hgnc['entrez_id'] = hgnc['entrez_id'].apply(lambda x: str(int(x)) if x != 'None' else 'None')\n",
    "\n",
    "# explode nested data\n",
    "explode_df_hgnc = explodes_data(hgnc.copy(), ['ensembl_gene_id', 'uniprot_id', 'symbol', 'name'], '|')\n",
    "\n",
    "# reformat hgnc gene type\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('region', 'biological region', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('complex locus constituent', 'complex locus constituent', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('endogenous retrovirus', 'endogenous retrovirus', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('fragile site', 'fragile site', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('immunoglobulin gene', 'IG_gene', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('immunoglobulin pseudogene', 'IG_pseudogene', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('RNA, long non-coding', 'lncRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('RNA, micro', 'miRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('RNA, misc', 'miscRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('phenotype only', 'phenotype only', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('gene with protein product', 'protein-coding', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('protocadherin', 'protocadherin', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('readthrough', 'readthrough', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('RNA, ribosomal', 'rRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('RNA, small nucleolar', 'snoRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('RNA, small nuclear', 'snRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('RNA, cluster', 'sRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('T cell receptor gene', 'TR_gene', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('T cell receptor pseudogene', 'TR_pseudogene', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('transposable element', 'transposable element', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('RNA, transfer', 'tRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('unknown', 'unknown', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('RNA, vault', 'vaultRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('virus integration site', 'virus integration site', inplace=True, regex=False)\n",
    "explode_df_hgnc['hgnc_gene_type'].replace('RNA, Y', 'Y RNA', inplace=True, regex=False)\n",
    "\n",
    "# master gene type\n",
    "explode_df_hgnc['master_gene_type'] = explode_df_hgnc['hgnc_gene_type']\n",
    "explode_df_hgnc['master_gene_type'].replace('biological region', 'other', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('complex locus constituent', 'other', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('endogenous retrovirus', 'other', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('fragile site', 'other', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('IG_gene', 'other', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('IG_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('lncRNA', 'ncRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('miRNA', 'ncRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('miscRNA', 'ncRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('phenotype only', 'other', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('protocadherin', 'other', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('readthrough', 'other', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('snoRNA', 'ncRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('snRNA', 'ncRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('sRNA', 'ncRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('TR_gene', 'other', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('TR_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('transposable element', 'other', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('tRNA', 'tRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('vaultRNA', 'ncRNA', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('virus integration site', 'other', inplace=True, regex=False)\n",
    "explode_df_hgnc['master_gene_type'].replace('Y RNA', 'ncRNA', inplace=True, regex=False)\n",
    "\n",
    "# remove original gene type column\n",
    "explode_df_hgnc.drop(['alias_symbol', 'alias_name'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "explode_df_hgnc.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_hgnc.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Ensembl Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Human Gene Set Data_ - `Homo_sapiens.GRCh38.99.gtf.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ensembl.org/pub/release-99/gtf/homo_sapiens/Homo_sapiens.GRCh38.99.gtf.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_geneset = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.99.gtf',\n",
    "                                  header = None,\n",
    "                                  delimiter='\\t',\n",
    "                                  skiprows=5,\n",
    "                                  low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be reformatted in order for it to be able to be merged with the other gene, RNA, and protein identifier data. To do this, we iterate over each row of the data and extract the fields shown below in `column_names`, making each of these extracted fields their own column. The final step is to update the gene_type variable such that each of the variable values is re-grouped to be `protein-coding`, `other` or `ncRNA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2905054/2905054 [02:47<00:00, 17349.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensembl_gene_id</th>\n",
       "      <th>transcript_stable_id</th>\n",
       "      <th>symbol</th>\n",
       "      <th>primary_symbol</th>\n",
       "      <th>ensembl_gene_type</th>\n",
       "      <th>transcript_name</th>\n",
       "      <th>ensembl_transcript_type</th>\n",
       "      <th>master_gene_type</th>\n",
       "      <th>master_transcript_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>None</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000456328</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-202</td>\n",
       "      <td>processed_transcript</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>protein-coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000450305</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-201</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ensembl_gene_id transcript_stable_id   symbol primary_symbol  \\\n",
       "0  ENSG00000223972                 None  DDX11L1        DDX11L1   \n",
       "1  ENSG00000223972      ENST00000456328  DDX11L1        DDX11L1   \n",
       "5  ENSG00000223972      ENST00000450305  DDX11L1        DDX11L1   \n",
       "\n",
       "                    ensembl_gene_type transcript_name  \\\n",
       "0  transcribed_unprocessed_pseudogene            None   \n",
       "1  transcribed_unprocessed_pseudogene     DDX11L1-202   \n",
       "5  transcribed_unprocessed_pseudogene     DDX11L1-201   \n",
       "\n",
       "              ensembl_transcript_type master_gene_type master_transcript_type  \n",
       "0                                None       pseudogene     not protein-coding  \n",
       "1                processed_transcript       pseudogene         protein-coding  \n",
       "5  transcribed_unprocessed_pseudogene       pseudogene     not protein-coding  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set list of data items to extract from column string\n",
    "data_cols = ['gene_id', 'transcript_id', 'gene_name', 'gene_biotype', 'transcript_name', 'transcript_biotype']\n",
    "\n",
    "# loop over items contained in the string of column 8\n",
    "cleaned_column = []\n",
    "\n",
    "for data_list in tqdm(list(ensembl_geneset[8])):\n",
    "    data_list = data_list if not data_list.endswith(';') else data_list[:-1]\n",
    "    temp_data = [data_list.split('; ')[[x.split(' ')[0] for x in data_list.split('; ')].index(col)] if col in data_list else col + ' None' for col in data_cols]\n",
    "    cleaned_column.append(temp_data) \n",
    "\n",
    "# update columns\n",
    "ensembl_geneset['ensembl_gene_id'] = [x[0].split(' ')[-1].replace('\"', '') for x in cleaned_column]\n",
    "ensembl_geneset['transcript_stable_id'] = [x[1].split(' ')[-1].replace('\"', '') for x in cleaned_column]\n",
    "ensembl_geneset['symbol'] = [x[2].split(' ')[-1].replace('\"', '') for x in cleaned_column]\n",
    "ensembl_geneset['primary_symbol'] = [x[2].split(' ')[-1].replace('\"', '') for x in cleaned_column]\n",
    "ensembl_geneset['ensembl_gene_type'] = [x[3].split(' ')[-1].replace('\"', '') for x in cleaned_column]\n",
    "ensembl_geneset['transcript_name'] = [x[4].split(' ')[-1].replace('\"', '') for x in cleaned_column]\n",
    "ensembl_geneset['ensembl_transcript_type'] = [x[5].split(' ')[-1].replace('\"', '') for x in cleaned_column]\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_geneset.fillna('None', inplace=True)\n",
    "\n",
    "# reformat ensembl gene type\n",
    "ensembl_geneset['ensembl_gene_type'].replace('misc_RNA', 'miscRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['ensembl_gene_type'].replace('TEC', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['ensembl_gene_type'].replace('protein_coding', 'protein-coding', inplace=True, regex=False)\n",
    "\n",
    "# reformat master gene type\n",
    "ensembl_geneset['master_gene_type'] = ensembl_geneset['ensembl_gene_type']\n",
    "ensembl_geneset['master_gene_type'].replace('IG_C_gene', 'other', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('IG_C_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('IG_D_gene', 'other', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('IG_J_gene', 'other', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('IG_J_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('IG_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('IG_V_gene', 'other', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('IG_V_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('lncRNA', 'ncRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('miRNA', 'ncRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('miscRNA', 'ncRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('Mt_rRNA', 'rRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('Mt_tRNA', 'tRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('polymorphic_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('processed_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('ribozyme', 'other', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('rRNA_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('scaRNA', 'ncRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('scRNA', 'ncRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('snoRNA', 'ncRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('snRNA', 'ncRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('sRNA', 'ncRNA', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('TR_C_gene', 'other', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('TR_D_gene', 'other', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('TR_J_gene', 'other', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('TR_J_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('TR_V_gene', 'other', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('TR_V_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('transcribed_processed_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('transcribed_unitary_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('transcribed_unprocessed_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('translated_processed_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('translated_unprocessed_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('unitary_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('unprocessed_pseudogene', 'pseudogene', inplace=True, regex=False)\n",
    "ensembl_geneset['master_gene_type'].replace('vaultRNA', 'ncRNA', inplace=True, regex=False)\n",
    "\n",
    "# reformat master transcript type\n",
    "ensembl_geneset['master_transcript_type'] = ensembl_geneset['ensembl_transcript_type']\n",
    "ensembl_geneset['master_transcript_type'].replace('IG_C_gene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('IG_C_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('IG_D_gene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('IG_J_gene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('IG_J_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('IG_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('IG_V_gene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('IG_V_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('lncRNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('miRNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('misc_RNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('Mt_rRNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('Mt_tRNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('non_stop_decay', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('None', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('nonsense_mediated_decay', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('polymorphic_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('processed_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('processed_transcript', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('protein_coding', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('retained_intron', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('ribozyme', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('rRNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('rRNA_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('scaRNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('scRNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('snoRNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('snRNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('sRNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('TEC', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('TR_C_gene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('TR_D_gene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('TR_J_gene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('TR_J_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('TR_V_gene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('TR_V_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('transcribed_processed_pseudogene', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('transcribed_unitary_pseudogene', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('transcribed_unprocessed_pseudogene', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('translated_processed_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('translated_unprocessed_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('unitary_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('unprocessed_pseudogene', 'protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('vaultRNA', 'not protein-coding', inplace=True, regex=False)\n",
    "ensembl_geneset['master_transcript_type'].replace('unknown', 'not protein-coding', inplace=True, regex=False)\n",
    "\n",
    "# remove uneeded columns\n",
    "ensembl_geneset.drop(list(range(9)), axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_geneset.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_geneset.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensembl Annotation Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ensembl-UniProt_ - `Homo_sapiens.GRCh38.98.uniprot.tsv`  \n",
    "Once the main ensembl gene set has been read in, the next step is to read in the `ensembl-uniprot` mapping file. These files are vital for successfully merging the ensembl identifiers with the uniprot data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_uniprot = 'ftp://ftp.ensembl.org/pub/release-99/tsv/homo_sapiens/Homo_sapiens.GRCh38.99.uniprot.tsv.gz'\n",
    "data_downloader(url_uniprot, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_uniprot = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.99.uniprot.tsv',\n",
    "                                  header=0,\n",
    "                                  delimiter='\\t',\n",
    "                                  low_memory=False)\n",
    "\n",
    "# rename columns\n",
    "ensembl_uniprot.rename(columns={'xref': 'uniprot_id', 'gene_stable_id': 'ensembl_gene_id'}, inplace=True)\n",
    "\n",
    "# replace \"-\"\n",
    "ensembl_uniprot.replace('-', 'None', inplace=True)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_uniprot.fillna('None', inplace=True)\n",
    "\n",
    "# remove uneeded columns\n",
    "ensembl_uniprot.drop(['db_name', 'info_type', 'source_identity', 'xref_identity', 'linkage_type'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_uniprot.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ensembl-Entrez_ - `Homo_sapiens.GRCh38.98.entrez.tsv`  \n",
    "Once the main ensembl gene set has been read in, the next step is to read in the `ensembl-entrez` mapping file. These files are vital for successfully merging the ensembl identifiers with the entrez data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_entrez = 'ftp://ftp.ensembl.org/pub/release-99/tsv/homo_sapiens/Homo_sapiens.GRCh38.99.entrez.tsv.gz'\n",
    "data_downloader(url_entrex, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ensembl-entrez data\n",
    "ensembl_entrez = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.99.entrez.tsv',\n",
    "                                 header=0,\n",
    "                                 delimiter='\\t',\n",
    "                                 low_memory=False)\n",
    "\n",
    "# rename columns\n",
    "ensembl_entrez.rename(columns={'xref': 'entrez_id', 'gene_stable_id': 'ensembl_gene_id'}, inplace=True)\n",
    "\n",
    "# remove all rows that are not dbname \"EntrezGene\"\n",
    "ensembl_entrez = ensembl_entrez.loc[ensembl_entrez['db_name'].apply(lambda x: x == 'EntrezGene')]\n",
    "\n",
    "# replace \"-\"\n",
    "ensembl_entrez.replace('-', 'None', inplace=True)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_entrez.fillna('None', inplace=True)\n",
    "\n",
    "# remove uneeded columns\n",
    "ensembl_entrez.drop(['db_name', 'info_type', 'source_identity', 'xref_identity', 'linkage_type'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_entrez.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Merge Annotation Data_ - `ensembl_uniprot` + `ensembl_entrez`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensembl_gene_id</th>\n",
       "      <th>transcript_stable_id</th>\n",
       "      <th>protein_stable_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>entrez_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000186092</td>\n",
       "      <td>ENST00000641515</td>\n",
       "      <td>ENSP00000493376</td>\n",
       "      <td>A0A2U3U0J3</td>\n",
       "      <td>79501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000186092</td>\n",
       "      <td>ENST00000335137</td>\n",
       "      <td>ENSP00000334393</td>\n",
       "      <td>Q8NH21</td>\n",
       "      <td>79501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000284733</td>\n",
       "      <td>ENST00000426406</td>\n",
       "      <td>ENSP00000409316</td>\n",
       "      <td>Q6IEY1</td>\n",
       "      <td>729759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ensembl_gene_id transcript_stable_id protein_stable_id  uniprot_id  \\\n",
       "0  ENSG00000186092      ENST00000641515   ENSP00000493376  A0A2U3U0J3   \n",
       "1  ENSG00000186092      ENST00000335137   ENSP00000334393      Q8NH21   \n",
       "2  ENSG00000284733      ENST00000426406   ENSP00000409316      Q6IEY1   \n",
       "\n",
       "  entrez_id  \n",
       "0     79501  \n",
       "1     79501  \n",
       "2    729759  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensembl_annot = pandas.merge(ensembl_uniprot,\n",
    "                             ensembl_entrez,\n",
    "                             left_on=['ensembl_gene_id', 'transcript_stable_id', 'protein_stable_id'],\n",
    "                             right_on=['ensembl_gene_id', 'transcript_stable_id', 'protein_stable_id'],\n",
    "                             how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_annot.fillna('None', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_annot.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Merge Ensembl Annotation and Gene Set Data_ - `ensembl_geneset` + `ensembl_annot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensembl_gene_id</th>\n",
       "      <th>transcript_stable_id</th>\n",
       "      <th>symbol</th>\n",
       "      <th>primary_symbol</th>\n",
       "      <th>ensembl_gene_type</th>\n",
       "      <th>transcript_name</th>\n",
       "      <th>ensembl_transcript_type</th>\n",
       "      <th>master_gene_type</th>\n",
       "      <th>master_transcript_type</th>\n",
       "      <th>protein_stable_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>entrez_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>None</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000456328</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-202</td>\n",
       "      <td>processed_transcript</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000450305</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-201</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ensembl_gene_id transcript_stable_id   symbol primary_symbol  \\\n",
       "0  ENSG00000223972                 None  DDX11L1        DDX11L1   \n",
       "1  ENSG00000223972      ENST00000456328  DDX11L1        DDX11L1   \n",
       "2  ENSG00000223972      ENST00000450305  DDX11L1        DDX11L1   \n",
       "\n",
       "                    ensembl_gene_type transcript_name  \\\n",
       "0  transcribed_unprocessed_pseudogene            None   \n",
       "1  transcribed_unprocessed_pseudogene     DDX11L1-202   \n",
       "2  transcribed_unprocessed_pseudogene     DDX11L1-201   \n",
       "\n",
       "              ensembl_transcript_type master_gene_type master_transcript_type  \\\n",
       "0                                None       pseudogene     not protein-coding   \n",
       "1                processed_transcript       pseudogene         protein-coding   \n",
       "2  transcribed_unprocessed_pseudogene       pseudogene     not protein-coding   \n",
       "\n",
       "  protein_stable_id uniprot_id entrez_id  \n",
       "0              None       None      None  \n",
       "1              None       None      None  \n",
       "2              None       None      None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensembl = pandas.merge(ensembl_geneset,\n",
    "                       ensembl_annot,\n",
    "                       left_on = ['ensembl_gene_id', 'transcript_stable_id'],\n",
    "                       right_on = ['ensembl_gene_id', 'transcript_stable_id'],\n",
    "                       how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl.fillna('None', inplace=True)\n",
    "ensembl.replace('NA','None', inplace=True, regex=False)\n",
    "\n",
    "# preview data\n",
    "ensembl.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Save Cleaned Ensembl Data_  \n",
    "Save the cleaned Ensembl data so that it can be used when generating node metadata for transcript identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl.to_csv(processed_data_location + 'ensembl_identifier_data_cleaned.txt',\n",
    "               header=True,\n",
    "               sep='\\t',\n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**UniProt Data**   \n",
    "_Human Gene Set Data_ - `uniprot_identifier_mapping.tab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.uniprot.org/uniprot/?query=&fil=organism%3A%22Homo%20sapiens%20(Human)%20%5B9606%5D%22&columns=id%2Cdatabase(GeneID)%2Cdatabase(Ensembl)%2Cdatabase(HGNC)%2Cgenes(PREFERRED)%2Cgenes(ALTERNATIVE)&format=tab'\n",
    "data_downloader(url, unprocessed_data_location, 'uniprot_identifier_mapping.tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot = pandas.read_csv(unprocessed_data_location + 'uniprot_identifier_mapping.tab',\n",
    "                          header=0,\n",
    "                          delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, and unnesting `;` and `\" \"` delimited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>entrez_id</th>\n",
       "      <th>transcript_stable_id</th>\n",
       "      <th>hgnc_id</th>\n",
       "      <th>symbol</th>\n",
       "      <th>synonyms</th>\n",
       "      <th>primary_symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q6ZSK4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>32293</td>\n",
       "      <td>C11orf39</td>\n",
       "      <td>C11orf39</td>\n",
       "      <td>NTM-AS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q6ZSK4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>32293</td>\n",
       "      <td>NTM-AS1</td>\n",
       "      <td>C11orf39</td>\n",
       "      <td>NTM-AS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q9Y263</td>\n",
       "      <td>9373</td>\n",
       "      <td>ENST00000397292</td>\n",
       "      <td>9043</td>\n",
       "      <td>PLAP</td>\n",
       "      <td>PLAP</td>\n",
       "      <td>PLAA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot_id entrez_id transcript_stable_id hgnc_id    symbol  synonyms  \\\n",
       "0     Q6ZSK4      None                 None   32293  C11orf39  C11orf39   \n",
       "1     Q6ZSK4      None                 None   32293   NTM-AS1  C11orf39   \n",
       "2     Q9Y263      9373      ENST00000397292    9043      PLAP      PLAP   \n",
       "\n",
       "  primary_symbol  \n",
       "0        NTM-AS1  \n",
       "1        NTM-AS1  \n",
       "2           PLAA  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace NaN with 'None'\n",
    "uniprot.fillna('None', inplace=True)\n",
    "\n",
    "# rename columns\n",
    "uniprot.rename(columns={'Entry': 'uniprot_id',\n",
    "                        'Cross-reference (GeneID)': 'entrez_id',\n",
    "                        'Ensembl transcript': 'transcript_stable_id',\n",
    "                        'Cross-reference (HGNC)': 'hgnc_id',\n",
    "                        'Gene names  (synonym )': 'synonyms',\n",
    "                        'Gene names  (primary )' :'symbol'}, inplace=True)\n",
    "\n",
    "# update space-delimited synonyms to a '|'\n",
    "uniprot['synonyms'] = uniprot['synonyms'].apply(lambda x: '|'.join(x.split()) if x.isupper() else x)\n",
    "\n",
    "# combine symbols and synonyms into single column\n",
    "uniprot['primary_symbol'] = uniprot['symbol']\n",
    "uniprot['symbol'] = uniprot['synonyms'] + '|' + uniprot['symbol']\n",
    "\n",
    "# explode nested data\n",
    "explode_df_uniprot = explodes_data(uniprot.copy(), ['transcript_stable_id', 'entrez_id', 'hgnc_id'], ';')\n",
    "explode_df_uniprot = explodes_data(explode_df_uniprot.copy(), ['symbol'], '|')\n",
    "\n",
    "\n",
    "# strip out uniprot names\n",
    "explode_df_uniprot['transcript_stable_id'].replace('\\s.*','', inplace=True, regex=True)\n",
    "\n",
    "# remove duplicates\n",
    "explode_df_uniprot.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_uniprot.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**NCBI Data**   \n",
    "_Human Gene Set Data_ - `Homo_sapiens.gene_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi_gene = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.gene_info',\n",
    "                            header=0,\n",
    "                            delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, updating data types (i.e. making all columns type `str`), and unnesting `|` delimited data. Then, the `gene_type` variable is cleaned such that each of the variable's values are re-grouped to be `protein-coding`, `other` or `ncRNA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entrez_id</th>\n",
       "      <th>symbol</th>\n",
       "      <th>synonyms</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>map_location</th>\n",
       "      <th>Other_designations</th>\n",
       "      <th>primary_symbol</th>\n",
       "      <th>name</th>\n",
       "      <th>entrez_gene_type</th>\n",
       "      <th>master_gene_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A1BG</td>\n",
       "      <td>A1B|ABG|GAB|HYST2477</td>\n",
       "      <td>19</td>\n",
       "      <td>19q13.43</td>\n",
       "      <td>alpha-1B-glycoprotein|HEL-S-163pA|epididymis s...</td>\n",
       "      <td>A1BG</td>\n",
       "      <td>alpha-1-B glycoprotein</td>\n",
       "      <td>protein-coding</td>\n",
       "      <td>protein-coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A1B</td>\n",
       "      <td>A1B|ABG|GAB|HYST2477</td>\n",
       "      <td>19</td>\n",
       "      <td>19q13.43</td>\n",
       "      <td>alpha-1B-glycoprotein|HEL-S-163pA|epididymis s...</td>\n",
       "      <td>A1BG</td>\n",
       "      <td>alpha-1-B glycoprotein</td>\n",
       "      <td>protein-coding</td>\n",
       "      <td>protein-coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ABG</td>\n",
       "      <td>A1B|ABG|GAB|HYST2477</td>\n",
       "      <td>19</td>\n",
       "      <td>19q13.43</td>\n",
       "      <td>alpha-1B-glycoprotein|HEL-S-163pA|epididymis s...</td>\n",
       "      <td>A1BG</td>\n",
       "      <td>alpha-1-B glycoprotein</td>\n",
       "      <td>protein-coding</td>\n",
       "      <td>protein-coding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entrez_id symbol              synonyms chromosome map_location  \\\n",
       "0         1   A1BG  A1B|ABG|GAB|HYST2477         19     19q13.43   \n",
       "2         1    A1B  A1B|ABG|GAB|HYST2477         19     19q13.43   \n",
       "3         1    ABG  A1B|ABG|GAB|HYST2477         19     19q13.43   \n",
       "\n",
       "                                  Other_designations primary_symbol  \\\n",
       "0  alpha-1B-glycoprotein|HEL-S-163pA|epididymis s...           A1BG   \n",
       "2  alpha-1B-glycoprotein|HEL-S-163pA|epididymis s...           A1BG   \n",
       "3  alpha-1B-glycoprotein|HEL-S-163pA|epididymis s...           A1BG   \n",
       "\n",
       "                     name entrez_gene_type master_gene_type  \n",
       "0  alpha-1-B glycoprotein   protein-coding   protein-coding  \n",
       "2  alpha-1-B glycoprotein   protein-coding   protein-coding  \n",
       "3  alpha-1-B glycoprotein   protein-coding   protein-coding  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all rows that are not human\n",
    "ncbi_gene = ncbi_gene.loc[ncbi_gene['#tax_id'].apply(lambda x: x == 9606)]\n",
    "\n",
    "# replace \"-\" with \"None\"\n",
    "ncbi_gene.replace('-','None', inplace=True)\n",
    "\n",
    "# rename columns before merging\n",
    "ncbi_gene.rename(columns={'GeneID': 'entrez_id', 'Symbol': 'symbol', 'Synonyms': 'synonyms'}, inplace=True)\n",
    "\n",
    "# combine symbol columns into single column\n",
    "ncbi_gene['primary_symbol'] = ncbi_gene['symbol']\n",
    "ncbi_gene['symbol'] = ncbi_gene['Symbol_from_nomenclature_authority'] + '|' + ncbi_gene['symbol'] + '|' + ncbi_gene['synonyms']\n",
    "ncbi_gene['name'] = ncbi_gene['Full_name_from_nomenclature_authority'] + '|' + ncbi_gene['description']\n",
    "\n",
    "# explode nested data\n",
    "explode_df_ncbi_gene = explodes_data(ncbi_gene.copy(), ['symbol', 'name'], '|')\n",
    "\n",
    "# make sure that merge columns are of same type\n",
    "explode_df_ncbi_gene['entrez_id'] = explode_df_ncbi_gene['entrez_id'].astype(str)\n",
    "\n",
    "# reformat entrez gene type\n",
    "explode_df_ncbi_gene['entrez_gene_type'] = explode_df_ncbi_gene['type_of_gene']\n",
    "explode_df_ncbi_gene['entrez_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "explode_df_ncbi_gene['entrez_gene_type'].replace('pseudo', 'pseudogene', inplace=True, regex=False)\n",
    "explode_df_ncbi_gene['entrez_gene_type'].replace('biological-region', 'biological region', inplace=True, regex=False)\n",
    "\n",
    "# reformat master gene type\n",
    "explode_df_ncbi_gene['master_gene_type'] = explode_df_ncbi_gene['entrez_gene_type']\n",
    "explode_df_ncbi_gene['master_gene_type'].replace('biological region', 'other', inplace=True, regex=False)\n",
    "explode_df_ncbi_gene['master_gene_type'].replace('scRNA', 'ncRNA', inplace=True, regex=False)\n",
    "explode_df_ncbi_gene['master_gene_type'].replace('snoRNA', 'ncRNA', inplace=True, regex=False)\n",
    "explode_df_ncbi_gene['master_gene_type'].replace('snRNA', 'ncRNA', inplace=True, regex=False)\n",
    "\n",
    "# remove uneeded columns\n",
    "explode_df_ncbi_gene.drop(['type_of_gene', 'dbXrefs', 'description', 'Nomenclature_status', 'Modification_date',\n",
    "                           'LocusTag', '#tax_id', 'Full_name_from_nomenclature_authority', 'Feature_type',\n",
    "                           'Symbol_from_nomenclature_authority'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "explode_df_ncbi_gene.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_ncbi_gene.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Protein Ontology Identifier Mapping Data**   \n",
    "_Protein Ontology Identifier Data_ - `promapping.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://proconsortium.org/download/current/promapping.txt'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pro_mapping = pandas.read_csv(unprocessed_data_location + 'promapping.txt',\n",
    "                              header=None,\n",
    "                              names=['pro_id', 'Entry', 'pro_mapping'],\n",
    "                              delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Basic filtering to to include `Protein Ontology` mappings to `Uniprot` identifiers and cleaning to update formatting of accession values in order to remove `UniProtKB:` is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pro_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PR:000000005</td>\n",
       "      <td>P37173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PR:000000005</td>\n",
       "      <td>P38438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PR:000000005</td>\n",
       "      <td>Q62312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pro_id uniprot_id\n",
       "6  PR:000000005     P37173\n",
       "7  PR:000000005     P38438\n",
       "8  PR:000000005     Q62312"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows without 'UniProtKB'\n",
    "pro_mapping = pro_mapping.loc[pro_mapping['Entry'].apply(lambda x: x.startswith('UniProtKB:'))] \n",
    "\n",
    "# remove identifier type, which appears before ':'\n",
    "pro_mapping['Entry'].replace('(^\\w*\\:)','', inplace=True, regex=True)\n",
    "\n",
    "# rename columns before merging\n",
    "pro_mapping.rename(columns={'Entry': 'uniprot_id'}, inplace=True)\n",
    "\n",
    "# remove uneeded columns\n",
    "pro_mapping.drop(['pro_mapping'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "pro_mapping.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "pro_mapping.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Merge Processed Data:** `hgnc` + `ensembl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensembl_gene_id</th>\n",
       "      <th>transcript_stable_id</th>\n",
       "      <th>symbol</th>\n",
       "      <th>primary_symbol</th>\n",
       "      <th>ensembl_gene_type</th>\n",
       "      <th>transcript_name</th>\n",
       "      <th>ensembl_transcript_type</th>\n",
       "      <th>master_gene_type</th>\n",
       "      <th>master_transcript_type</th>\n",
       "      <th>protein_stable_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>entrez_id</th>\n",
       "      <th>hgnc_id</th>\n",
       "      <th>hgnc_gene_type</th>\n",
       "      <th>name</th>\n",
       "      <th>map_location</th>\n",
       "      <th>synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>None</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000456328</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-202</td>\n",
       "      <td>processed_transcript</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000450305</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-201</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ensembl_gene_id transcript_stable_id   symbol primary_symbol  \\\n",
       "0  ENSG00000223972                 None  DDX11L1        DDX11L1   \n",
       "1  ENSG00000223972      ENST00000456328  DDX11L1        DDX11L1   \n",
       "2  ENSG00000223972      ENST00000450305  DDX11L1        DDX11L1   \n",
       "\n",
       "                    ensembl_gene_type transcript_name  \\\n",
       "0  transcribed_unprocessed_pseudogene            None   \n",
       "1  transcribed_unprocessed_pseudogene     DDX11L1-202   \n",
       "2  transcribed_unprocessed_pseudogene     DDX11L1-201   \n",
       "\n",
       "              ensembl_transcript_type master_gene_type master_transcript_type  \\\n",
       "0                                None       pseudogene     not protein-coding   \n",
       "1                processed_transcript       pseudogene         protein-coding   \n",
       "2  transcribed_unprocessed_pseudogene       pseudogene     not protein-coding   \n",
       "\n",
       "  protein_stable_id uniprot_id entrez_id hgnc_id hgnc_gene_type  name  \\\n",
       "0              None       None      None    None           None  None   \n",
       "1              None       None      None    None           None  None   \n",
       "2              None       None      None    None           None  None   \n",
       "\n",
       "  map_location synonyms  \n",
       "0         None     None  \n",
       "1         None     None  \n",
       "2         None     None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge uniprot and ncbi data\n",
    "ensembl_hgnc_merged_data = pandas.merge(ensembl, explode_df_hgnc,\n",
    "                                        left_on=['ensembl_gene_id', 'entrez_id', 'uniprot_id',\n",
    "                                                 'master_gene_type', 'symbol', 'primary_symbol'],\n",
    "                                        right_on=['ensembl_gene_id', 'entrez_id', 'uniprot_id',\n",
    "                                                  'master_gene_type', 'symbol', 'primary_symbol'],\n",
    "                                        how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_hgnc_merged_data.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_hgnc_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Merge Processed Data:** `ensembl_hgnc_merged_data` + `explode_df_uniprot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensembl_gene_id</th>\n",
       "      <th>transcript_stable_id</th>\n",
       "      <th>symbol</th>\n",
       "      <th>primary_symbol</th>\n",
       "      <th>ensembl_gene_type</th>\n",
       "      <th>transcript_name</th>\n",
       "      <th>ensembl_transcript_type</th>\n",
       "      <th>master_gene_type</th>\n",
       "      <th>master_transcript_type</th>\n",
       "      <th>protein_stable_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>entrez_id</th>\n",
       "      <th>hgnc_id</th>\n",
       "      <th>hgnc_gene_type</th>\n",
       "      <th>name</th>\n",
       "      <th>map_location</th>\n",
       "      <th>synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>None</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000456328</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-202</td>\n",
       "      <td>processed_transcript</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000450305</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-201</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ensembl_gene_id transcript_stable_id   symbol primary_symbol  \\\n",
       "0  ENSG00000223972                 None  DDX11L1        DDX11L1   \n",
       "1  ENSG00000223972      ENST00000456328  DDX11L1        DDX11L1   \n",
       "2  ENSG00000223972      ENST00000450305  DDX11L1        DDX11L1   \n",
       "\n",
       "                    ensembl_gene_type transcript_name  \\\n",
       "0  transcribed_unprocessed_pseudogene            None   \n",
       "1  transcribed_unprocessed_pseudogene     DDX11L1-202   \n",
       "2  transcribed_unprocessed_pseudogene     DDX11L1-201   \n",
       "\n",
       "              ensembl_transcript_type master_gene_type master_transcript_type  \\\n",
       "0                                None       pseudogene     not protein-coding   \n",
       "1                processed_transcript       pseudogene         protein-coding   \n",
       "2  transcribed_unprocessed_pseudogene       pseudogene     not protein-coding   \n",
       "\n",
       "  protein_stable_id uniprot_id entrez_id hgnc_id hgnc_gene_type  name  \\\n",
       "0              None       None      None    None           None  None   \n",
       "1              None       None      None    None           None  None   \n",
       "2              None       None      None    None           None  None   \n",
       "\n",
       "  map_location synonyms  \n",
       "0         None     None  \n",
       "1         None     None  \n",
       "2         None     None  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge uniprot and ncbi data\n",
    "ensembl_hgnc_uniprot_merged_data = pandas.merge(ensembl_hgnc_merged_data, explode_df_uniprot,\n",
    "                                                left_on=['entrez_id', 'hgnc_id', 'uniprot_id', 'transcript_stable_id',\n",
    "                                                         'symbol', 'primary_symbol', 'synonyms'],\n",
    "                                                right_on=['entrez_id', 'hgnc_id', 'uniprot_id', 'transcript_stable_id',\n",
    "                                                          'symbol', 'primary_symbol', 'synonyms'],\n",
    "                                                how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_hgnc_uniprot_merged_data.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_hgnc_uniprot_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_uniprot_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Merge Processed Data:** `ensembl_hgnc_uniprot_merged_data` + `Homo_sapiens.gene_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensembl_gene_id</th>\n",
       "      <th>transcript_stable_id</th>\n",
       "      <th>symbol</th>\n",
       "      <th>primary_symbol</th>\n",
       "      <th>ensembl_gene_type</th>\n",
       "      <th>transcript_name</th>\n",
       "      <th>ensembl_transcript_type</th>\n",
       "      <th>master_gene_type</th>\n",
       "      <th>master_transcript_type</th>\n",
       "      <th>protein_stable_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>entrez_id</th>\n",
       "      <th>hgnc_id</th>\n",
       "      <th>hgnc_gene_type</th>\n",
       "      <th>name</th>\n",
       "      <th>map_location</th>\n",
       "      <th>synonyms</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>Other_designations</th>\n",
       "      <th>entrez_gene_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>None</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000456328</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-202</td>\n",
       "      <td>processed_transcript</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000450305</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-201</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ensembl_gene_id transcript_stable_id   symbol primary_symbol  \\\n",
       "0  ENSG00000223972                 None  DDX11L1        DDX11L1   \n",
       "1  ENSG00000223972      ENST00000456328  DDX11L1        DDX11L1   \n",
       "2  ENSG00000223972      ENST00000450305  DDX11L1        DDX11L1   \n",
       "\n",
       "                    ensembl_gene_type transcript_name  \\\n",
       "0  transcribed_unprocessed_pseudogene            None   \n",
       "1  transcribed_unprocessed_pseudogene     DDX11L1-202   \n",
       "2  transcribed_unprocessed_pseudogene     DDX11L1-201   \n",
       "\n",
       "              ensembl_transcript_type master_gene_type master_transcript_type  \\\n",
       "0                                None       pseudogene     not protein-coding   \n",
       "1                processed_transcript       pseudogene         protein-coding   \n",
       "2  transcribed_unprocessed_pseudogene       pseudogene     not protein-coding   \n",
       "\n",
       "  protein_stable_id uniprot_id entrez_id hgnc_id hgnc_gene_type  name  \\\n",
       "0              None       None      None    None           None  None   \n",
       "1              None       None      None    None           None  None   \n",
       "2              None       None      None    None           None  None   \n",
       "\n",
       "  map_location synonyms chromosome Other_designations entrez_gene_type  \n",
       "0         None     None       None               None             None  \n",
       "1         None     None       None               None             None  \n",
       "2         None     None       None               None             None  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensembl_hgnc_uniprot_ncbi_merged_data = pandas.merge(ensembl_hgnc_uniprot_merged_data, explode_df_ncbi_gene,\n",
    "                                                     left_on=['entrez_id', 'master_gene_type', 'symbol', 'primary_symbol',\n",
    "                                                              'synonyms', 'name', 'map_location'],\n",
    "                                                     right_on=['entrez_id', 'master_gene_type', 'symbol', 'primary_symbol',\n",
    "                                                               'synonyms', 'name', 'map_location'],\n",
    "                                                     how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Merge Processed Data:** `ensembl_hgnc_uniprot_ncbi_merged_data` + `promapping.txt`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensembl_gene_id</th>\n",
       "      <th>transcript_stable_id</th>\n",
       "      <th>symbol</th>\n",
       "      <th>primary_symbol</th>\n",
       "      <th>ensembl_gene_type</th>\n",
       "      <th>transcript_name</th>\n",
       "      <th>ensembl_transcript_type</th>\n",
       "      <th>master_gene_type</th>\n",
       "      <th>master_transcript_type</th>\n",
       "      <th>protein_stable_id</th>\n",
       "      <th>...</th>\n",
       "      <th>entrez_id</th>\n",
       "      <th>hgnc_id</th>\n",
       "      <th>hgnc_gene_type</th>\n",
       "      <th>name</th>\n",
       "      <th>map_location</th>\n",
       "      <th>synonyms</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>Other_designations</th>\n",
       "      <th>entrez_gene_type</th>\n",
       "      <th>pro_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>None</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000456328</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-202</td>\n",
       "      <td>processed_transcript</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000450305</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-201</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ensembl_gene_id transcript_stable_id   symbol primary_symbol  \\\n",
       "0  ENSG00000223972                 None  DDX11L1        DDX11L1   \n",
       "1  ENSG00000223972      ENST00000456328  DDX11L1        DDX11L1   \n",
       "2  ENSG00000223972      ENST00000450305  DDX11L1        DDX11L1   \n",
       "\n",
       "                    ensembl_gene_type transcript_name  \\\n",
       "0  transcribed_unprocessed_pseudogene            None   \n",
       "1  transcribed_unprocessed_pseudogene     DDX11L1-202   \n",
       "2  transcribed_unprocessed_pseudogene     DDX11L1-201   \n",
       "\n",
       "              ensembl_transcript_type master_gene_type master_transcript_type  \\\n",
       "0                                None       pseudogene     not protein-coding   \n",
       "1                processed_transcript       pseudogene         protein-coding   \n",
       "2  transcribed_unprocessed_pseudogene       pseudogene     not protein-coding   \n",
       "\n",
       "  protein_stable_id  ... entrez_id hgnc_id hgnc_gene_type  name map_location  \\\n",
       "0              None  ...      None    None           None  None         None   \n",
       "1              None  ...      None    None           None  None         None   \n",
       "2              None  ...      None    None           None  None         None   \n",
       "\n",
       "  synonyms chromosome Other_designations entrez_gene_type pro_id  \n",
       "0     None       None               None             None   None  \n",
       "1     None       None               None             None   None  \n",
       "2     None       None               None             None   None  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = pandas.merge(ensembl_hgnc_uniprot_ncbi_merged_data, pro_mapping,\n",
    "                           left_on='uniprot_id',\n",
    "                           right_on='uniprot_id',\n",
    "                           how='outer')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "merged_data.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fix Symbol Formatting_  \n",
    "some genes are formatted similarly to dates (e.g. `DEC1`), which can be erroneously re-formatted during input as a date value (i.e. `1-DEC`). In order for the data to be successfully merged with other data sources, all date-formatted genes need to be resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2094450/2094450 [00:01<00:00, 1756214.74it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_dates = []\n",
    "\n",
    "for x in tqdm(list(merged_data['symbol'])):\n",
    "    if '-' in x and len(x.split('-')[0]) < 3 and len(x.split('-')[1]) == 3:\n",
    "        clean_dates.append(x.split('-')[1].upper() + x.split('-')[0])\n",
    "    else:\n",
    "        clean_dates.append(x)\n",
    "\n",
    "# add cleaned date var back to data set\n",
    "merged_data['symbol'] = clean_dates\n",
    "\n",
    "# remove duplicates\n",
    "merged_data.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write Merged Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensembl_gene_id</th>\n",
       "      <th>transcript_stable_id</th>\n",
       "      <th>symbol</th>\n",
       "      <th>primary_symbol</th>\n",
       "      <th>ensembl_gene_type</th>\n",
       "      <th>transcript_name</th>\n",
       "      <th>ensembl_transcript_type</th>\n",
       "      <th>master_gene_type</th>\n",
       "      <th>master_transcript_type</th>\n",
       "      <th>protein_stable_id</th>\n",
       "      <th>...</th>\n",
       "      <th>entrez_id</th>\n",
       "      <th>hgnc_id</th>\n",
       "      <th>hgnc_gene_type</th>\n",
       "      <th>name</th>\n",
       "      <th>map_location</th>\n",
       "      <th>synonyms</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>Other_designations</th>\n",
       "      <th>entrez_gene_type</th>\n",
       "      <th>pro_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>None</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>None</td>\n",
       "      <td>unknown</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>unknown</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000456328</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-202</td>\n",
       "      <td>processed_transcript</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>unknown</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>ENST00000450305</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>DDX11L1-201</td>\n",
       "      <td>transcribed_unprocessed_pseudogene</td>\n",
       "      <td>pseudogene</td>\n",
       "      <td>not protein-coding</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>unknown</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ensembl_gene_id transcript_stable_id   symbol primary_symbol  \\\n",
       "0  ENSG00000223972                 None  DDX11L1        DDX11L1   \n",
       "1  ENSG00000223972      ENST00000456328  DDX11L1        DDX11L1   \n",
       "2  ENSG00000223972      ENST00000450305  DDX11L1        DDX11L1   \n",
       "\n",
       "                    ensembl_gene_type transcript_name  \\\n",
       "0  transcribed_unprocessed_pseudogene            None   \n",
       "1  transcribed_unprocessed_pseudogene     DDX11L1-202   \n",
       "2  transcribed_unprocessed_pseudogene     DDX11L1-201   \n",
       "\n",
       "              ensembl_transcript_type master_gene_type master_transcript_type  \\\n",
       "0                             unknown       pseudogene     not protein-coding   \n",
       "1                processed_transcript       pseudogene         protein-coding   \n",
       "2  transcribed_unprocessed_pseudogene       pseudogene     not protein-coding   \n",
       "\n",
       "  protein_stable_id  ... entrez_id hgnc_id hgnc_gene_type  name map_location  \\\n",
       "0              None  ...      None    None        unknown  None         None   \n",
       "1              None  ...      None    None        unknown  None         None   \n",
       "2              None  ...      None    None        unknown  None         None   \n",
       "\n",
       "  synonyms chromosome Other_designations entrez_gene_type pro_id  \n",
       "0     None       None               None          unknown   None  \n",
       "1     None       None               None          unknown   None  \n",
       "2     None       None               None          unknown   None  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure that all gene and transcript type colunmns have none recoded to unknown or not protein-coding\n",
    "merged_data['hgnc_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['ensembl_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['entrez_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['master_gene_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "merged_data['master_transcript_type'].replace('None', 'not protein-coding', inplace=True, regex=False)\n",
    "merged_data['ensembl_transcript_type'].replace('None', 'unknown', inplace=True, regex=False)\n",
    "\n",
    "# remove duplicates\n",
    "merged_data_clean = merged_data.drop_duplicates(subset=None, keep='first')\n",
    "\n",
    "# write data\n",
    "merged_data_clean.to_csv(processed_data_location + 'Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt',\n",
    "                         header=True,\n",
    "                         sep='\\t',\n",
    "                         index=False)\n",
    "    \n",
    "# preview data\n",
    "merged_data_clean.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Create a Master Mapping Dictionary**  \n",
    "Although the above steps result in a `pandas.Dataframe` of the merged identifiers, there is still work needed in order to be able to obtain a complete mapping between the identifiers. For example, if you were to search for entrez identifier `entrez_259234` you would find the following mappings: `entrez_259234-ENSG00000233316`, `entrez_259234-DSCR10`. If you only had `ENSG00000233316`, from the current data you would be unable to obtain the gene symbol without first mapping to the Entrez gene identifier. \n",
    "\n",
    "To solve this problem, we build a master dictionary where the keys are `ensembl_gene_id`, `transcript_stable_id`, `symbol`, `protein_stable_id`, `uniprot_id`, `entrez_id`, `hgnc_id`, and `pro_id` identifiers and values are the list of identifiers that match to each identifier. It's important to note that there are several labeling identifiers (i.e. `name`, `primary_symbol`, `chromosome`, `map_location`, `Other_designations`, `synonyms`, `transcript_name`, `gene_type_update`, and `trasnscript_type_update`), which will only be mapped when clustered against one of the primary identifier types (i.e. the keys described above).\n",
    "\n",
    "_Note_. The next chunk takes approximately ~100-120 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/180 [00:00<?, ?it/s]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 623342.71it/s]\u001b[A\n",
      "  1%|          | 1/180 [00:16<47:49, 16.03s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 498799.52it/s]\u001b[A\n",
      "  1%|          | 2/180 [00:33<48:37, 16.39s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 451607.09it/s]\u001b[A\n",
      "  2%|▏         | 3/180 [00:55<53:19, 18.07s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 503263.74it/s]\u001b[A\n",
      "  2%|▏         | 4/180 [01:12<52:34, 17.92s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 556110.51it/s]\u001b[A\n",
      "  3%|▎         | 5/180 [01:28<50:35, 17.34s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 468904.91it/s]\u001b[A\n",
      "  3%|▎         | 6/180 [01:45<49:16, 16.99s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 546653.25it/s]\u001b[A\n",
      "  4%|▍         | 7/180 [02:00<47:49, 16.58s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 515661.02it/s]\u001b[A\n",
      "  4%|▍         | 8/180 [02:16<46:49, 16.33s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 536405.89it/s]\u001b[A\n",
      "  5%|▌         | 9/180 [02:31<45:30, 15.97s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 533179.84it/s]\u001b[A\n",
      "  6%|▌         | 10/180 [02:47<45:03, 15.90s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 511611.67it/s]\u001b[A\n",
      "  6%|▌         | 11/180 [03:03<44:51, 15.93s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 520421.45it/s]\u001b[A\n",
      "  7%|▋         | 12/180 [03:18<44:09, 15.77s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 551572.21it/s]\u001b[A\n",
      "  7%|▋         | 13/180 [03:33<43:32, 15.64s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 512931.74it/s]\u001b[A\n",
      "  8%|▊         | 14/180 [03:49<43:09, 15.60s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 488917.99it/s]\u001b[A\n",
      "  8%|▊         | 15/180 [04:05<42:52, 15.59s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 515840.09it/s]\u001b[A\n",
      "  9%|▉         | 16/180 [04:23<44:35, 16.31s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      " 54%|█████▍    | 34309/63748 [00:00<00:00, 343087.87it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 267626.58it/s]\u001b[A\n",
      "  9%|▉         | 17/180 [04:42<46:30, 17.12s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 536599.67it/s]\u001b[A\n",
      " 10%|█         | 18/180 [05:03<49:59, 18.51s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 558634.12it/s]\u001b[A\n",
      " 11%|█         | 19/180 [05:25<51:56, 19.36s/it]\n",
      "  0%|          | 0/63748 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 63748/63748 [00:00<00:00, 455430.93it/s]\u001b[A\n",
      " 11%|█         | 20/180 [05:42<50:05, 18.78s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 23%|██▎       | 56840/243818 [00:00<00:00, 568396.48it/s]\u001b[A\n",
      " 44%|████▍     | 107098/243818 [00:00<00:00, 546909.68it/s]\u001b[A\n",
      " 65%|██████▌   | 158522/243818 [00:00<00:00, 536679.63it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 533931.12it/s]\u001b[A\n",
      " 12%|█▏        | 21/180 [06:41<1:21:38, 30.81s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 22%|██▏       | 52528/243818 [00:00<00:00, 525279.25it/s]\u001b[A\n",
      " 42%|████▏     | 103443/243818 [00:00<00:00, 520330.91it/s]\u001b[A\n",
      " 64%|██████▎   | 155013/243818 [00:00<00:00, 518929.72it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 497307.54it/s]\u001b[A\n",
      " 12%|█▏        | 22/180 [07:38<1:41:57, 38.72s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 49103/243818 [00:00<00:00, 491019.93it/s]\u001b[A\n",
      " 41%|████▏     | 100716/243818 [00:00<00:00, 498291.03it/s]\u001b[A\n",
      " 62%|██████▏   | 150234/243818 [00:00<00:00, 497353.41it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 505116.04it/s]\u001b[A\n",
      " 13%|█▎        | 23/180 [08:49<2:06:44, 48.44s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█▉        | 48248/243818 [00:00<00:00, 482479.31it/s]\u001b[A\n",
      " 40%|███▉      | 96597/243818 [00:00<00:00, 482780.83it/s]\u001b[A\n",
      " 60%|██████    | 146768/243818 [00:00<00:00, 488307.68it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 489615.80it/s]\u001b[A\n",
      " 13%|█▎        | 24/180 [10:01<2:23:56, 55.36s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 22%|██▏       | 52823/243818 [00:00<00:00, 528229.24it/s]\u001b[A\n",
      " 42%|████▏     | 101432/243818 [00:00<00:00, 514838.37it/s]\u001b[A\n",
      " 62%|██████▏   | 151589/243818 [00:00<00:00, 510783.02it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 509597.40it/s]\u001b[A\n",
      " 14%|█▍        | 25/180 [11:12<2:35:16, 60.11s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 22%|██▏       | 52962/243818 [00:00<00:00, 529619.24it/s]\u001b[A\n",
      " 42%|████▏     | 102005/243818 [00:00<00:00, 517218.45it/s]\u001b[A\n",
      " 63%|██████▎   | 152932/243818 [00:00<00:00, 514806.64it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 511158.30it/s]\u001b[A\n",
      " 14%|█▍        | 26/180 [12:08<2:31:21, 58.97s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 21%|██▏       | 51934/243818 [00:00<00:00, 519334.30it/s]\u001b[A\n",
      " 43%|████▎     | 103908/243818 [00:00<00:00, 519454.24it/s]\u001b[A\n",
      " 64%|██████▍   | 156702/243818 [00:00<00:00, 521969.86it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 510621.05it/s]\u001b[A\n",
      " 15%|█▌        | 27/180 [13:05<2:28:32, 58.25s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 23%|██▎       | 55891/243818 [00:00<00:00, 558909.20it/s]\u001b[A\n",
      " 46%|████▌     | 112468/243818 [00:00<00:00, 560948.47it/s]\u001b[A\n",
      " 69%|██████▉   | 167858/243818 [00:00<00:00, 558814.93it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 540107.35it/s]\u001b[A\n",
      " 16%|█▌        | 28/180 [14:14<2:35:59, 61.57s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▍        | 36351/243818 [00:00<00:00, 363506.01it/s]\u001b[A\n",
      " 29%|██▉       | 71256/243818 [00:00<00:00, 359042.75it/s]\u001b[A\n",
      " 48%|████▊     | 117211/243818 [00:00<00:00, 384254.48it/s]\u001b[A\n",
      " 68%|██████▊   | 166535/243818 [00:00<00:00, 411530.96it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 439019.71it/s]\u001b[A\n",
      " 16%|█▌        | 29/180 [15:11<2:31:44, 60.29s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 49030/243818 [00:00<00:00, 490295.79it/s]\u001b[A\n",
      " 39%|███▉      | 95200/243818 [00:00<00:00, 481348.82it/s]\u001b[A\n",
      " 58%|█████▊    | 140395/243818 [00:00<00:00, 472134.00it/s]\u001b[A\n",
      " 77%|███████▋  | 187740/243818 [00:00<00:00, 472526.81it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 471203.49it/s]\u001b[A\n",
      " 17%|█▋        | 30/180 [16:13<2:31:21, 60.54s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 49323/243818 [00:00<00:00, 493219.89it/s]\u001b[A\n",
      " 34%|███▍      | 82831/243818 [00:00<00:00, 432046.87it/s]\u001b[A\n",
      " 50%|████▉     | 121267/243818 [00:00<00:00, 416540.17it/s]\u001b[A\n",
      " 62%|██████▏   | 152059/243818 [00:00<00:00, 376674.90it/s]\u001b[A\n",
      " 77%|███████▋  | 187526/243818 [00:00<00:00, 369788.49it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 384836.36it/s]\u001b[A\n",
      " 17%|█▋        | 31/180 [17:19<2:34:31, 62.22s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 49770/243818 [00:00<00:00, 497695.73it/s]\u001b[A\n",
      " 40%|████      | 98014/243818 [00:00<00:00, 493017.00it/s]\u001b[A\n",
      " 60%|██████    | 147419/243818 [00:00<00:00, 493323.41it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 490136.75it/s]\u001b[A\n",
      " 18%|█▊        | 32/180 [18:17<2:30:39, 61.08s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█▉        | 48705/243818 [00:00<00:00, 487049.30it/s]\u001b[A\n",
      " 40%|████      | 98009/243818 [00:00<00:00, 488828.54it/s]\u001b[A\n",
      " 61%|██████    | 147910/243818 [00:00<00:00, 491836.10it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 497004.70it/s]\u001b[A\n",
      " 18%|█▊        | 33/180 [19:15<2:27:09, 60.06s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█▉        | 48394/243818 [00:00<00:00, 483934.69it/s]\u001b[A\n",
      " 37%|███▋      | 91018/243818 [00:00<00:00, 465049.36it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 139596/243818 [00:00<00:00, 471078.86it/s]\u001b[A\n",
      " 77%|███████▋  | 187567/243818 [00:00<00:00, 473635.21it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 468404.44it/s]\u001b[A\n",
      " 19%|█▉        | 34/180 [20:20<2:29:36, 61.49s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 21%|██        | 51087/243818 [00:00<00:00, 510865.62it/s]\u001b[A\n",
      " 40%|████      | 97737/243818 [00:00<00:00, 496694.23it/s]\u001b[A\n",
      " 60%|█████▉    | 145572/243818 [00:00<00:00, 491043.61it/s]\u001b[A\n",
      " 79%|███████▉  | 193311/243818 [00:00<00:00, 486861.74it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 473591.21it/s]\u001b[A\n",
      " 19%|█▉        | 35/180 [21:22<2:29:28, 61.85s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 21%|██        | 50498/243818 [00:00<00:00, 504974.46it/s]\u001b[A\n",
      " 42%|████▏     | 101531/243818 [00:00<00:00, 506567.63it/s]\u001b[A\n",
      " 62%|██████▏   | 150917/243818 [00:00<00:00, 502685.88it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 509445.34it/s]\u001b[A\n",
      " 20%|██        | 36/180 [22:21<2:25:55, 60.80s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 19%|█▊        | 45354/243818 [00:00<00:00, 453535.03it/s]\u001b[A\n",
      " 39%|███▉      | 94497/243818 [00:00<00:00, 464272.95it/s]\u001b[A\n",
      " 59%|█████▉    | 144041/243818 [00:00<00:00, 473200.63it/s]\u001b[A\n",
      " 79%|███████▊  | 191882/243818 [00:00<00:00, 474748.59it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 476187.44it/s]\u001b[A\n",
      " 21%|██        | 37/180 [23:27<2:28:46, 62.43s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 21%|██        | 50464/243818 [00:00<00:00, 504634.47it/s]\u001b[A\n",
      " 41%|████      | 98914/243818 [00:00<00:00, 498418.89it/s]\u001b[A\n",
      " 62%|██████▏   | 151039/243818 [00:00<00:00, 505052.41it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 508886.10it/s]\u001b[A\n",
      " 21%|██        | 38/180 [24:27<2:25:49, 61.62s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 21%|██        | 50071/243818 [00:00<00:00, 500709.28it/s]\u001b[A\n",
      " 41%|████      | 100462/243818 [00:00<00:00, 501663.58it/s]\u001b[A\n",
      " 62%|██████▏   | 152003/243818 [00:00<00:00, 505708.61it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 502054.97it/s]\u001b[A\n",
      " 22%|██▏       | 39/180 [25:25<2:22:47, 60.76s/it]\n",
      "  0%|          | 0/243818 [00:00<?, ?it/s]\u001b[A\n",
      " 21%|██        | 51175/243818 [00:00<00:00, 511744.39it/s]\u001b[A\n",
      " 40%|████      | 98708/243818 [00:00<00:00, 500245.67it/s]\u001b[A\n",
      " 60%|██████    | 147392/243818 [00:00<00:00, 496143.98it/s]\u001b[A\n",
      " 80%|████████  | 195393/243818 [00:00<00:00, 491189.39it/s]\u001b[A\n",
      "100%|██████████| 243818/243818 [00:00<00:00, 481402.78it/s]\u001b[A\n",
      " 22%|██▏       | 40/180 [26:32<2:25:56, 62.55s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 487806.33it/s]\u001b[A\n",
      " 23%|██▎       | 41/180 [26:59<2:00:15, 51.91s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 459789.63it/s]\u001b[A\n",
      " 23%|██▎       | 42/180 [27:26<1:42:22, 44.51s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 45605/90465 [00:00<00:00, 456049.35it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 426082.69it/s]\u001b[A\n",
      " 24%|██▍       | 43/180 [27:52<1:28:53, 38.93s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      " 52%|█████▏    | 47211/90465 [00:00<00:00, 472100.32it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 443962.82it/s]\u001b[A\n",
      " 24%|██▍       | 44/180 [28:18<1:19:03, 34.88s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      " 24%|██▍       | 22001/90465 [00:00<00:00, 220005.49it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 320100.92it/s]\u001b[A\n",
      " 25%|██▌       | 45/180 [28:48<1:15:02, 33.35s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 483309.04it/s]\u001b[A\n",
      " 26%|██▌       | 46/180 [29:15<1:10:39, 31.64s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 483108.43it/s]\u001b[A\n",
      " 26%|██▌       | 47/180 [29:40<1:05:42, 29.65s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 483487.63it/s]\u001b[A\n",
      " 27%|██▋       | 48/180 [30:09<1:04:31, 29.33s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 473862.37it/s]\u001b[A\n",
      " 27%|██▋       | 49/180 [30:31<59:22, 27.20s/it]  \n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 473581.44it/s]\u001b[A\n",
      " 28%|██▊       | 50/180 [30:54<55:54, 25.80s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      " 48%|████▊     | 43647/90465 [00:00<00:00, 436469.38it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 440675.78it/s]\u001b[A\n",
      " 28%|██▊       | 51/180 [31:16<53:11, 24.74s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 494855.93it/s]\u001b[A\n",
      " 29%|██▉       | 52/180 [31:38<51:02, 23.92s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 464306.43it/s]\u001b[A\n",
      " 29%|██▉       | 53/180 [32:00<49:37, 23.44s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 482083.39it/s]\u001b[A\n",
      " 30%|███       | 54/180 [32:22<48:17, 22.99s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 457259.09it/s]\u001b[A\n",
      " 31%|███       | 55/180 [32:44<47:27, 22.78s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      " 53%|█████▎    | 48075/90465 [00:00<00:00, 480740.14it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 439354.71it/s]\u001b[A\n",
      " 31%|███       | 56/180 [33:07<46:43, 22.61s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 463219.82it/s]\u001b[A\n",
      " 32%|███▏      | 57/180 [33:33<48:30, 23.67s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 29579/90465 [00:00<00:00, 295789.58it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 303022.43it/s]\u001b[A\n",
      " 32%|███▏      | 58/180 [34:00<50:29, 24.83s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      " 42%|████▏     | 38285/90465 [00:00<00:00, 382847.63it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 306255.26it/s]\u001b[A\n",
      " 33%|███▎      | 59/180 [34:26<50:54, 25.24s/it]\n",
      "  0%|          | 0/90465 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 90465/90465 [00:00<00:00, 476595.52it/s]\u001b[A\n",
      " 33%|███▎      | 60/180 [34:54<52:07, 26.06s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 49%|████▉     | 54337/110629 [00:00<00:00, 543369.22it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 524658.51it/s]\u001b[A\n",
      " 34%|███▍      | 61/180 [35:22<52:18, 26.37s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 37%|███▋      | 40577/110629 [00:00<00:00, 405769.42it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 410950.18it/s][A\n",
      " 34%|███▍      | 62/180 [35:49<52:12, 26.55s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 46%|████▌     | 50362/110629 [00:00<00:00, 503609.67it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 479997.99it/s][A\n",
      " 35%|███▌      | 63/180 [36:16<52:14, 26.79s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 29%|██▉       | 31984/110629 [00:00<00:00, 319794.56it/s]\u001b[A\n",
      " 41%|████      | 45178/110629 [00:00<00:00, 224078.95it/s]\u001b[A\n",
      " 60%|█████▉    | 66248/110629 [00:00<00:00, 219889.45it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 268270.04it/s]\u001b[A\n",
      " 36%|███▌      | 64/180 [36:49<55:18, 28.61s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 39%|███▉      | 43345/110629 [00:00<00:00, 433423.55it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 409282.05it/s][A\n",
      " 36%|███▌      | 65/180 [37:28<1:01:05, 31.88s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 35%|███▍      | 38226/110629 [00:00<00:00, 382252.16it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 382165.55it/s][A\n",
      " 37%|███▋      | 66/180 [37:58<59:27, 31.29s/it]  \n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 42%|████▏     | 46194/110629 [00:00<00:00, 461939.34it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 468276.72it/s][A\n",
      " 37%|███▋      | 67/180 [38:28<58:14, 30.93s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 41%|████      | 45609/110629 [00:00<00:00, 456085.00it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 463022.64it/s][A\n",
      " 38%|███▊      | 68/180 [38:55<55:20, 29.65s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 43%|████▎     | 47214/110629 [00:00<00:00, 472134.82it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 483600.88it/s][A\n",
      " 38%|███▊      | 69/180 [39:22<53:11, 28.75s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 45%|████▍     | 49373/110629 [00:00<00:00, 493719.88it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 486253.37it/s][A\n",
      " 39%|███▉      | 70/180 [39:54<54:52, 29.93s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 31%|███       | 34185/110629 [00:00<00:00, 341839.73it/s]\u001b[A\n",
      " 59%|█████▊    | 64890/110629 [00:00<00:00, 330597.00it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 304611.10it/s][A\n",
      " 39%|███▉      | 71/180 [40:26<55:23, 30.49s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 43%|████▎     | 48117/110629 [00:00<00:00, 481169.31it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 438181.31it/s][A\n",
      " 40%|████      | 72/180 [41:01<57:19, 31.85s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 44%|████▎     | 48229/110629 [00:00<00:00, 482284.71it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 477225.06it/s][A\n",
      " 41%|████      | 73/180 [41:30<55:16, 30.99s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 44%|████▍     | 48948/110629 [00:00<00:00, 489475.80it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 481123.29it/s][A\n",
      " 41%|████      | 74/180 [41:59<53:53, 30.51s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 44%|████▍     | 48502/110629 [00:00<00:00, 485014.68it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 486189.69it/s][A\n",
      " 42%|████▏     | 75/180 [42:28<52:37, 30.07s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 45%|████▌     | 50117/110629 [00:00<00:00, 501169.28it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 488684.87it/s][A\n",
      " 42%|████▏     | 76/180 [42:55<50:25, 29.09s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 43%|████▎     | 47324/110629 [00:00<00:00, 473230.30it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 487915.62it/s][A\n",
      " 43%|████▎     | 77/180 [43:23<49:26, 28.80s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 45%|████▌     | 50299/110629 [00:00<00:00, 502979.69it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 496222.98it/s][A\n",
      " 43%|████▎     | 78/180 [43:52<49:00, 28.83s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 46%|████▋     | 51321/110629 [00:00<00:00, 513205.60it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 486429.23it/s][A\n",
      " 44%|████▍     | 79/180 [44:22<48:57, 29.08s/it]\n",
      "  0%|          | 0/110629 [00:00<?, ?it/s]\u001b[A\n",
      " 36%|███▌      | 40068/110629 [00:00<00:00, 400675.61it/s]\u001b[A\n",
      "100%|██████████| 110629/110629 [00:00<00:00, 438779.22it/s][A\n",
      " 44%|████▍     | 80/180 [44:52<49:05, 29.45s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 16%|█▌        | 55126/344272 [00:00<00:00, 551253.95it/s]\u001b[A\n",
      " 31%|███       | 106268/344272 [00:00<00:00, 538661.98it/s]\u001b[A\n",
      " 46%|████▌     | 157077/344272 [00:00<00:00, 529109.52it/s]\u001b[A\n",
      " 56%|█████▌    | 191163/344272 [00:00<00:00, 416674.46it/s]\u001b[A\n",
      " 70%|███████   | 241713/344272 [00:00<00:00, 439860.73it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 481376.24it/s]\u001b[A\n",
      " 45%|████▌     | 81/180 [46:17<1:15:42, 45.88s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█▎        | 45442/344272 [00:00<00:00, 454415.02it/s]\u001b[A\n",
      " 27%|██▋       | 92576/344272 [00:00<00:00, 459360.74it/s]\u001b[A\n",
      " 42%|████▏     | 146143/344272 [00:00<00:00, 479866.42it/s]\u001b[A\n",
      " 58%|█████▊    | 200447/344272 [00:00<00:00, 497218.18it/s]\u001b[A\n",
      " 74%|███████▍  | 256450/344272 [00:00<00:00, 514527.89it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 510552.17it/s]\u001b[A\n",
      " 46%|████▌     | 82/180 [47:41<1:33:55, 57.50s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█▎        | 46461/344272 [00:00<00:00, 464604.90it/s]\u001b[A\n",
      " 28%|██▊       | 95279/344272 [00:00<00:00, 471434.64it/s]\u001b[A\n",
      " 42%|████▏     | 145601/344272 [00:00<00:00, 480537.67it/s]\u001b[A\n",
      " 56%|█████▌    | 193492/344272 [00:00<00:00, 480043.87it/s]\u001b[A\n",
      " 70%|███████   | 242535/344272 [00:00<00:00, 483110.30it/s]\u001b[A\n",
      " 85%|████████▍ | 291603/344272 [00:00<00:00, 485354.99it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 489884.94it/s]\u001b[A\n",
      " 46%|████▌     | 83/180 [49:25<1:55:24, 71.39s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▌        | 52912/344272 [00:00<00:00, 529119.24it/s]\u001b[A\n",
      " 30%|███       | 104785/344272 [00:00<00:00, 525958.04it/s]\u001b[A\n",
      " 45%|████▍     | 153541/344272 [00:00<00:00, 513818.00it/s]\u001b[A\n",
      " 59%|█████▉    | 202977/344272 [00:00<00:00, 507820.30it/s]\u001b[A\n",
      " 73%|███████▎  | 251626/344272 [00:00<00:00, 501225.66it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 500752.15it/s]\u001b[A\n",
      " 47%|████▋     | 84/180 [51:05<2:07:57, 79.97s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█▎        | 43722/344272 [00:00<00:00, 437211.04it/s]\u001b[A\n",
      " 26%|██▌       | 88409/344272 [00:00<00:00, 440063.17it/s]\u001b[A\n",
      " 40%|████      | 137749/344272 [00:00<00:00, 454811.75it/s]\u001b[A\n",
      " 52%|█████▏    | 179122/344272 [00:00<00:00, 441653.81it/s]\u001b[A\n",
      " 62%|██████▏   | 212963/344272 [00:00<00:00, 369485.14it/s]\u001b[A\n",
      " 71%|███████   | 244677/344272 [00:00<00:00, 344913.70it/s]\u001b[A\n",
      " 85%|████████▍ | 292446/344272 [00:00<00:00, 376290.55it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 410042.37it/s]\u001b[A\n",
      " 47%|████▋     | 85/180 [52:40<2:13:37, 84.40s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 41534/344272 [00:00<00:00, 415331.48it/s]\u001b[A\n",
      " 21%|██▏       | 73633/344272 [00:00<00:00, 381675.89it/s]\u001b[A\n",
      " 35%|███▍      | 119547/344272 [00:00<00:00, 402023.82it/s]\u001b[A\n",
      " 48%|████▊     | 164152/344272 [00:00<00:00, 414290.29it/s]\u001b[A\n",
      " 62%|██████▏   | 214671/344272 [00:00<00:00, 437928.53it/s]\u001b[A\n",
      " 77%|███████▋  | 264080/344272 [00:00<00:00, 453388.90it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 451487.01it/s]\u001b[A\n",
      " 48%|████▊     | 86/180 [54:05<2:12:43, 84.72s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▌        | 51663/344272 [00:00<00:00, 516629.26it/s]\u001b[A\n",
      " 30%|███       | 103336/344272 [00:00<00:00, 516658.15it/s]\u001b[A\n",
      " 45%|████▍     | 154529/344272 [00:00<00:00, 515229.22it/s]\u001b[A\n",
      " 58%|█████▊    | 200893/344272 [00:00<00:00, 498581.09it/s]\u001b[A\n",
      " 72%|███████▏  | 246411/344272 [00:00<00:00, 484714.20it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 492739.99it/s]\u001b[A\n",
      " 48%|████▊     | 87/180 [55:39<2:15:24, 87.35s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▌        | 52517/344272 [00:00<00:00, 525165.49it/s]\u001b[A\n",
      " 29%|██▉       | 101299/344272 [00:00<00:00, 513373.15it/s]\u001b[A\n",
      " 44%|████▎     | 150208/344272 [00:00<00:00, 505838.51it/s]\u001b[A\n",
      " 58%|█████▊    | 199994/344272 [00:00<00:00, 503416.93it/s]\u001b[A\n",
      " 72%|███████▏  | 249535/344272 [00:00<00:00, 500986.14it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 500079.28it/s]\u001b[A\n",
      " 49%|████▉     | 88/180 [57:26<2:23:16, 93.44s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▍        | 50245/344272 [00:00<00:00, 502449.28it/s]\u001b[A\n",
      " 30%|██▉       | 102696/344272 [00:00<00:00, 508867.12it/s]\u001b[A\n",
      " 45%|████▍     | 153658/344272 [00:00<00:00, 509091.08it/s]\u001b[A\n",
      " 60%|█████▉    | 205962/344272 [00:00<00:00, 513195.71it/s]\u001b[A\n",
      " 74%|███████▍  | 256316/344272 [00:00<00:00, 510257.15it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 506862.89it/s]\u001b[A\n",
      " 49%|████▉     | 89/180 [58:58<2:20:53, 92.89s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 47916/344272 [00:00<00:00, 479159.31it/s]\u001b[A\n",
      " 28%|██▊       | 95584/344272 [00:00<00:00, 478411.58it/s]\u001b[A\n",
      " 38%|███▊      | 130373/344272 [00:00<00:00, 430011.71it/s]\u001b[A\n",
      " 52%|█████▏    | 179541/344272 [00:00<00:00, 446821.91it/s]\u001b[A\n",
      " 65%|██████▌   | 224375/344272 [00:00<00:00, 447273.83it/s]\u001b[A\n",
      " 78%|███████▊  | 269458/344272 [00:00<00:00, 448332.04it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 443346.12it/s]\u001b[A\n",
      " 50%|█████     | 90/180 [1:00:22<2:15:16, 90.19s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 47978/344272 [00:00<00:00, 479775.88it/s]\u001b[A\n",
      " 29%|██▊       | 98685/344272 [00:00<00:00, 487648.99it/s]\u001b[A\n",
      " 43%|████▎     | 148756/344272 [00:00<00:00, 491492.21it/s]\u001b[A\n",
      " 58%|█████▊    | 198175/344272 [00:00<00:00, 492296.84it/s]\u001b[A\n",
      " 71%|███████   | 244545/344272 [00:00<00:00, 483328.72it/s]\u001b[A\n",
      " 84%|████████▎ | 287699/344272 [00:00<00:00, 466530.63it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 474217.52it/s]\u001b[A\n",
      " 51%|█████     | 91/180 [1:01:51<2:13:27, 89.97s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 49539/344272 [00:00<00:00, 495389.29it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 96131/344272 [00:00<00:00, 486161.25it/s]\u001b[A\n",
      " 41%|████      | 141802/344272 [00:00<00:00, 476932.98it/s]\u001b[A\n",
      " 55%|█████▌    | 190983/344272 [00:00<00:00, 481297.82it/s]\u001b[A\n",
      " 70%|██████▉   | 240188/344272 [00:00<00:00, 484472.24it/s]\u001b[A\n",
      " 84%|████████▍ | 289634/344272 [00:00<00:00, 487423.67it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 481800.43it/s]\u001b[A\n",
      " 51%|█████     | 92/180 [1:03:27<2:14:43, 91.86s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 42256/344272 [00:00<00:00, 422555.37it/s]\u001b[A\n",
      " 26%|██▌       | 89463/344272 [00:00<00:00, 436281.21it/s]\u001b[A\n",
      " 40%|████      | 138423/344272 [00:00<00:00, 451013.73it/s]\u001b[A\n",
      " 54%|█████▍    | 187205/344272 [00:00<00:00, 461457.49it/s]\u001b[A\n",
      " 67%|██████▋   | 231693/344272 [00:00<00:00, 456350.73it/s]\u001b[A\n",
      " 79%|███████▊  | 270382/344272 [00:00<00:00, 382544.42it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 385089.54it/s]\u001b[A\n",
      " 52%|█████▏    | 93/180 [1:05:02<2:14:19, 92.64s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 30450/344272 [00:00<00:01, 304498.11it/s]\u001b[A\n",
      " 22%|██▏       | 76594/344272 [00:00<00:00, 339097.49it/s]\u001b[A\n",
      " 36%|███▌      | 124399/344272 [00:00<00:00, 371490.43it/s]\u001b[A\n",
      " 48%|████▊     | 164354/344272 [00:00<00:00, 379483.36it/s]\u001b[A\n",
      " 57%|█████▋    | 196296/344272 [00:00<00:00, 350396.84it/s]\u001b[A\n",
      " 69%|██████▉   | 237978/344272 [00:00<00:00, 367986.40it/s]\u001b[A\n",
      " 81%|████████  | 278041/344272 [00:00<00:00, 377205.76it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 399381.95it/s]\u001b[A\n",
      " 52%|█████▏    | 94/180 [1:06:38<2:14:24, 93.77s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 47943/344272 [00:00<00:00, 479420.17it/s]\u001b[A\n",
      " 27%|██▋       | 93593/344272 [00:00<00:00, 472304.43it/s]\u001b[A\n",
      " 41%|████▏     | 142289/344272 [00:00<00:00, 476606.43it/s]\u001b[A\n",
      " 55%|█████▌    | 190654/344272 [00:00<00:00, 478694.95it/s]\u001b[A\n",
      " 69%|██████▉   | 238781/344272 [00:00<00:00, 479464.36it/s]\u001b[A\n",
      " 83%|████████▎ | 285939/344272 [00:00<00:00, 477068.89it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 473257.32it/s]\u001b[A\n",
      " 53%|█████▎    | 95/180 [1:08:21<2:16:36, 96.43s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 49803/344272 [00:00<00:00, 498024.54it/s]\u001b[A\n",
      " 28%|██▊       | 97760/344272 [00:00<00:00, 492336.19it/s]\u001b[A\n",
      " 43%|████▎     | 147279/344272 [00:00<00:00, 493004.63it/s]\u001b[A\n",
      " 56%|█████▌    | 192031/344272 [00:00<00:00, 478415.47it/s]\u001b[A\n",
      " 70%|███████   | 242658/344272 [00:00<00:00, 486443.06it/s]\u001b[A\n",
      " 85%|████████▌ | 292751/344272 [00:00<00:00, 490699.15it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 489025.38it/s]\u001b[A\n",
      " 53%|█████▎    | 96/180 [1:10:04<2:17:38, 98.32s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|█▍        | 50382/344272 [00:00<00:00, 503814.47it/s]\u001b[A\n",
      " 29%|██▉       | 100276/344272 [00:00<00:00, 502340.50it/s]\u001b[A\n",
      " 44%|████▍     | 151537/344272 [00:00<00:00, 505376.60it/s]\u001b[A\n",
      " 58%|█████▊    | 198264/344272 [00:00<00:00, 493305.88it/s]\u001b[A\n",
      " 72%|███████▏  | 247697/344272 [00:00<00:00, 493611.05it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 503265.67it/s]\u001b[A\n",
      " 54%|█████▍    | 97/180 [1:11:33<2:12:27, 95.75s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 48994/344272 [00:00<00:00, 489934.63it/s]\u001b[A\n",
      " 28%|██▊       | 96674/344272 [00:00<00:00, 485917.25it/s]\u001b[A\n",
      " 40%|███▉      | 136017/344272 [00:00<00:00, 453901.35it/s]\u001b[A\n",
      " 52%|█████▏    | 179313/344272 [00:00<00:00, 447409.08it/s]\u001b[A\n",
      " 62%|██████▏   | 214987/344272 [00:00<00:00, 410014.45it/s]\u001b[A\n",
      " 72%|███████▏  | 248591/344272 [00:00<00:00, 297807.59it/s]\u001b[A\n",
      " 81%|████████  | 278362/344272 [00:00<00:00, 297777.33it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 364407.67it/s]\u001b[A\n",
      " 54%|█████▍    | 98/180 [1:13:07<2:10:04, 95.17s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 36137/344272 [00:00<00:00, 361363.45it/s]\u001b[A\n",
      " 22%|██▏       | 77072/344272 [00:00<00:00, 374534.90it/s]\u001b[A\n",
      " 37%|███▋      | 127174/344272 [00:00<00:00, 405224.42it/s]\u001b[A\n",
      " 52%|█████▏    | 178723/344272 [00:00<00:00, 433011.18it/s]\u001b[A\n",
      " 67%|██████▋   | 232078/344272 [00:00<00:00, 458953.59it/s]\u001b[A\n",
      " 80%|████████  | 275549/344272 [00:00<00:00, 451399.71it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 429673.51it/s]\u001b[A\n",
      " 55%|█████▌    | 99/180 [1:14:40<2:07:40, 94.58s/it]\n",
      "  0%|          | 0/344272 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▍        | 48908/344272 [00:00<00:00, 489075.80it/s]\u001b[A\n",
      " 26%|██▌       | 88381/344272 [00:00<00:00, 456353.19it/s]\u001b[A\n",
      " 38%|███▊      | 130749/344272 [00:00<00:00, 446032.85it/s]\u001b[A\n",
      " 49%|████▊     | 167304/344272 [00:00<00:00, 418394.44it/s]\u001b[A\n",
      " 58%|█████▊    | 198003/344272 [00:00<00:00, 360202.21it/s]\u001b[A\n",
      " 66%|██████▌   | 227651/344272 [00:00<00:00, 324013.32it/s]\u001b[A\n",
      " 79%|███████▉  | 271415/344272 [00:00<00:00, 351381.17it/s]\u001b[A\n",
      "100%|██████████| 344272/344272 [00:00<00:00, 390949.94it/s]\u001b[A\n",
      " 56%|█████▌    | 100/180 [1:16:25<2:09:55, 97.44s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 536369.06it/s]\u001b[A\n",
      " 56%|█████▌    | 101/180 [1:16:44<1:37:34, 74.11s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 491540.76it/s]\u001b[A\n",
      " 57%|█████▋    | 102/180 [1:17:07<1:16:28, 58.83s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 362286.57it/s]\u001b[A\n",
      " 57%|█████▋    | 103/180 [1:17:29<1:01:09, 47.66s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      " 46%|████▌     | 28079/61596 [00:00<00:00, 280786.92it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 297004.18it/s]\u001b[A\n",
      " 58%|█████▊    | 104/180 [1:17:49<49:49, 39.33s/it]  \n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|██████▋   | 41096/61596 [00:00<00:00, 410940.80it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 256366.03it/s]\u001b[A\n",
      " 58%|█████▊    | 105/180 [1:18:10<42:19, 33.86s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 505788.77it/s]\u001b[A\n",
      " 59%|█████▉    | 106/180 [1:18:29<36:23, 29.50s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 487982.07it/s]\u001b[A\n",
      " 59%|█████▉    | 107/180 [1:18:45<30:49, 25.34s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 460951.67it/s]\u001b[A\n",
      " 60%|██████    | 108/180 [1:19:01<27:12, 22.68s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 348403.04it/s]\u001b[A\n",
      " 61%|██████    | 109/180 [1:19:25<26:59, 22.81s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 484385.53it/s]\u001b[A\n",
      " 61%|██████    | 110/180 [1:19:47<26:20, 22.57s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 459013.17it/s]\u001b[A\n",
      " 62%|██████▏   | 111/180 [1:20:09<25:50, 22.47s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 461179.60it/s]\u001b[A\n",
      " 62%|██████▏   | 112/180 [1:20:29<24:33, 21.67s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 455329.07it/s]\u001b[A\n",
      " 63%|██████▎   | 113/180 [1:20:53<25:15, 22.61s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 493455.07it/s]\u001b[A\n",
      " 63%|██████▎   | 114/180 [1:21:12<23:37, 21.47s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 474628.85it/s]\u001b[A\n",
      " 64%|██████▍   | 115/180 [1:21:32<22:44, 21.00s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 484432.76it/s]\u001b[A\n",
      " 64%|██████▍   | 116/180 [1:21:54<22:39, 21.24s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 452831.86it/s]\u001b[A\n",
      " 65%|██████▌   | 117/180 [1:22:10<20:44, 19.75s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 554993.71it/s]\u001b[A\n",
      " 66%|██████▌   | 118/180 [1:22:31<20:36, 19.95s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 469639.32it/s]\u001b[A\n",
      " 66%|██████▌   | 119/180 [1:22:48<19:31, 19.20s/it]\n",
      "  0%|          | 0/61596 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 61596/61596 [00:00<00:00, 465001.21it/s]\u001b[A\n",
      " 67%|██████▋   | 120/180 [1:23:05<18:28, 18.48s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41808/41808 [00:00<00:00, 563548.56it/s]\n",
      " 67%|██████▋   | 121/180 [1:23:23<18:01, 18.34s/it]\n",
      "  0%|          | 0/41808 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 286546.79it/s]\u001b[A\n",
      " 68%|██████▊   | 122/180 [1:23:38<16:49, 17.41s/it]\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 465059.84it/s]\n",
      " 68%|██████▊   | 123/180 [1:23:54<15:58, 16.82s/it]\n",
      "  0%|          | 0/41808 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 366865.48it/s]\u001b[A\n",
      " 69%|██████▉   | 124/180 [1:24:05<14:13, 15.24s/it]\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 453925.39it/s]\n",
      " 69%|██████▉   | 125/180 [1:24:22<14:26, 15.76s/it]\n",
      "  0%|          | 0/41808 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 332684.10it/s]\u001b[A\n",
      " 70%|███████   | 126/180 [1:24:38<14:06, 15.68s/it]\n",
      "  0%|          | 0/41808 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 382506.38it/s]\u001b[A\n",
      " 71%|███████   | 127/180 [1:24:51<13:21, 15.13s/it]\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 466747.04it/s]\n",
      " 71%|███████   | 128/180 [1:25:08<13:30, 15.59s/it]\n",
      "  0%|          | 0/41808 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 401132.47it/s]\u001b[A\n",
      " 72%|███████▏  | 129/180 [1:25:21<12:33, 14.77s/it]\n",
      "  0%|          | 0/41808 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 402860.40it/s]\u001b[A\n",
      " 72%|███████▏  | 130/180 [1:25:36<12:22, 14.85s/it]\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 467630.78it/s]\n",
      " 73%|███████▎  | 131/180 [1:25:52<12:25, 15.21s/it]\n",
      "  0%|          | 0/41808 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 397478.21it/s]\u001b[A\n",
      " 73%|███████▎  | 132/180 [1:26:06<11:45, 14.70s/it]\n",
      "  0%|          | 0/41808 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|███▉      | 16581/41808 [00:00<00:00, 165808.18it/s]\u001b[A\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 185929.06it/s]\u001b[A\n",
      " 74%|███████▍  | 133/180 [1:26:22<12:02, 15.36s/it]\n",
      "  0%|          | 0/41808 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 364079.94it/s]\u001b[A\n",
      " 74%|███████▍  | 134/180 [1:26:37<11:33, 15.07s/it]\n",
      "  0%|          | 0/41808 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 336984.21it/s]\u001b[A\n",
      " 75%|███████▌  | 135/180 [1:26:54<11:52, 15.83s/it]\n",
      "  0%|          | 0/41808 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 370015.36it/s]\u001b[A\n",
      " 76%|███████▌  | 136/180 [1:27:06<10:44, 14.66s/it]\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 429302.47it/s]\n",
      " 76%|███████▌  | 137/180 [1:27:21<10:34, 14.76s/it]\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 471075.40it/s]\n",
      " 77%|███████▋  | 138/180 [1:27:33<09:42, 13.86s/it]\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 487443.78it/s]\n",
      " 77%|███████▋  | 139/180 [1:27:47<09:24, 13.76s/it]\n",
      "100%|██████████| 41808/41808 [00:00<00:00, 476244.11it/s]\n",
      " 78%|███████▊  | 140/180 [1:27:58<08:46, 13.16s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 26%|██▌       | 53079/207748 [00:00<00:00, 530785.44it/s]\u001b[A\n",
      " 50%|█████     | 104792/207748 [00:00<00:00, 526603.86it/s]\u001b[A\n",
      " 73%|███████▎  | 151429/207748 [00:00<00:00, 506959.53it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 499156.63it/s]\u001b[A\n",
      " 78%|███████▊  | 141/180 [1:28:56<17:16, 26.57s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▍       | 51515/207748 [00:00<00:00, 515144.35it/s]\u001b[A\n",
      " 49%|████▊     | 101215/207748 [00:00<00:00, 509562.10it/s]\u001b[A\n",
      " 73%|███████▎  | 150657/207748 [00:00<00:00, 504922.75it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 438654.31it/s]\u001b[A\n",
      " 79%|███████▉  | 142/180 [1:29:58<23:31, 37.16s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 24%|██▍       | 50234/207748 [00:00<00:00, 502339.28it/s]\u001b[A\n",
      " 49%|████▉     | 102384/207748 [00:00<00:00, 507934.97it/s]\u001b[A\n",
      " 74%|███████▍  | 153754/207748 [00:00<00:00, 509646.65it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 512222.00it/s]\u001b[A\n",
      " 79%|███████▉  | 143/180 [1:30:56<26:39, 43.23s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 23%|██▎       | 47544/207748 [00:00<00:00, 475439.32it/s]\u001b[A\n",
      " 45%|████▍     | 93274/207748 [00:00<00:00, 469846.96it/s]\u001b[A\n",
      " 69%|██████▊   | 142313/207748 [00:00<00:00, 475826.65it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 488690.03it/s]\u001b[A\n",
      " 80%|████████  | 144/180 [1:31:54<28:38, 47.72s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 22%|██▏       | 44704/207748 [00:00<00:00, 447035.10it/s]\u001b[A\n",
      " 44%|████▍     | 91558/207748 [00:00<00:00, 453274.96it/s]\u001b[A\n",
      " 67%|██████▋   | 138780/207748 [00:00<00:00, 458796.08it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 467317.71it/s]\u001b[A\n",
      " 81%|████████  | 145/180 [1:32:47<28:43, 49.25s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 23%|██▎       | 47439/207748 [00:00<00:00, 474380.27it/s]\u001b[A\n",
      " 46%|████▋     | 96421/207748 [00:00<00:00, 478907.81it/s]\u001b[A\n",
      " 70%|███████   | 145856/207748 [00:00<00:00, 483438.01it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 486716.47it/s]\u001b[A\n",
      " 81%|████████  | 146/180 [1:33:50<30:21, 53.56s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 24%|██▍       | 49372/207748 [00:00<00:00, 493118.52it/s]\u001b[A\n",
      " 46%|████▋     | 96084/207748 [00:00<00:00, 485016.99it/s]\u001b[A\n",
      " 68%|██████▊   | 141984/207748 [00:00<00:00, 476904.33it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 473166.46it/s]\u001b[A\n",
      " 82%|████████▏ | 147/180 [1:34:46<29:48, 54.20s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 24%|██▎       | 49217/207748 [00:00<00:00, 492159.91it/s]\u001b[A\n",
      " 47%|████▋     | 98359/207748 [00:00<00:00, 491934.67it/s]\u001b[A\n",
      " 72%|███████▏  | 149562/207748 [00:00<00:00, 497792.70it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 501824.93it/s]\u001b[A\n",
      " 82%|████████▏ | 148/180 [1:35:43<29:18, 54.94s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 52637/207748 [00:00<00:00, 526369.25it/s]\u001b[A\n",
      " 51%|█████▏    | 106977/207748 [00:00<00:00, 531363.61it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 538825.40it/s]\u001b[A\n",
      " 83%|████████▎ | 149/180 [1:36:35<27:55, 54.06s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 22%|██▏       | 44982/207748 [00:00<00:00, 449810.78it/s]\u001b[A\n",
      " 44%|████▍     | 92248/207748 [00:00<00:00, 456427.77it/s]\u001b[A\n",
      " 67%|██████▋   | 139738/207748 [00:00<00:00, 461814.01it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 463648.74it/s]\u001b[A\n",
      " 83%|████████▎ | 150/180 [1:37:30<27:16, 54.56s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 23%|██▎       | 47977/207748 [00:00<00:00, 479765.88it/s]\u001b[A\n",
      " 44%|████▍     | 91841/207748 [00:00<00:00, 466634.64it/s]\u001b[A\n",
      " 67%|██████▋   | 139394/207748 [00:00<00:00, 469267.91it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 463092.84it/s]\u001b[A\n",
      " 84%|████████▍ | 151/180 [1:38:29<26:59, 55.86s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 19%|█▉        | 39951/207748 [00:00<00:00, 399509.43it/s]\u001b[A\n",
      " 39%|███▊      | 80116/207748 [00:00<00:00, 400146.75it/s]\u001b[A\n",
      " 60%|██████    | 125592/207748 [00:00<00:00, 415100.69it/s]\u001b[A\n",
      " 83%|████████▎ | 172185/207748 [00:00<00:00, 429139.79it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 376890.93it/s]\u001b[A\n",
      " 84%|████████▍ | 152/180 [1:39:27<26:16, 56.32s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▍       | 50949/207748 [00:00<00:00, 509485.63it/s]\u001b[A\n",
      " 48%|████▊     | 100492/207748 [00:00<00:00, 505184.58it/s]\u001b[A\n",
      " 72%|███████▏  | 148685/207748 [00:00<00:00, 497974.23it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 492959.00it/s]\u001b[A\n",
      " 85%|████████▌ | 153/180 [1:40:18<24:41, 54.86s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 23%|██▎       | 47795/207748 [00:00<00:00, 477911.72it/s]\u001b[A\n",
      " 45%|████▍     | 92956/207748 [00:00<00:00, 469703.44it/s]\u001b[A\n",
      " 68%|██████▊   | 140482/207748 [00:00<00:00, 471355.51it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 460937.10it/s]\u001b[A\n",
      " 86%|████████▌ | 154/180 [1:41:17<24:20, 56.19s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 23%|██▎       | 46908/207748 [00:00<00:00, 469070.38it/s]\u001b[A\n",
      " 46%|████▌     | 95119/207748 [00:00<00:00, 472906.07it/s]\u001b[A\n",
      " 70%|██████▉   | 144540/207748 [00:00<00:00, 479100.34it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 480168.95it/s]\u001b[A\n",
      " 86%|████████▌ | 155/180 [1:42:15<23:32, 56.49s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 24%|██▍       | 50026/207748 [00:00<00:00, 500259.28it/s]\u001b[A\n",
      " 48%|████▊     | 100294/207748 [00:00<00:00, 500978.55it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 150468/207748 [00:00<00:00, 501205.09it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 497683.25it/s]\u001b[A\n",
      " 87%|████████▋ | 156/180 [1:43:11<22:33, 56.40s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 24%|██▍       | 49879/207748 [00:00<00:00, 498789.29it/s]\u001b[A\n",
      " 47%|████▋     | 97249/207748 [00:00<00:00, 490984.68it/s]\u001b[A\n",
      " 69%|██████▉   | 143455/207748 [00:00<00:00, 481932.75it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 482003.80it/s]\u001b[A\n",
      " 87%|████████▋ | 157/180 [1:44:14<22:23, 58.43s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 22%|██▏       | 46723/207748 [00:00<00:00, 467224.88it/s]\u001b[A\n",
      " 48%|████▊     | 99155/207748 [00:00<00:00, 483001.27it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 521273.87it/s]\u001b[A\n",
      " 88%|████████▊ | 158/180 [1:45:09<21:01, 57.32s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 52722/207748 [00:00<00:00, 527219.25it/s]\u001b[A\n",
      " 50%|█████     | 104784/207748 [00:00<00:00, 525220.61it/s]\u001b[A\n",
      " 75%|███████▍  | 155002/207748 [00:00<00:00, 518088.07it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 509579.95it/s]\u001b[A\n",
      " 88%|████████▊ | 159/180 [1:46:07<20:11, 57.69s/it]\n",
      "  0%|          | 0/207748 [00:00<?, ?it/s]\u001b[A\n",
      " 19%|█▉        | 39349/207748 [00:00<00:00, 393486.62it/s]\u001b[A\n",
      " 34%|███▍      | 70407/207748 [00:00<00:00, 364310.27it/s]\u001b[A\n",
      " 42%|████▏     | 87988/207748 [00:00<00:00, 255624.32it/s]\u001b[A\n",
      " 51%|█████     | 105344/207748 [00:00<00:00, 223865.97it/s]\u001b[A\n",
      " 69%|██████▉   | 143965/207748 [00:00<00:00, 256112.93it/s]\u001b[A\n",
      " 85%|████████▌ | 176637/207748 [00:00<00:00, 273867.14it/s]\u001b[A\n",
      "100%|██████████| 207748/207748 [00:00<00:00, 273286.45it/s]\u001b[A\n",
      " 89%|████████▉ | 160/180 [1:47:15<20:14, 60.72s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 30%|███       | 50241/166593 [00:00<00:00, 502406.89it/s]\u001b[A\n",
      " 60%|██████    | 100436/166593 [00:00<00:00, 502266.64it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 488425.27it/s]\u001b[A\n",
      " 89%|████████▉ | 161/180 [1:48:10<18:41, 59.02s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 27%|██▋       | 44607/166593 [00:00<00:00, 446065.11it/s]\u001b[A\n",
      " 53%|█████▎    | 88466/166593 [00:00<00:00, 443793.19it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 438312.57it/s]\u001b[A\n",
      " 90%|█████████ | 162/180 [1:48:55<16:26, 54.81s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 26%|██▋       | 44121/166593 [00:00<00:00, 441200.95it/s]\u001b[A\n",
      " 54%|█████▍    | 90214/166593 [00:00<00:00, 446939.22it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 452338.46it/s]\u001b[A\n",
      " 91%|█████████ | 163/180 [1:49:39<14:37, 51.61s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 54784/166593 [00:00<00:00, 547828.77it/s]\u001b[A\n",
      " 64%|██████▍   | 106394/166593 [00:00<00:00, 537904.48it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 501601.69it/s]\u001b[A\n",
      " 91%|█████████ | 164/180 [1:50:27<13:28, 50.50s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 41865/166593 [00:00<00:00, 418641.42it/s]\u001b[A\n",
      " 48%|████▊     | 80568/166593 [00:00<00:00, 408626.10it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 420587.44it/s]\u001b[A\n",
      " 92%|█████████▏| 165/180 [1:51:14<12:23, 49.58s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 14077/166593 [00:00<00:01, 140754.36it/s]\u001b[A\n",
      " 17%|█▋        | 29129/166593 [00:00<00:00, 143547.12it/s]\u001b[A\n",
      " 33%|███▎      | 55783/166593 [00:00<00:00, 166611.29it/s]\u001b[A\n",
      " 46%|████▌     | 75830/166593 [00:00<00:00, 175433.85it/s]\u001b[A\n",
      " 62%|██████▏   | 104107/166593 [00:00<00:00, 197977.85it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 239731.46it/s]\u001b[A\n",
      " 92%|█████████▏| 166/180 [1:52:05<11:36, 49.72s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 28%|██▊       | 47332/166593 [00:00<00:00, 473310.30it/s]\u001b[A\n",
      " 57%|█████▋    | 94475/166593 [00:00<00:00, 472741.72it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 468139.46it/s]\u001b[A\n",
      " 93%|█████████▎| 167/180 [1:52:52<10:36, 48.95s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 29%|██▉       | 48295/166593 [00:00<00:00, 482940.10it/s]\u001b[A\n",
      " 56%|█████▌    | 93044/166593 [00:00<00:00, 471725.95it/s]\u001b[A\n",
      " 81%|████████  | 134525/166593 [00:00<00:00, 453071.56it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 412072.15it/s]\u001b[A\n",
      " 93%|█████████▎| 168/180 [1:53:44<09:59, 49.97s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 28%|██▊       | 46523/166593 [00:00<00:00, 465224.90it/s]\u001b[A\n",
      " 55%|█████▌    | 92096/166593 [00:00<00:00, 462333.60it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 449857.26it/s]\u001b[A\n",
      " 94%|█████████▍| 169/180 [1:54:30<08:56, 48.73s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 28%|██▊       | 46793/166593 [00:00<00:00, 467929.33it/s]\u001b[A\n",
      " 55%|█████▌    | 92429/166593 [00:00<00:00, 464396.19it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 459111.00it/s]\u001b[A\n",
      " 94%|█████████▍| 170/180 [1:55:20<08:10, 49.06s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▍       | 40858/166593 [00:00<00:00, 408575.52it/s]\u001b[A\n",
      " 51%|█████     | 84457/166593 [00:00<00:00, 416430.74it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 443167.74it/s]\u001b[A\n",
      " 95%|█████████▌| 171/180 [1:56:08<07:20, 48.90s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 33454/166593 [00:00<00:00, 334537.13it/s]\u001b[A\n",
      " 46%|████▌     | 75992/166593 [00:00<00:00, 357436.96it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 421532.58it/s]\u001b[A\n",
      " 96%|█████████▌| 172/180 [1:57:00<06:38, 49.87s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 28%|██▊       | 47260/166593 [00:00<00:00, 472590.31it/s]\u001b[A\n",
      " 56%|█████▌    | 93256/166593 [00:00<00:00, 468725.02it/s]\u001b[A\n",
      " 75%|███████▌  | 125057/166593 [00:00<00:00, 410371.05it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 371878.61it/s]\u001b[A\n",
      " 96%|█████████▌| 173/180 [1:57:52<05:53, 50.48s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 29%|██▊       | 47856/166593 [00:00<00:00, 478550.19it/s]\u001b[A\n",
      " 57%|█████▋    | 94825/166593 [00:00<00:00, 475854.26it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 458688.76it/s]\u001b[A\n",
      " 97%|█████████▋| 174/180 [1:58:48<05:12, 52.02s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 27%|██▋       | 45133/166593 [00:00<00:00, 451320.75it/s]\u001b[A\n",
      " 55%|█████▌    | 92034/166593 [00:00<00:00, 456484.35it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 460691.39it/s]\u001b[A\n",
      " 97%|█████████▋| 175/180 [1:59:34<04:11, 50.29s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 42440/166593 [00:00<00:00, 424396.36it/s]\u001b[A\n",
      " 51%|█████▏    | 85670/166593 [00:00<00:00, 426736.76it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 436981.48it/s]\u001b[A\n",
      " 98%|█████████▊| 176/180 [2:00:22<03:18, 49.51s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 29%|██▉       | 47950/166593 [00:00<00:00, 479494.74it/s]\u001b[A\n",
      " 52%|█████▏    | 85800/166593 [00:00<00:00, 443955.26it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 435330.02it/s]\u001b[A\n",
      " 98%|█████████▊| 177/180 [2:01:11<02:27, 49.27s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 23%|██▎       | 38898/166593 [00:00<00:00, 388968.32it/s]\u001b[A\n",
      " 49%|████▉     | 81278/166593 [00:00<00:00, 398800.20it/s]\u001b[A\n",
      " 75%|███████▍  | 124581/166593 [00:00<00:00, 408484.76it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 411719.12it/s]\u001b[A\n",
      " 99%|█████████▉| 178/180 [2:01:53<01:34, 47.34s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 27%|██▋       | 44975/166593 [00:00<00:00, 449746.14it/s]\u001b[A\n",
      " 49%|████▉     | 81951/166593 [00:00<00:00, 422336.96it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 433795.20it/s]\u001b[A\n",
      " 99%|█████████▉| 179/180 [2:02:37<00:46, 46.10s/it]\n",
      "  0%|          | 0/166593 [00:00<?, ?it/s]\u001b[A\n",
      " 29%|██▉       | 48088/166593 [00:00<00:00, 480875.87it/s]\u001b[A\n",
      " 57%|█████▋    | 94913/166593 [00:00<00:00, 477014.20it/s]\u001b[A\n",
      "100%|██████████| 166593/166593 [00:00<00:00, 467556.19it/s]\u001b[A\n",
      "100%|██████████| 180/180 [2:03:20<00:00, 41.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# get all permutations of identifiers (e.g. ['entrez_id', 'ensembl_gene_id'])\n",
    "identifiers = ['ensembl_gene_id', 'transcript_stable_id', 'primary_symbol', 'protein_stable_id', 'uniprot_id',\n",
    "               'entrez_id', 'hgnc_id', 'pro_id', 'symbol', 'synonyms', 'ensembl_gene_type', 'transcript_name',\n",
    "               'ensembl_transcript_type', 'master_gene_type', 'master_transcript_type', 'hgnc_gene_type',\n",
    "               'name', 'map_location', 'chromosome', 'Other_designations', 'entrez_gene_type']\n",
    "\n",
    "# get list of data types that ignores subjects of a permutation pair that are metadata\n",
    "identifier_list = [x for x in list(itertools.permutations(identifiers, 2)) if x[0] not in identifiers[9:]]\n",
    "\n",
    "# create master dictionary of all identifiers\n",
    "master_dict = {}\n",
    "\n",
    "for ids in tqdm(identifier_list):\n",
    "    maps = {k: [ids[1] + '_' + x for x in set(g[ids[1]].tolist()) if x != 'None'] for k,g in merged_data.groupby(ids[0])}\n",
    "\n",
    "    for key in tqdm(maps.keys()):\n",
    "        if ids[0] + '_' + key in master_dict.keys():\n",
    "            master_dict[ids[0] + '_' + key] += maps[key]\n",
    "        else:\n",
    "            master_dict[ids[0] + '_' + key] = maps[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Condensing Master Mapping Dictionary_  \n",
    "Once we have all pairwise mapping data between all of the identifiers, the next step is to condense all information by identifier, into a format that can easily be used downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1330677/1330677 [11:15<00:00, 1970.40it/s] \n"
     ]
    }
   ],
   "source": [
    "reformatted_mapped_identifiers = {}\n",
    "\n",
    "# set globals\n",
    "gene_type_var = 'master_gene_type'\n",
    "transcript_type_var = 'master_transcript_type'\n",
    "\n",
    "for ident in tqdm(master_dict.keys()):\n",
    "    identifier_info = set()\n",
    "    \n",
    "    for x_id in master_dict[ident]:\n",
    "        # get all identifying information for all linked identifiers\n",
    "        if not any(x for x in identifiers[9:] if x_id.startswith(x)) and x_id in master_dict.keys():\n",
    "            identifier_info |= set(master_dict[x_id])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # clean gene and transcript types mapped to multiple types\n",
    "    gene_types = [x.split('_')[-1] for x in identifier_info if x.startswith(gene_type_var)]\n",
    "    trans_types = [x.split('_')[-1] for x in identifier_info if x.startswith(transcript_type_var)]\n",
    "    type_updates = []\n",
    "\n",
    "    for types in [gene_types if len(gene_types) > 0 else ['None'], trans_types if len(trans_types) > 0 else ['None']]:\n",
    "        if 'protein-coding' in set(types):\n",
    "            type_updates.append('protein-coding')\n",
    "        else:\n",
    "            type_updates.append('not protein-coding')\n",
    "\n",
    "    # update identifier set information\n",
    "    identifier_info = [x for x in identifier_info if not x.startswith(gene_type_var) and not x.startswith(transcript_type_var)]\n",
    "    identifier_info += ['gene_type_update_' + type_updates[0], 'transcript_type_update_' + type_updates[1]]\n",
    "    reformatted_mapped_identifiers[ident] = identifier_info\n",
    "\n",
    "# save a copy of the dictionary\n",
    "pickle.dump(reformatted_mapped_identifiers,\n",
    "            open(processed_data_location + 'Merged_gene_rna_protein_identifiers.pkl', 'wb'),\n",
    "            protocol=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformatted_mapped_identifiers_pkl = pickle.load(open(processed_data_location + 'Merged_gene_rna_protein_identifiers.pkl', 'rb'),\n",
    "#                                              encoding='bytes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembl Gene-Entrez Gene <a class=\"anchor\" id=\"ensemblgene-entrezgene\"></a>\n",
    "\n",
    "\n",
    "**Purpose:** To map Ensembl gene identifiers to Entrez gene identifiers when creating the following edges:   \n",
    "- gene-gene\n",
    "\n",
    "**Output:** [`ENSEMBL_GENE_ENTREZ_GENE_MAP.txt`](https://www.dropbox.com/s/crghjh2we5v7pws/ENSEMBL_GENE_ENTREZ_GENE_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENSEMBL_GENE_ENTREZ_GENE_MAP.txt',\n",
    "                  'ensembl_gene_id',\n",
    "                  'entrez_id',\n",
    "                  'gene_type_update')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egeg_data = pandas.read_csv(processed_data_location + 'ENSEMBL_GENE_ENTREZ_GENE_MAP.txt',\n",
    "                            header=None,\n",
    "                            names=['Ensembl_Gene_IDs', 'Entrez_Gene_IDs', 'Gene_Type'],\n",
    "                            delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} ensembl gene-entrez gene edges'.format(edge_count=len(egeg_data.drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egeg_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembl Transcript-Protein Ontology <a class=\"anchor\" id=\"ensembltranscript-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Ensembl transcript identifiers to Protein Ontology identifiers when creating the following edges: \n",
    "- rna-protein  \n",
    "\n",
    "**Output:** [`ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/ckrw11nfyu6a08c/ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt',\n",
    "                  'transcript_stable_id',\n",
    "                  'pro_id',\n",
    "                  'transcript_type_update')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etpr_data = pandas.read_csv(processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt',\n",
    "                            header=None,\n",
    "                            names=['Ensembl_Transcript_IDs', 'Protein_Ontology_IDs', 'Transcript_Type'],\n",
    "                            delimiter='\\t',\n",
    "                            low_memory=False)\n",
    "\n",
    "print('There are {edge_count} ensembl transcript-protein ontology edges'.format(edge_count=len(etpr_data.drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrez Gene-Ensembl Transcript <a class=\"anchor\" id=\"entrezgene-ensembltranscript\"></a>\n",
    "\n",
    "**Purpose:** To map entrez gene identifiers to Ensembl transcript identifiers when creating the following edges: \n",
    "- gene-rna \n",
    "\n",
    "**Output:** [`ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt`](https://www.dropbox.com/s/yqnofd8h90luygu/ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                  'entrez_id',\n",
    "                  'transcript_stable_id',\n",
    "                  'transcript_type_update')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eet_data = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                           header=None,\n",
    "                           names=['Entrez_Gene_IDs', 'Ensembl_Transcript_IDs', 'Gene_Type', 'Transcript_Type'],\n",
    "                           delimiter='\\t',\n",
    "                           low_memory=False)\n",
    "\n",
    "print('There are {edge_count} entrez gene identifiers-ensembl transcript edges'.format(edge_count=len(eet_data.drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eet_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrez Gene-Protein Ontology <a class=\"anchor\" id=\"entrezgene-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Protein Ontology identifiers to Ensembl transcript identifiers when creating the following edges:   \n",
    "- chemical-protein  \n",
    "- gene-protein\n",
    "\n",
    "**Output:** [`ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/ufbp5o6zgagriw7/ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt',\n",
    "                  'entrez_id',\n",
    "                  'pro_id',\n",
    "                  'gene_type_update')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egpr_data = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header=None,\n",
    "                            names=['Gene_IDs', 'Protein_Ontology_IDs', 'Gene_Type'],\n",
    "                            delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} entrez gene-protein ontology edges'.format(edge_count=len(egpr_data.drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gene Symbol-Ensembl Transcript <a class=\"anchor\" id=\"genesymbol-ensembltranscript\"></a>\n",
    "\n",
    "**Purpose:** To map gene symbols to Ensembl transcript identifiers when creating the following edges: \n",
    "- chemical-rna  \n",
    "- rna-anatomy  \n",
    "- rna-cell  \n",
    "\n",
    "**Output:** [`GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt`](https://www.dropbox.com/s/5o8yt7eejbf819x/GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                  'symbol',\n",
    "                  'transcript_stable_id',\n",
    "                  'transcript_type_update')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data = pandas.read_csv(processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                            header=None,\n",
    "                            names=['Gene_Symbols', 'Ensembl_Transcript_IDs', 'Gene_Type', 'Transcript_Type'],\n",
    "                            delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} gene symbol-ensembl transcript edges'.format(edge_count=len(set_data.drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STRING-Protein Ontology <a class=\"anchor\" id=\"string-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map STRING identifiers to Protein Ontology identifiers when creating the following edges:   \n",
    "- protein-protein  \n",
    "\n",
    "**Output:** [`STRING_PRO_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/mekh5lr3bxp7gvu/STRING_PRO_ONTOLOGY_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt',\n",
    "                  'protein_stable_id',\n",
    "                  'pro_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpr_data = pandas.read_csv(processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header=None,\n",
    "                            names=['STRING_IDs', 'Protein_Ontology_IDs', 'gene_type'],\n",
    "                            delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} string-protein ontology edges'.format(edge_count=len(stpr_data.drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniprot Accession-Protein Ontology <a class=\"anchor\" id=\"uniprotaccession-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Uniprot accession identifiers to Protein Ontology identifiers when creating the following edges:  \n",
    "- protein-gobp  \n",
    "- protein-gomf  \n",
    "- protein-gocc  \n",
    "- protein-cofactor  \n",
    "- protein-catalyst \n",
    "- protein-pathway\n",
    "\n",
    "**Output:** [`UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt`](https://www.dropbox.com/s/txp8tqdipzwus9p/UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt',\n",
    "                  'uniprot_id',\n",
    "                  'pro_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uapr_data = pandas.read_csv(processed_data_location + 'UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header=None,\n",
    "                            names=['Uniprot_Accession_IDs', 'Protein_Ontology_IDs', 'gene_type'],\n",
    "                            delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} uniprot accession-protein ontology edges'.format(edge_count=len(uapr_data.drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uapr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Other Identifier Mapping <a class=\"anchor\" id=\"other-identifier-mapping\"></a>\n",
    "***\n",
    "* [ChEBI Identifiers](#mesh-chebi)  \n",
    "* [Human Protein Atlas Tissue and Cell Types](#hpa-uberon) \n",
    "* [Human Disease and Phenotype Identifiers](#disease-identifiers) \n",
    "* [Reactome Pathways and the Pathway Ontology](#reactome-pw)  \n",
    "* [Genomic Identifiers and the Sequence Ontology](#genomic-so)  \n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### ChEBI-MeSH Identifiers <a class=\"anchor\" id=\"mesh-chebi\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [mapping-mesh-to-chebi](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#mapping-mesh-identifiers-to-chebi-identifiers)  \n",
    "\n",
    "**Purpose:** Map MeSH identifiers to ChEBI identifiers when creating the following edges:  \n",
    "- chemical-gene  \n",
    "- chemical-disease\n",
    "\n",
    "**Dependencies:** This script assumes that the [`ncbo_rest_api.py`](https://gist.github.com/callahantiff/a28fb3160782f42f104e9ec41553af0d) script was run and the data generated from this file was written to `./resources/processed_data/temp`. \n",
    "\n",
    "**Output:** [`MESH_CHEBI_MAP.txt`](https://www.dropbox.com/s/5nr87v5h6x8oc1b/MESH_CHEBI_MAP.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'MESH_CHEBI_MAP.txt', 'w') as out:\n",
    "    for filename in tqdm(glob.glob(processed_data_location + 'temp/*.txt')):\n",
    "        for row in list(filter(None, open(filename, 'r').read().split('\\n'))):\n",
    "            mesh = '_'.join(row.split('\\t')[0].split('/')[-2:])\n",
    "            chebi = row.split('\\t')[1].split('/')[-1]\n",
    "            out.write(mesh + '\\t' + chebi + '\\n')\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_data = pandas.read_csv(processed_data_location + 'MESH_CHEBI_MAP.txt',\n",
    "                          delimiter='\\t',\n",
    "                          header=None,\n",
    "                          names=['MeSH_IDs', 'ChEBI_IDs'])\n",
    "\n",
    "print('There are {edge_count} MeSH-ChEBI edges'.format(edge_count=len(mc_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disease and Phenotype Identifiers <a class=\"anchor\" id=\"disease-identifiers\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [disgenet](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#disgenet)  \n",
    "\n",
    "**Purpose:** This script downloads the [disease_mappings.tsv](https://www.disgenet.org/static/disgenet_ap1/files/downloads/disease_mappings.tsv.gz) to map UMLS identifiers to Human Disease and Human Phenotype identifiers when creating the following edges:  \n",
    "- chemical-disease  \n",
    "- disease-phenotype\n",
    "\n",
    "**Output:**   \n",
    "- Human Disease Ontology Mappings ➞ [`DISEASE_DOID_MAP.txt`](https://www.dropbox.com/s/q30ferujl7k574j/DISEASE_DOID_MAP.txt?dl=1)  \n",
    "- Human Phenotype Ontology Mappings ➞ [`PHENOTYPE_HPO_MAP.txt`](https://www.dropbox.com/s/5ayl0c5qm7r4tdm/PHENOTYPE_HPO_MAP.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.disgenet.org/static/disgenet_ap1/files/downloads/disease_mappings.tsv.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diseaseId</th>\n",
       "      <th>name</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>code</th>\n",
       "      <th>vocabularyName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C0018923</td>\n",
       "      <td>Hemangiosarcoma</td>\n",
       "      <td>DO</td>\n",
       "      <td>0001816</td>\n",
       "      <td>angiosarcoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C0854893</td>\n",
       "      <td>Angiosarcoma non-metastatic</td>\n",
       "      <td>DO</td>\n",
       "      <td>0001816</td>\n",
       "      <td>angiosarcoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0033999</td>\n",
       "      <td>Pterygium</td>\n",
       "      <td>DO</td>\n",
       "      <td>0002116</td>\n",
       "      <td>pterygium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  diseaseId                         name vocabulary     code vocabularyName\n",
       "0  C0018923              Hemangiosarcoma         DO  0001816   angiosarcoma\n",
       "1  C0854893  Angiosarcoma non-metastatic         DO  0001816   angiosarcoma\n",
       "2  C0033999                    Pterygium         DO  0002116      pterygium"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disease_data = pandas.read_csv(unprocessed_data_location + 'disease_mappings.tsv',\n",
    "                               header=0,\n",
    "                               delimiter='|')\n",
    "\n",
    "disease_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Build Disease Identifier Dictionary_  \n",
    "In order to improve efficiency when mapping different disease terminology identifiers to the [Human Disease Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-disease-ontology) and [Human Phenotype Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-phenotype-ontology), we create a dictionary of disease identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dictionary\n",
    "disease_dict = {}\n",
    "\n",
    "for idx, row in tqdm(disease_data.iterrows(), total=disease_data.shape[0]):\n",
    "    if row['vocabulary'] == 'MSH':\n",
    "        mesh_finder(disease_data, row['code'], 'MESH:', disease_dict)\n",
    "        print(row['code'])\n",
    "    elif row['vocabulary'] == 'OMIM':\n",
    "        mesh_finder(disease_data, row['code'], 'OMIM:', disease_dict)\n",
    "        print(row['code'])\n",
    "    elif row['vocabulary'] == 'ORDO':\n",
    "        mesh_finder(disease_data, row['code'], 'ORPHA:', disease_dict)\n",
    "        print(row['code'])\n",
    "    elif row['diseaseId'] in disease_dict.keys():\n",
    "        if row['vocabulary'] == 'DO':\n",
    "            disease_dict[row['diseaseId']].append('DOID_' + row['code']) \n",
    "        if row['vocabulary'] == 'HPO':\n",
    "            disease_dict[row['diseaseId']].append(row['code'].replace('HP:', 'HP_'))\n",
    "    else:\n",
    "        if row['vocabulary'] == 'DO':\n",
    "            disease_dict[row['diseaseId']] = ['DOID_' + row['code']] \n",
    "        if row['vocabulary'] == 'HPO':\n",
    "            disease_dict[row['diseaseId']] = [row['code'].replace('HP:', 'HP_')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write Mapping Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'DISEASE_DOID_MAP.txt', 'w') as outfile1,open(processed_data_location + 'PHENOTYPE_HPO_MAP.txt', 'w') as outfile2:\n",
    "    for key, value in tqdm(disease_dict.items()):\n",
    "        for i in value:\n",
    "            # get diseases\n",
    "            if i.startswith('DOID_'): \n",
    "                outfile1.write(key.split(':')[-1] + '\\t' + i + '\\n')\n",
    "\n",
    "            # get phenotypes\n",
    "            if i.startswith('HP_'): \n",
    "                outfile2.write(key.split(':')[-1] + '\\t' + i + '\\n')\n",
    "\n",
    "outfile1.close()\n",
    "outfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Human Disease Ontology Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_data = pandas.read_csv(processed_data_location + 'DISEASE_DOID_MAP.txt',\n",
    "                           header=None,\n",
    "                           names=['Disease_IDs', 'DOID_IDs'],\n",
    "                           delimiter='\\t')\n",
    "\n",
    "print('There are {} disease-DOID edges'.format(len(dis_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Human Phenotype Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_data = pandas.read_csv(processed_data_location + 'PHENOTYPE_HPO_MAP.txt',\n",
    "                          header=None,\n",
    "                          names=['Disease_IDs', 'HP_IDs'],\n",
    "                          delimiter='\\t')\n",
    "\n",
    "print('There are {} phenotype-HPO edges'.format(len(hp_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Protein Atlas/GTEx Tissue/Cells - UBERON + Cell Ontology + Cell Line Ontology <a class=\"anchor\" id=\"hpa-uberon\"></a>\n",
    "\n",
    "**Data Source Wiki Page:**  \n",
    "- [human-protein-atlas](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#human-protein-atlas) \n",
    "- [genotype-tissue-expression-project](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#the-genotype-tissue-expression-gtex-project)  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Purpose:** Downloads a query for cell, tissue, and blood types with overexpressed protein-coding genes in the human proteome ([`proteinatlas_search.tsv`](https://www.proteinatlas.org/api/search_download.php?search=&columns=g,eg,up,pe,rnatsm,rnaclsm,rnacasm,rnabrsm,rnabcsm,rnablsm,scl,t_RNA_adipose_tissue,t_RNA_adrenal_gland,t_RNA_amygdala,t_RNA_appendix,t_RNA_basal_ganglia,t_RNA_bone_marrow,t_RNA_breast,t_RNA_cerebellum,t_RNA_cerebral_cortex,t_RNA_cervix,_uterine,t_RNA_colon,t_RNA_corpus_callosum,t_RNA_ductus_deferens,t_RNA_duodenum,t_RNA_endometrium_1,t_RNA_epididymis,t_RNA_esophagus,t_RNA_fallopian_tube,t_RNA_gallbladder,t_RNA_heart_muscle,t_RNA_hippocampal_formation,t_RNA_hypothalamus,t_RNA_kidney,t_RNA_liver,t_RNA_lung,t_RNA_lymph_node,t_RNA_midbrain,t_RNA_olfactory_region,t_RNA_ovary,t_RNA_pancreas,t_RNA_parathyroid_gland,t_RNA_pituitary_gland,t_RNA_placenta,t_RNA_pons_and_medulla,t_RNA_prostate,t_RNA_rectum,t_RNA_retina,t_RNA_salivary_gland,t_RNA_seminal_vesicle,t_RNA_skeletal_muscle,t_RNA_skin_1,t_RNA_small_intestine,t_RNA_smooth_muscle,t_RNA_spinal_cord,t_RNA_spleen,t_RNA_stomach_1,t_RNA_testis,t_RNA_thalamus,t_RNA_thymus,t_RNA_thyroid_gland,t_RNA_tongue,t_RNA_tonsil,t_RNA_urinary_bladder,t_RNA_vagina,t_RNA_B-cells,t_RNA_dendritic_cells,t_RNA_granulocytes,t_RNA_monocytes,t_RNA_NK-cells,t_RNA_T-cells,t_RNA_total_PBMC,cell_RNA_A-431,cell_RNA_A549,cell_RNA_AF22,cell_RNA_AN3-CA,cell_RNA_ASC_diff,cell_RNA_ASC_TERT1,cell_RNA_BEWO,cell_RNA_BJ,cell_RNA_BJ_hTERT+,cell_RNA_BJ_hTERT+_SV40_Large_T+,cell_RNA_BJ_hTERT+_SV40_Large_T+_RasG12V,cell_RNA_CACO-2,cell_RNA_CAPAN-2,cell_RNA_Daudi,cell_RNA_EFO-21,cell_RNA_fHDF/TERT166,cell_RNA_HaCaT,cell_RNA_HAP1,cell_RNA_HBEC3-KT,cell_RNA_HBF_TERT88,cell_RNA_HDLM-2,cell_RNA_HEK_293,cell_RNA_HEL,cell_RNA_HeLa,cell_RNA_Hep_G2,cell_RNA_HHSteC,cell_RNA_HL-60,cell_RNA_HMC-1,cell_RNA_HSkMC,cell_RNA_hTCEpi,cell_RNA_hTEC/SVTERT24-B,cell_RNA_hTERT-HME1,cell_RNA_HUVEC_TERT2,cell_RNA_K-562,cell_RNA_Karpas-707,cell_RNA_LHCN-M2,cell_RNA_MCF7,cell_RNA_MOLT-4,cell_RNA_NB-4,cell_RNA_NTERA-2,cell_RNA_PC-3,cell_RNA_REH,cell_RNA_RH-30,cell_RNA_RPMI-8226,cell_RNA_RPTEC_TERT1,cell_RNA_RT4,cell_RNA_SCLC-21H,cell_RNA_SH-SY5Y,cell_RNA_SiHa,cell_RNA_SK-BR-3,cell_RNA_SK-MEL-30,cell_RNA_T-47d,cell_RNA_THP-1,cell_RNA_TIME,cell_RNA_U-138_MG,cell_RNA_U-2_OS,cell_RNA_U-2197,cell_RNA_U-251_MG,cell_RNA_U-266/70,cell_RNA_U-266/84,cell_RNA_U-698,cell_RNA_U-87_MG,cell_RNA_U-937,cell_RNA_WM-115,blood_RNA_basophil,blood_RNA_classical_monocyte,blood_RNA_eosinophil,blood_RNA_gdT-cell,blood_RNA_intermediate_monocyte,blood_RNA_MAIT_T-cell,blood_RNA_memory_B-cell,blood_RNA_memory_CD4_T-cell,blood_RNA_memory_CD8_T-cell,blood_RNA_myeloid_DC,blood_RNA_naive_B-cell,blood_RNA_naive_CD4_T-cell,blood_RNA_naive_CD8_T-cell,blood_RNA_neutrophil,blood_RNA_NK-cell,blood_RNA_non-classical_monocyte,blood_RNA_plasmacytoid_DC,blood_RNA_T-reg,blood_RNA_total_PBMC,brain_RNA_amygdala,brain_RNA_basal_ganglia,brain_RNA_cerebellum,brain_RNA_cerebral_cortex,brain_RNA_hippocampal_formation,brain_RNA_hypothalamus,brain_RNA_midbrain,brain_RNA_olfactory_region,brain_RNA_pons_and_medulla,brain_RNA_thalamus&format=tsv)) and median gene-level TPM by tissue for all genes that are not protein-coding ([`GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct`](https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz)) in order to create mappings between cell and tissue type strings to the Uber-Anatomy, Cell Ontology, and Cell Line Ontology concepts (see [human-protein-atlas](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-protein-atlas) for details on the mapping process). The mappings are then used to create the following edge types:  \n",
    "- rna-cell line  \n",
    "- rna-tissue type   \n",
    "- protein-cell line  \n",
    "- protein-tissue type  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Output:**  \n",
    "- All HPA tissue and cell type strings ➞ [`HPA_tissues.txt`](https://www.dropbox.com/s/m0spn8h1l8kxb61/HPA_tissues.txt?dl=1)  \n",
    "- Mapping HPA strings to ontology concepts (documentation) ➞ [`zooma_tissue_cell_mapping_04JAN2020.xlsx`](https://www.dropbox.com/s/u7elnc056zxypc6/HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt?dl=1)  \n",
    "- Final HPA-ontology mappings ➞ [`HPA_GTEx_TISSUE_CELL_MAP.txt`](https://www.dropbox.com/s/snzdwv1cvs0v9pp/HPA_GTEx_TISSUE_CELL_MAP.txt?dl=1)\n",
    "- HPA Edges ➞ [`HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt`](https://www.dropbox.com/s/u7elnc056zxypc6/HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human Protein Atlas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Download Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.proteinatlas.org/api/search_download.php?search=&columns=g,eg,up,pe,rnatsm,rnaclsm,rnacasm,rnabrsm,rnabcsm,rnablsm,scl,t_RNA_adipose_tissue,t_RNA_adrenal_gland,t_RNA_amygdala,t_RNA_appendix,t_RNA_basal_ganglia,t_RNA_bone_marrow,t_RNA_breast,t_RNA_cerebellum,t_RNA_cerebral_cortex,t_RNA_cervix,_uterine,t_RNA_colon,t_RNA_corpus_callosum,t_RNA_ductus_deferens,t_RNA_duodenum,t_RNA_endometrium_1,t_RNA_epididymis,t_RNA_esophagus,t_RNA_fallopian_tube,t_RNA_gallbladder,t_RNA_heart_muscle,t_RNA_hippocampal_formation,t_RNA_hypothalamus,t_RNA_kidney,t_RNA_liver,t_RNA_lung,t_RNA_lymph_node,t_RNA_midbrain,t_RNA_olfactory_region,t_RNA_ovary,t_RNA_pancreas,t_RNA_parathyroid_gland,t_RNA_pituitary_gland,t_RNA_placenta,t_RNA_pons_and_medulla,t_RNA_prostate,t_RNA_rectum,t_RNA_retina,t_RNA_salivary_gland,t_RNA_seminal_vesicle,t_RNA_skeletal_muscle,t_RNA_skin_1,t_RNA_small_intestine,t_RNA_smooth_muscle,t_RNA_spinal_cord,t_RNA_spleen,t_RNA_stomach_1,t_RNA_testis,t_RNA_thalamus,t_RNA_thymus,t_RNA_thyroid_gland,t_RNA_tongue,t_RNA_tonsil,t_RNA_urinary_bladder,t_RNA_vagina,t_RNA_B-cells,t_RNA_dendritic_cells,t_RNA_granulocytes,t_RNA_monocytes,t_RNA_NK-cells,t_RNA_T-cells,t_RNA_total_PBMC,cell_RNA_A-431,cell_RNA_A549,cell_RNA_AF22,cell_RNA_AN3-CA,cell_RNA_ASC_diff,cell_RNA_ASC_TERT1,cell_RNA_BEWO,cell_RNA_BJ,cell_RNA_BJ_hTERT+,cell_RNA_BJ_hTERT+_SV40_Large_T+,cell_RNA_BJ_hTERT+_SV40_Large_T+_RasG12V,cell_RNA_CACO-2,cell_RNA_CAPAN-2,cell_RNA_Daudi,cell_RNA_EFO-21,cell_RNA_fHDF/TERT166,cell_RNA_HaCaT,cell_RNA_HAP1,cell_RNA_HBEC3-KT,cell_RNA_HBF_TERT88,cell_RNA_HDLM-2,cell_RNA_HEK_293,cell_RNA_HEL,cell_RNA_HeLa,cell_RNA_Hep_G2,cell_RNA_HHSteC,cell_RNA_HL-60,cell_RNA_HMC-1,cell_RNA_HSkMC,cell_RNA_hTCEpi,cell_RNA_hTEC/SVTERT24-B,cell_RNA_hTERT-HME1,cell_RNA_HUVEC_TERT2,cell_RNA_K-562,cell_RNA_Karpas-707,cell_RNA_LHCN-M2,cell_RNA_MCF7,cell_RNA_MOLT-4,cell_RNA_NB-4,cell_RNA_NTERA-2,cell_RNA_PC-3,cell_RNA_REH,cell_RNA_RH-30,cell_RNA_RPMI-8226,cell_RNA_RPTEC_TERT1,cell_RNA_RT4,cell_RNA_SCLC-21H,cell_RNA_SH-SY5Y,cell_RNA_SiHa,cell_RNA_SK-BR-3,cell_RNA_SK-MEL-30,cell_RNA_T-47d,cell_RNA_THP-1,cell_RNA_TIME,cell_RNA_U-138_MG,cell_RNA_U-2_OS,cell_RNA_U-2197,cell_RNA_U-251_MG,cell_RNA_U-266/70,cell_RNA_U-266/84,cell_RNA_U-698,cell_RNA_U-87_MG,cell_RNA_U-937,cell_RNA_WM-115,blood_RNA_basophil,blood_RNA_classical_monocyte,blood_RNA_eosinophil,blood_RNA_gdT-cell,blood_RNA_intermediate_monocyte,blood_RNA_MAIT_T-cell,blood_RNA_memory_B-cell,blood_RNA_memory_CD4_T-cell,blood_RNA_memory_CD8_T-cell,blood_RNA_myeloid_DC,blood_RNA_naive_B-cell,blood_RNA_naive_CD4_T-cell,blood_RNA_naive_CD8_T-cell,blood_RNA_neutrophil,blood_RNA_NK-cell,blood_RNA_non-classical_monocyte,blood_RNA_plasmacytoid_DC,blood_RNA_T-reg,blood_RNA_total_PBMC,brain_RNA_amygdala,brain_RNA_basal_ganglia,brain_RNA_cerebellum,brain_RNA_cerebral_cortex,brain_RNA_hippocampal_formation,brain_RNA_hypothalamus,brain_RNA_midbrain,brain_RNA_olfactory_region,brain_RNA_pons_and_medulla,brain_RNA_thalamus&format=tsv'\n",
    "data_downloader(url, unprocessed_data_location, 'proteinatlas_search.tsv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Load Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa = pandas.read_csv(unprocessed_data_location + 'proteinatlas_search.tsv',\n",
    "                      header=0,\n",
    "                      delimiter='\\t')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "hpa.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Identify HPA Terms Needing Mapping_  \n",
    "To expedite the mapping process, all HPA tissues, cells, cell lines, and fluid types are extracted from the HPA data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve terms to map\n",
    "terms_to_map = list(hpa.columns)\n",
    "\n",
    "# write results\n",
    "with open(unprocessed_data_location + 'HPA_tissues.txt', 'w') as outfile:\n",
    "    for x in tqdm(terms_to_map):\n",
    "        if x.endswith('[NX]'):\n",
    "            term = x.split('RNA - ')[-1].split(' [NX]')[:-1][0]\n",
    "            outfile.write(term + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genotype-Tissue Expression Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Download Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Load Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtex = pandas.read_csv(unprocessed_data_location + 'GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct',\n",
    "                       header=0,\n",
    "                       skiprows=2,\n",
    "                       delimiter='\\t')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "gtex.fillna('None', inplace=True)\n",
    "\n",
    "# remove identifier type, which appears after '.'\n",
    "gtex['Name'].replace('(\\..*)','', inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Mapping Data**   \n",
    "Import the tissues, cells, cell lines, and fluids that we externally mapped from HPA and GTEx data to [UBERON](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#uber-anatomy-ontology), the [Cell Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#cell-ontology), and the [Cell Line Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#cell-line-ontology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data = pandas.read_excel(open(unprocessed_data_location + 'zooma_tissue_cell_mapping_04JAN2020.xlsx', 'rb'),\n",
    "                                 sheet_name='Concept_Mapping - 04JAN2020',\n",
    "                                 header=0)\n",
    "\n",
    "# convert NaN to None\n",
    "mapping_data.fillna('None', inplace=True)\n",
    "\n",
    "# preview data\n",
    "mapping_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write HPA and GTEx Mapping Data_  \n",
    "The HPA and GTEx mapping data is written locally so that it can be used by the `PheKnowLator` algorithm when creating the knowledge graph edge lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'HPA_GTEx_TISSUE_CELL_MAP.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(mapping_data.iterrows(), total=mapping_data.shape[0]):\n",
    "        if row['UBERON ID'] != 'None':\n",
    "            outfile.write(str(row['ORIGINAL TERM']).strip() + '\\t' + str(row['UBERON ID']).strip() + '\\n')\n",
    "        if row['CL ID'] != 'None':\n",
    "            outfile.write(str(row['ORIGINAL TERM']).strip() + '\\t' + str(row['CL ID']).strip() + '\\n')\n",
    "        if row['CLO ID'] != 'None':\n",
    "            outfile.write(str(row['ORIGINAL TERM']).strip() + '\\t' + str(row['CLO ID']).strip() + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data = pandas.read_csv(processed_data_location + 'HPA_GTEx_TISSUE_CELL_MAP.txt',\n",
    "                               header=None,\n",
    "                               names=['TISSUE_CELL_TERM', 'ONTOLOGY_IDs'],\n",
    "                               delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Edge Data Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Human Protein Atlas_  \n",
    "The `HPA` data looped over and reformatted such all all tissue, cell, cell lines, and fluid types are stored as a nested list. As shown in the code chunk, you will see that the anatomy type is specified as an item in the list according to its type. This is done in order to make mapping more efficient while building the knowledge graph edge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_results = []\n",
    "\n",
    "for idx, row in tqdm(hpa.iterrows(), total=hpa.shape[0]):\n",
    "    if row['RNA tissue specific NX'] != 'None':\n",
    "        for x in row['RNA tissue specific NX'].split(';'):\n",
    "            hpa_results += [[row['Ensembl'], row['Gene'], row['Uniprot'], row['Evidence'], 'anatomy', x.split(':')[0]]]\n",
    "\n",
    "    if row['RNA cell line specific NX'] != 'None':\n",
    "        for x in row['RNA cell line specific NX'].split(';'):\n",
    "            hpa_results += [[row['Ensembl'], row['Gene'], row['Uniprot'], row['Evidence'], 'cell line', x.split(':')[0]]]\n",
    "\n",
    "    if row['RNA brain regional specific NX'] != 'None':\n",
    "        for x in row['RNA brain regional specific NX'].split(';'):\n",
    "            hpa_results += [[row['Ensembl'], row['Gene'], row['Uniprot'], row['Evidence'], 'anatomy', x.split(':')[0]]]\n",
    "\n",
    "    if row['RNA blood cell specific NX'] != 'None':\n",
    "        for x in row['RNA blood cell specific NX'].split(';'):\n",
    "            hpa_results += [[row['Ensembl'], row['Gene'], row['Uniprot'], row['Evidence'], 'anatomy', x.split(':')[0]]]\n",
    "\n",
    "    if row['RNA blood lineage specific NX'] != 'None':\n",
    "        for x in row['RNA blood lineage specific NX'].split(';'):\n",
    "            hpa_results += [[row['Ensembl'], row['Gene'], row['Uniprot'], row['Evidence'], 'anatomy', x.split(':')[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Genotype-Tissue Expression Project_  \n",
    "The `GTEx` edge data is created by first filtering out all _protein-coding_ genes that appear in the `HPA` cell transcriptome data set. Once filter so that we are only left noncoding genes, we perform an additional filtering step to only add genes and their corresponding tissue, cell, or fluid, if the median expression is `>= 1.0`. The `GTEx` is formatted such all all tissue, cell, and fluid types occur as their own column and all unique genes occur as a row, thus the expression filtering step is performed while also reformatting the file. The genes and tissues/cells/fluids that meet criteria are stored as a nested list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of hpa protein-coding genes\n",
    "hpa_genes = list(hpa['Ensembl'].drop_duplicates(keep='first', inplace=False))\n",
    "\n",
    "# remove rows that contain protein coding genes\n",
    "gtex = gtex.loc[gtex['Name'].apply(lambda x: x not in hpa_genes)]\n",
    "\n",
    "# loop over data and re-organize - only keep results with tpm >= 1 and if gene symbol is not a protein-coding gene\n",
    "gtex_results = []\n",
    "\n",
    "for idx, row in tqdm(gtex.iterrows(), total=gtex.shape[0]):    \n",
    "    for col in list(gtex.columns)[2:]:\n",
    "        if row[col] >= 1.0:           \n",
    "            gtex_results += [[row['Name'], row['Description'], 'None', 'Evidence at transcript level', 'cell line' if 'Cells' in col else 'anatomy', col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write Results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt', 'w') as outfile:\n",
    "    for res in tqdm(hpa_results + gtex_results):\n",
    "        outfile.write(str(res[0]) + '\\t' + str(res[1]) + '\\t' + str(res[2]) + '\\t' + str(res[3]) + '\\t' + str(res[4]) + '\\t' + str(res[5]) + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_edges = pandas.read_csv(processed_data_location + 'HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt',\n",
    "                           header=None,\n",
    "                           names=['Ensembl_IDs', 'Gene_Symbols', 'Uniport_IDs', 'Evidence', 'Anatomy_Type', 'Anatomy'],\n",
    "                           low_memory=False,\n",
    "                           sep='\\t')\n",
    "\n",
    "print('There are {edge_count} edges'.format(edge_count=len(hpa_edges)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_edges.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Mapping Reactome Pathways to the Pathway Ontology <a class=\"anchor\" id=\"reactome-pw\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Pathway Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#pathway-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the [canonical pathways](http://compath.scai.fraunhofer.de/export_mappings) and [kegg-reactome pathway mappings](https://github.com/ComPath/resources/blob/master/mappings/kegg_reactome.csv) files from the [ComPath Ecosystem](https://github.com/ComPath) in order to create the following identifier mappings:  \n",
    "- `Reactome Pathway Identifiers`  ➞ `KEGG Pathway Identifiers` ➞ `Pathway Ontology Identifiers` \n",
    "\n",
    "**Output:**  \n",
    "- [`REACTOME_PW_GO_MAPPINGS.txt`](https://www.dropbox.com/s/gtvsjypkwdedu4f/REACTOME_PW_GO_MAPPINGS.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the Pathway Ontology**   \n",
    "Use [OWL Tools](https://github.com/owlcollab/owltools/wiki) to download the [Pathway Ontology](http://www.obofoundry.org/ontology/pw.html). Once downloaded, we read the ontology in as a `RDFLib` graph object so that we can query it to obtain all `DbXRefs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ontology using subprocess and OWLTOOLS in order to get the ontology and its imported ontologies\n",
    "subprocess.check_call(['./resources/lib/owltools',\n",
    "                       'http://purl.obolibrary.org/obo/pw.owl',\n",
    "                       '--merge-import-closure',\n",
    "                       '-o',\n",
    "                       unprocessed_data_location + 'pw_with_imports.owl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_graph = Graph()\n",
    "pw_graph.parse(unprocessed_data_location + 'pw_with_imports.owl')\n",
    "\n",
    "print('There are {} axioms in the ontology (date: {})'.format(len(pw_graph), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pw_graph.query(\n",
    "    \"\"\"SELECT DISTINCT ?c ?xref ?rel_syns ?exc_syns\n",
    "           WHERE {\n",
    "              ?c rdf:type owl:Class .\n",
    "              ?c rdfs:label ?c_label .\n",
    "              ?c_annot owl:annotatedSource ?c .\n",
    "              ?c_annot oboInOwl:hasDbXref ?xref .\n",
    "              ?c oboInOwl:hasRelatedSynonym ?rel_syns . \n",
    "              ?c oboInOwl:hasExactSynonym ?exc_syns .}\n",
    "           \"\"\", initNs={\"rdf\": 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "                        \"rdfs\": 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "                        \"owl\": 'http://www.w3.org/2002/07/owl#',\n",
    "                        \"oboInOwl\": 'http://www.geneontology.org/formats/oboInOwl#'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reformat Mapping Results_  \n",
    "Create a dictionary of mapping results where pathway ontology identifiers are values and the keys are `DbXRef` identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mappings = {}\n",
    "\n",
    "for res in tqdm(results):\n",
    "    for x in res:\n",
    "        if 'http' not in x and 'PMID' not in x:\n",
    "            if str(x) in id_mappings.keys():\n",
    "                id_mappings[str(x)] |= set([str(res[0])])\n",
    "            else:\n",
    "                id_mappings[str(x)] = set([str(res[0])])\n",
    "\n",
    "print('There are {} results (date: {})'.format(len(id_mappings), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Human Reactome Pathways**  \n",
    "Download a file of all [Reactome Pathways](https://reactome.org/download/current/ReactomePathways.txt). This file will be filtered to only include human pathways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://reactome.org/download/current/ReactomePathways.txt'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactome_pathways = pandas.read_csv(unprocessed_data_location + 'ReactomePathways.txt',\n",
    "                                    header=None,\n",
    "                                    delimiter='\\t',\n",
    "                                    low_memory=False)\n",
    "\n",
    "# remove all non-human pathways\n",
    "reactome_pathways = reactome_pathways.loc[reactome_pathways[2].apply(lambda x: x == 'Homo sapiens')] \n",
    "\n",
    "# save as list\n",
    "mapped_reactome_identifiers = {x:set(['PW_0000001']) for x in set(list(reactome_pathways[0]))}                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ComPath Reactome Pathway Mappings**  \n",
    "Use [ComPath Mappings](https://github.com/ComPath/resources/tree/master/mappings) to obtain the following mappings:  \n",
    "- `Reactome Pathway Identifiers`  ➞ `KEGG Pathway Identifiers` ➞ `Pathway Ontology Identifiers` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Canonical Pathways_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'http://compath.scai.fraunhofer.de/export_mappings'\n",
    "data_downloader(url1, unprocessed_data_location, 'compath_canonical_pathway_mappings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compath_cannonical = pandas.read_csv(unprocessed_data_location + 'compath_canonical_pathway_mappings.txt',\n",
    "                               header=None,\n",
    "                               delimiter='\\t',\n",
    "                               low_memory=False)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "compath_cannonical.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(compath_cannonical.iterrows(), total=compath_cannonical.shape[0]):\n",
    "    if row[6] == 'kegg' and 'KEGG:' + row[5].strip('path:hsa') in id_mappings.keys() and row[2] == 'reactome':\n",
    "        for x in id_mappings['KEGG:' + row[5].strip('path:hsa')]:\n",
    "            if row[1] in mapped_reactome_identifiers.keys():\n",
    "                mapped_reactome_identifiers[row[1]] |= set([x.split('/')[-1]])\n",
    "            else:\n",
    "                mapped_reactome_identifiers[row[1]] = set([x.split('/')[-1]])\n",
    "    \n",
    "    if (row[2] == 'kegg' and 'KEGG:' + row[1].strip('path:hsa') in id_mappings.keys()) and row[6] == 'reactome':\n",
    "        for x in id_mappings['KEGG:' + row[1].strip('path:hsa')]:\n",
    "            if row[5] in mapped_reactome_identifiers.keys():\n",
    "                mapped_reactome_identifiers[row[5]] |= set([x.split('/')[-1]])\n",
    "            else:\n",
    "                mapped_reactome_identifiers[row[5]] = set([x.split('/')[-1]])         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_KEGG - Reactome Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://raw.githubusercontent.com/ComPath/resources/master/mappings/kegg_reactome.csv'\n",
    "data_downloader(url2, unprocessed_data_location, 'kegg_reactome.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_reactome_map = pandas.read_csv(unprocessed_data_location + 'kegg_reactome.csv',\n",
    "                                    header=0,\n",
    "                                    delimiter=',',\n",
    "                                    low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(kegg_reactome_map.iterrows(), total=kegg_reactome_map.shape[0]):\n",
    "    if row['Source Resource'] == 'reactome' and 'KEGG:' + row['Target ID'].strip('path:hsa') in id_mappings.keys():\n",
    "        for x in id_mappings['KEGG:' + row['Target ID'].strip('path:hsa')]:\n",
    "            if row['Source ID'] in mapped_reactome_identifiers.keys():\n",
    "                mapped_reactome_identifiers[row['Source ID']] |= set([x.split('/')[-1]])\n",
    "            else:\n",
    "                mapped_reactome_identifiers[row['Source ID']] = set([x.split('/')[-1]])\n",
    "    \n",
    "    if row['Target Resource'] == 'reactome' and 'KEGG:' + row['Source Resource'].strip('path:hsa') in id_mappings.keys():\n",
    "        for x in id_mappings['KEGG:' + row['Source ID'].strip('path:hsa')]:\n",
    "            if row['Target ID'] in mapped_reactome_identifiers.keys():\n",
    "                mapped_reactome_identifiers[row['Target ID']] |= set([x.split('/')[-1]])\n",
    "            else:\n",
    "                mapped_reactome_identifiers[row['Target ID']] = set([x.split('/')[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reactome Pathway GO Annotation Mappings**  \n",
    "Use Reactome's [API](https://reactome.org/dev/content-service) to obtain the following mappings:  \n",
    "- `Reactome Pathway Identifiers`  ➞ `Gene Ontology Identifiers`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for request_ids in tqdm(list(chunks(list(mapped_reactome_identifiers.keys()), 20))):\n",
    "    for res in content.query_ids(ids=','.join(request_ids)):\n",
    "        for key in res.keys(): \n",
    "            if key == 'goBiologicalProcess':\n",
    "                for x in res[key]:\n",
    "                    if res['stId'] in mapped_reactome_identifiers.keys():\n",
    "                        mapped_reactome_identifiers[res['stId']] |= set(['GO_' + res[key]['accession']])\n",
    "                    else:\n",
    "                        mapped_reactome_identifiers[res['stId']] = set(['GO_' + res[key]['accession']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'REACTOME_PW_GO_MAPPINGS.txt', 'w') as outfile:\n",
    "    for key in tqdm(mapped_reactome_identifiers.keys()):\n",
    "        for mapping in mapped_reactome_identifiers[key]:\n",
    "            outfile.write(key + '\\t' + mapping + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3359 pathway ontology mappings\n"
     ]
    }
   ],
   "source": [
    "pw_data = pandas.read_csv(processed_data_location + 'REACTOME_PW_GO_MAPPINGS.txt',\n",
    "                           header=None,\n",
    "                           names=['Pathway_IDs', 'Mapping_IDs'],\n",
    "                           delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} pathway ontology mappings'.format(edge_count=len(pw_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pathway_IDs</th>\n",
       "      <th>Mapping_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R-HSA-6804757</td>\n",
       "      <td>PW_0000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R-HSA-5357609</td>\n",
       "      <td>PW_0000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R-HSA-73779</td>\n",
       "      <td>PW_0000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R-HSA-73779</td>\n",
       "      <td>GO_0006367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R-HSA-5389840</td>\n",
       "      <td>GO_0070125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pathway_IDs Mapping_IDs\n",
       "0  R-HSA-6804757  PW_0000001\n",
       "1  R-HSA-5357609  PW_0000001\n",
       "2    R-HSA-73779  PW_0000001\n",
       "3    R-HSA-73779  GO_0006367\n",
       "4  R-HSA-5389840  GO_0070125"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pw_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Mapping Genomic Identifiers to the Sequence Ontology <a class=\"anchor\" id=\"#genomic-so\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Sequence Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/_edit#sequence-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the [genomic identifier mapping](https://www.dropbox.com/s/g0blo27qc8ogvk2/genomic_sequence_ontology_mappings.xlsx?dl=1) file in order to create the following identifier mappings:  \n",
    "- `Gene BioTypes`  ➞ `Sequence Ontology Identifiers`  \n",
    "- `RNA BioTypes`  ➞ `Sequence Ontology Identifiers`  \n",
    "- `variant Types`  ➞ `Sequence Ontology Identifiers`\n",
    "\n",
    "**Output:**  \n",
    "- [`SO_GENE_TRANSCRIPT_VARIANT_TYPE_MAPPING.txt`](https://www.dropbox.com/s/9ry9ju74ett45b1/SO_GENE_TRANSCRIPT_VARIANT_TYPE_MAPPING.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data = pandas.read_excel(open(unprocessed_data_location + 'genomic_sequence_ontology_mappings.xlsx', 'rb'),\n",
    "                                 sheet_name='Mapping_Date_09Mar2020',\n",
    "                                 header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'SO_GENE_TRANSCRIPT_VARIANT_TYPE_MAPPING.txt', 'w') as outfile:\n",
    "    for idx, row in tqdm(mapping_data.iterrows(), total=mapping_data.shape[0]):\n",
    "        outfile.write(row['source_*_type'] + '\\t' + row['SO ID'] + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_data = pandas.read_csv(processed_data_location + 'SO_GENE_TRANSCRIPT_VARIANT_TYPE_MAPPING.txt',\n",
    "                           header=None,\n",
    "                           names=['Genomic_Type', 'Sequence_Ontology_ID'],\n",
    "                           delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} sequence ontology mappings'.format(edge_count=len(so_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### CREATE EDGE DATASETS  <a class=\"anchor\" id=\"create-edge-datasets\"></a>\n",
    "***\n",
    "***\n",
    "\n",
    "### Ontologies  <a class=\"anchor\" id=\"ontologies\"></a>\n",
    "***\n",
    "- [Protein Ontology](#protein-ontology)  \n",
    "- [Relations Ontology](#relations-ontology)  \n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Protein Ontology <a class=\"anchor\" id=\"protein-ontology\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [protein-ontology](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#human-phenotype-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the [pr.owl](http://purl.obolibrary.org/obo/pr.owl) file from [ProConsortium.org](https://proconsortium.org/) in order to create a version of the ontology that contains only human proteins. This is achieved by performing forward and reverse breadth first search over all proteins which are `owl:subClassOf` [Homo sapiens protein](https://proconsortium.org/app/entry/PR%3A000029067/).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Output:**  \n",
    "- Human Protein Ontology ➞ [`human_pro.owl`](https://www.dropbox.com/s/jw8jksgnqbcz9sm/human_pro.owl?dl=1)\n",
    "- Classified Human Protein Ontology (Hermit) ➞ [`human_pro_closed.owl`](https://www.dropbox.com/s/6ux85agl95ja3wx/human_pro_closed.owl?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ontology using subprocess and OWLTOOLS in order to get the ontology and its imported ontologies\n",
    "subprocess.check_call(['./resources/lib/owltools',\n",
    "                       'http://purl.obolibrary.org/obo/pr.owl',\n",
    "                       '--merge-import-closure',\n",
    "                       '-o',\n",
    "                       unprocessed_data_location + 'pw_with_imports.owl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ontology as graph (the ontology is large so this takes ~60 minutes) - 11,757,623 edges on 12/18/2019\n",
    "graph = Graph()\n",
    "graph.parse(unprocessed_data_location + 'pw_with_imports.owl')\n",
    "\n",
    "print('There are {} axioms in the ontology (date: {})'.format(len(graph), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Convert Ontology to Directed MulitGraph_  \n",
    "In order to create a version of the ontology which includes all relevant human edges, we need to first convert the KG to a [directed multigraph](https://networkx.github.io/documentation/stable/reference/classes/multidigraph.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert RDF graph to multidigraph\n",
    "networkx_mdg: networkx.MultiDiGraph = networkx.MultiDiGraph()\n",
    "    \n",
    "for s, p, o in tqdm(graph):\n",
    "    networkx_mdg.add_edge(s, o, **{'key': p})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Identify Human Proteins_   \n",
    "A list of human proteins is obtained by querying the ontology to return all ontology classes `only_in_taxon some Homo sapiens`. To expedite the query time, the following SPARQL query is run from the [ProConsortium](https://proconsortium.org/pro_sparql.shtml) SPARQL endpoint: \n",
    "\n",
    "```SPARQL\n",
    "PREFIX obo: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT ?PRO_term\n",
    "FROM <http://purl.obolibrary.org/obo/pr>\n",
    "WHERE {\n",
    "       ?PRO_term rdf:type owl:Class .\n",
    "       ?PRO_term rdfs:subClassOf ?restriction .\n",
    "       ?restriction owl:onProperty obo:RO_0002160 .\n",
    "       ?restriction owl:someValuesFrom obo:NCBITaxon_9606 .\n",
    "\n",
    "       # use this to filter-out things like hgnc ids\n",
    "       FILTER (regex(?PRO_term,\"http://purl.obolibrary.org/obo/*\")) .\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data - pro classes only_in_taxon some Homo sapiens (61,064 classes on 03/10/2020)\n",
    "url = 'https://sparql.proconsortium.org/virtuoso/sparql?default-graph-uri=&query=PREFIX+obo%3A+%3Chttp%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F%3E%0D%0A%0D%0ASELECT+%3FPRO_term%0D%0AFROM+%3Chttp%3A%2F%2Fpurl.obolibrary.org%2Fobo%2Fpr%3E%0D%0AWHERE+%7B%0D%0A+++++++%3FPRO_term+rdf%3Atype+owl%3AClass+.%0D%0A+++++++%3FPRO_term+rdfs%3AsubClassOf+%3Frestriction+.%0D%0A+++++++%3Frestriction+owl%3AonProperty+obo%3ARO_0002160+.%0D%0A+++++++%3Frestriction+owl%3AsomeValuesFrom+obo%3ANCBITaxon_9606+.%0D%0A%0D%0A+++++++%23+use+this+to+filter-out+things+like+hgnc+ids%0D%0A+++++++FILTER+%28regex%28%3FPRO_term%2C%22http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F*%22%29%29+.%0D%0A%7D&format=text%2Fhtml&timeout=5000&debug=on'\n",
    "html = requests.get(url, allow_redirects=True).content\n",
    "\n",
    "# extract data from html table\n",
    "df_list = pandas.read_html(html)\n",
    "human_pro_classes = list(df_list[-1]['PRO_term'])\n",
    "\n",
    "print('There are {} edges in the ontology (date:{})'.format(len(human_pro_classes), datetime.datetime.now().strftime('%m/%d/%Y')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Construct Human PRO_   \n",
    "Now that we have all of the paths from the original graph that are relevant to humans, we can construct a human-only version of the PRotein ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new graph using bfs paths\n",
    "human_pro_graph = Graph()\n",
    "human_networkx_mdg = networkx.MultiDiGraph()\n",
    "\n",
    "for node in tqdm(human_pro_classes):\n",
    "    forward = list(networkx.edge_bfs(networkx_mdg, URIRef(node), orientation='original'))\n",
    "    reverse = list(networkx.edge_bfs(networkx_mdg, URIRef(node), orientation='reverse'))\n",
    "    \n",
    "    # add edges from forward and reverse bfs paths\n",
    "    for path in forward + reverse:\n",
    "        human_pro_graph.add((path[0], path[2], path[1]))\n",
    "        human_networkx_mdg.add_edge(path[0], path[1], path[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the constructed ontology only has 1 component\n",
    "networkx.number_connected_components(human_networkx_mdg.to_undirected())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered ontology\n",
    "human_pro_graph.serialize(destination=unprocessed_data_location + 'human_pro.owl', format='xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Classify Ontology_  \n",
    "To ensure that we have correclty built the new ontology, we run the hermit reasoner over it to ensure that there are no incomplete triples or inconsistent classes. In order to do this, we will call the reasoner using [OWLTools](https://github.com/owlcollab/owltools), which this script assumes has already been downloaded to the `./resources/lib` directory. The following arguments are then called to run the reasoner (from the command line):  \n",
    "\n",
    "```bash\n",
    "./resources/lib/owltools ./resources/processed_data/unprocessed_data/human_pro.owl --reasoner hermit --run-reasoner --assert-implied -o ./resources/processed_data/human_pro_closed.owl\n",
    "```\n",
    "\n",
    "_**Note.** This step takes around 30-45 minutes to run. When run from the command line the reasoner determined that the ontology was consistent and 174 new axioms were inferred (03/10/2020)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run reasoner -- RUN FROM COMMAND LINE NOT HERE\n",
    "subprocess.run(['./resources/lib/owltools',\n",
    "                './resources/unprocessed_data/human_pro_filtered.owl',\n",
    "                '--reasoner hermit',\n",
    "                '--run-reasoner',\n",
    "                '--assert-implied',\n",
    "                '--list-unsatisfiable',\n",
    "                '-o ./resources/processed_data/human_pro_closed.owl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Examine Cleaned Human PRO_  \n",
    "Once we have cleaned the ontology we can get counts of components, nodes, edges, and then write the cleaned graph to the `../../resources/processed_data` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gets_ontology_statistics('./resources/processed_data/human_pro_closed.owl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations Ontology <a class=\"anchor\" id=\"relations-ontology\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [RO](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#relation-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the [ro.owl](http://purl.obolibrary.org/obo/ro.owl) file from [obofoundry.org](http://www.obofoundry.org/) in order to obtain all `ObjectProperties` and their inverse relations.  \n",
    "\n",
    "**Output:** \n",
    "- Relations and Inverse Relations ➞ [`INVERSE_RELATIONS.txt`](https://www.dropbox.com/s/sd8qlib8f6gqyz4/INVERSE_RELATIONS.txt?dl=1)\n",
    "- Relations and Labels ➞ [`RELATIONS_LABELS.txt`](https://www.dropbox.com/s/k2hm9p0r8l9ecj3/RELATIONS_LABELS.txt?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Download Ontology_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ontology using subprocess and OWLTOOLS in order to get the ontology and its imported ontologies\n",
    "subprocess.run(['./resources/lib/owltools',\n",
    "                'http://purl.obolibrary.org/obo/ro.owl',\n",
    "                '--merge-import-closure',\n",
    "                '-o',\n",
    "                unprocessed_data_location + 'ro_with_imports.owl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Load Ontology to RDFLib Graph_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_graph = Graph()\n",
    "ro_graph.parse(unprocessed_data_location + 'ro_with_imports.owl')\n",
    "\n",
    "print('There are {} edges in the ontology (date:{})'.format(len(ro_graph), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Relations and Inverse Relations**  \n",
    "Identify all relations and their inverse relations using the `owl:inverseOf` property. To make it easier to look up the inverse relations, each pair is listed twice, for example:  \n",
    "- [location of](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001015) `owl:inverseOf` [located in](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001025)  \n",
    "- [located in](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001025) `owl:inverseOf` [location of](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(relations_data_location + 'INVERSE_RELATIONS.txt', 'w') as outfile:\n",
    "    \n",
    "    # write column names\n",
    "    outfile.write('Relation' + '\\t' + 'Inverse_Relation' + '\\n')\n",
    "\n",
    "    # find inverse relations\n",
    "    for s, p, o in tqdm(ro_graph):\n",
    "        if 'owl#inverseOf' in str(p):\n",
    "            if 'RO' in str(s) and 'RO' in str(o):\n",
    "                outfile.write(str(s.split('/')[-1]) + '\\t' + str(o.split('/')[-1]) + '\\n')\n",
    "                outfile.write(str(o.split('/')[-1]) + '\\t' + str(s.split('/')[-1]) + '\\n')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data = pandas.read_csv(relations_data_location + 'INVERSE_RELATIONS.txt',\n",
    "                          header=0,\n",
    "                          delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} RO Relations and Inverse Relations'.format(edge_count=len(ro_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Relations Labels**  \n",
    "Identify all relations and their labels for use when building the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ro_graph.query(\n",
    "    \"\"\"SELECT DISTINCT ?p ?p_label\n",
    "           WHERE {\n",
    "              ?p rdf:type owl:ObjectProperty .\n",
    "              ?p rdfs:label ?p_label . }\n",
    "           \"\"\", initNs={\"rdf\": 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "                        \"rdfs\": 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "                        \"owl\": 'http://www.w3.org/2002/07/owl#'})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to file\n",
    "with open(relations_data_location + 'RELATIONS_LABELS.txt', 'w') as outfile:\n",
    "    \n",
    "    # write column names\n",
    "    outfile.write('Relation' + '\\t' + 'Label' + '\\n')\n",
    "\n",
    "    for p, p_label in list(results):\n",
    "        outfile.write(str(p).split('/')[-1] + '\\t' + str(p_label) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data_label = pandas.read_csv(relations_data_location + 'RELATIONS_LABELS.txt',\n",
    "                                header=0,\n",
    "                                delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} RO Relations and Labels'.format(edge_count=len(ro_data_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_data_label.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Linked Data <a class=\"anchor\" id=\"linked-data\"></a>\n",
    "***\n",
    "* [Clinvar Variant-Diseases and Phenotypes](#clinvar-variant) \n",
    "* [Uniprot Protein-Cofactor and Protein-Catalyst](#uniprot-protein-cofactorcatalyst)  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Clinvar Variant-Diseases and Phenotypes <a class=\"anchor\" id=\"clinvar-variant\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Clinvar](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#clinvar)  \n",
    "\n",
    "**Purpose:** This script downloads the [variant_summary.txt](ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz) file from [CLinVar](https://www.ncbi.nlm.nih.gov/clinvar/) in order to create the following edges:  \n",
    "- gene-variant  \n",
    "- variant-disease  \n",
    "- variant-phenotype  \n",
    "\n",
    "**Output:** [`CLINVAR_VARIANT_GENE_DISEASE_PHENOTYPE_EDGES.txt`](https://www.dropbox.com/s/1doj3lj46ufgdpd/CLINVAR_VARIANT_GENE_DISEASE_PHENOTYPE_EDGES.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinvar_data = pandas.read_csv(unprocessed_data_location + 'variant_summary.txt',\n",
    "                               header=0,\n",
    "                               delimiter='\\t',\n",
    "                               low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with 'None'\n",
    "clinvar_data.fillna('None', inplace=True)\n",
    "\n",
    "# explode nested data\n",
    "explode_df_clinvar = explodes_data(clinvar_data.copy(), ['PhenotypeIDS'], ';')\n",
    "explode_df_clinvar = explodes_dataexplode_df_clinvar.copy(), ['PhenotypeIDS'], ',')\n",
    "\n",
    "# edit column formatting\n",
    "explode_df_clinvar['PhenotypeIDS'].replace('Orphanet:ORPHA','ORPHA:', inplace=True, regex=True)\n",
    "explode_df_clinvar['PhenotypeIDS'].replace('Human Phenotype Ontology:HP:','HP_', inplace=True, regex=True)\n",
    "\n",
    "# write data\n",
    "explode_df_clinvar.to_csv(processed_data_location + 'CLINVAR_VARIANT_GENE_DISEASE_PHENOTYPE_EDGES.txt',\n",
    "                          header=True,\n",
    "                          sep='\\t',\n",
    "                          encoding='utf-8',\n",
    "                          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {edge_count} variant edges'.format(edge_count=len(explode_df_clinvar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview data\n",
    "explode_df_clinvar.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniprot  Protein-Cofactor and Protein-Catalyst <a class=\"anchor\" id=\"uniprot-protein-cofactorcatalyst\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Uniprot](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#uniprot-knowledgebase)  \n",
    "\n",
    "**Purpose:** This script downloads the [uniprot-cofactor-catalyst.tab](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#uniprot-knowledgebase) file from the [Uniprot Knowledge Base](https://www.uniprot.org) in order to create the following edges:  \n",
    "- protein-cofactor  \n",
    "- protein-catalyst  \n",
    "\n",
    "**Output:**  \n",
    "- protein-cofactor ➞ [`UNIPROT_PROTEIN_COFACTOR.txt`](https://www.dropbox.com/s/ij9t89botd8nmmj/UNIPROT_PROTEIN_COFACTOR.txt?dl=1)\n",
    "- protein-catalyst ➞ [`UNIPROT_PROTEIN_CATALYST.txt`](https://www.dropbox.com/s/pvopvs0iq8x3oq2/UNIPROT_PROTEIN_CATALYST.txt?dl=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.uniprot.org/uniprot/?query=&fil=organism%3A%22Homo%20sapiens%20(Human)%20%5B9606%5D%22&columns=id%2Centry%20name%2Creviewed%2Cdatabase(PRO)%2Cchebi(Cofactor)%2Cchebi(Catalytic%20activity)&format=tab'\n",
    "data_downloader(url, unprocessed_data_location, 'uniprot-cofactor-catalyst.tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(unprocessed_data_location + 'uniprot-cofactor-catalyst.tab').readlines()\n",
    "\n",
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'UNIPROT_PROTEIN_COFACTOR.txt', 'w') as outfile1, open(processed_data_location + 'UNIPROT_PROTEIN_CATALYST.txt', 'w') as outfile2:\n",
    "    for line in tqdm(data):\n",
    "\n",
    "        # get cofactors\n",
    "        if 'CHEBI' in line.split('\\t')[4]: \n",
    "            for i in line.split('\\t')[4].split(';'):\n",
    "                chebi = i.split('[')[-1].replace(']', '').replace(':', '_')\n",
    "                outfile1.write('PR_' + line.split('\\t')[3].strip(';') + '\\t' + chebi + '\\n')\n",
    "        \n",
    "        # get catalysts\n",
    "        if 'CHEBI' in line.split('\\t')[5]:       \n",
    "            for i in line.split('\\t')[5].split(';'):\n",
    "                chebi = i.split('[')[-1].replace(']', '').replace(':', '_')\n",
    "                outfile2.write('PR_' + line.split('\\t')[3].strip(';') + '\\t' + chebi + '\\n')\n",
    "\n",
    "outfile1.close()\n",
    "outfile2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Cofactor Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp1_data = pandas.read_csv(processed_data_location + 'UNIPROT_PROTEIN_COFACTOR.txt',\n",
    "                            header=None,\n",
    "                            names=['Protein_Ontology_IDs', 'CHEBI_IDs'],\n",
    "                            delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} protein-cofactor edges'.format(edge_count=len(pcp1_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp1_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Catalyst Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp2_data = pandas.read_csv(processed_data_location + 'UNIPROT_PROTEIN_CATALYST.txt',\n",
    "                            header=None,\n",
    "                            names=['Protein_Ontology_IDs', 'CHEBI_IDs'],\n",
    "                            delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} protein-catalyst edges'.format(edge_count=len(pcp2_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp2_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### INSTANCE METADATA <a class=\"anchor\" id=\"create-instance-metadata\"></a>\n",
    "***\n",
    "\n",
    "**Data Source Wiki Page:** [Dependencies](https://github.com/callahantiff/PheKnowLator/wiki/Dependencies/#node-metadata) \n",
    "\n",
    "<br>\n",
    "\n",
    "**Purpose:** The goal of this section is to obtain metadata for each instance data source used in the knowledge graph. To determine which of the edges contains instance data, the [`Master_Edge_List_Dict.json`](https://www.dropbox.com/s/4j0vrwx26dh8hd1/Master_Edge_List_Dict.json?dl=1) file is parsed and saved to a nested dictionary (see example below). \n",
    "\n",
    "```python\n",
    "{\n",
    "  'complex': {\n",
    "              'chemical-complex': [[node_1, node_2]...[node_n, node_m]],\n",
    "              'complex-complex':  [[node_1, node_2]...[node_n, node_m]],\n",
    "              'complex-pathway':  [[node_1, node_2]...[node_n, node_m]],\n",
    "              },\n",
    "     'gene': {\n",
    "                'chemical-gene':  [[node_1, node_2]...[node_n, node_m]],\n",
    "                 'gene-disease':  [[node_1, node_2]...[node_n, node_m]],\n",
    "              }\n",
    "}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Once this dictionary is created, each major data type (examples shown in the list below) will be processed. For **[`Release V2.0.0`](https://github.com/callahantiff/PheKnowLator/wiki/v2.0.0)**, the following are instance data and require the compiling of metadata:\n",
    "- [Genes](#gene-metadata)\n",
    "- [RNA](#rna-metadata)\n",
    "- [Pathways](#pathway-metadata)\n",
    "- [Variants](#variant-metadata)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "____\n",
    "\n",
    "**Metadata:** The <u>metadata</u> we will gather includes:  \n",
    "\n",
    "| **Metadata Type** | **Definition** | **Example Node**  | **Example Node Metadata** | \n",
    "| :---: | :---: | :---: | :---: | \n",
    "| Label | The primary label or name for the node | `R-HSA-1006173` | \"CFH:Host cell surface\" |       \n",
    "| Description | A definition or other useful details about the node | `rs794727058` | This `germline` `single nucleotide variant (allele alteration: C➞T)` located on chromosome `5 (GRCh38: NC_000005.10, start/stop positions (126555930/126555930))` with `pathogenic` clinical significance and a last review date of `2/23/2015` (review status: `criteria provided, single submitter`). |        \n",
    "| Synonym | Alternative terms used for a node | `81399` | \"OR1-1, OR7-21\" |           \n",
    "\n",
    "<br>\n",
    "\n",
    "The metadata information will be used to create the following edges in the knowledge graph:  \n",
    "- **Label** ➞ node `rdfs:label`  \n",
    "- **Description** ➞ node `obo:IAO_0000115` description \n",
    "- **Synonyms** ➞ node `oboInOwl:hasExactSynonym` synonym \n",
    "\n",
    "<br>\n",
    "\n",
    "*<b>NOTE.</b> All node metadata datasets are written to the `node_data` directory. The algorithm will look for data in this directory and if it is not there, then no node metadata will be created.*\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Metadata Dictionaries\n",
    "***\n",
    "\n",
    "**Purpose:** To create the resources needed in order to create metadata dictionaries, which are in turn used to obtain metadata for instance data nodes. This process has the following steps:\n",
    "\n",
    "**1. [Identify Instance Data Nodes](#identify-instance-data-nodes):** In order to automatically obtain the list of edges that include an instance data source and their corresponding edge lists, the `Master_Edge_List_Dict.json` is read in and processed.  \n",
    "  - <u>Input Data</u>: [`Master_Edge_List_Dict.json`](./resources/Master_Edge_List_Dict.json)  \n",
    "\n",
    "<br>\n",
    "\n",
    "**2. [Generate Metadata Dictionaries](#generate-metadata-dictionaries):** In order to efficiently obtain metadata for the instance data nodes identified in _Step 1_, we first read in the data for each node type (i.e. genes, rna, pathways, and variants) and convert it into a dictionary. Then, each metadata dictionary is saved to a `master_metadata_dictionary`, keyed by node type.\n",
    "  - <u>Input Datasets</u>:  \n",
    "    - Genes ➞ [`Homo_sapiens.gene_info`](https://www.dropbox.com/s/f2nz5q6g46u0tth/Merged_gene_rna_protein_identifiers.json?dl=1)    \n",
    "    - RNA ➞ [`ensembl_identifier_data_cleaned.txt`](https://www.dropbox.com/s/ssf5xopzqtqyho3/ensembl_identifier_data_cleaned.txt?dl=1) \n",
    "    - Pathways ➞ [`reactome2py API`](https://github.com/reactome/reactome2py)   \n",
    "    - Variants ➞ [`variant_summary.txt.gz`](ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz)  \n",
    "\n",
    "<br>\n",
    "\n",
    "**3. [Write Metadata Files](#write-metadata-files):** The Instance data node dictionary from _Step 1_ and metadata dictionaries from _Step 2_ are used to write `.txt` files for all `edge-type` data included in the instance node dictionary.\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Instance Data Nodes  <a class=\"anchor\" id=\"identify-instance-data-nodes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data files for each edge type\n",
    "edge_data = json.load(open('./resources/Master_Edge_List_Dict.json', 'r'))\n",
    "edge_dict = {key:[edge_data[key]['data_type'], edge_data[key]['edge_list']] for key in edge_data.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort Data**  \n",
    "For all edges in the `edge_dict()` that include instance data, we create a new dictionary where each edge type is further organized by node from the edge type that references the instance data (e.g. from the `chemical-gene` edge type, the `gene` node references instance data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 72597.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# sort data files\n",
    "metadata_file_info = {}\n",
    "\n",
    "for edge in tqdm(edge_dict.keys()): \n",
    "    if 'instance' in edge_dict[edge][0]:\n",
    "        \n",
    "        # get instance type\n",
    "        inst_type = edge.split('-')[edge_dict[edge][0].split('-').index('instance')]\n",
    "        \n",
    "        # read in data\n",
    "        if inst_type in metadata_file_info.keys(): \n",
    "            metadata_file_info[inst_type][edge] = {}\n",
    "            metadata_file_info[inst_type][edge]['data'] = edge_dict[edge][1]\n",
    "            metadata_file_info[inst_type][edge]['instance_idx'] = edge_dict[edge][0]\n",
    "        else:\n",
    "            metadata_file_info[inst_type] = {}\n",
    "            metadata_file_info[inst_type][edge] =  {}\n",
    "            metadata_file_info[inst_type][edge]['data'] =  edge_dict[edge][1]\n",
    "            metadata_file_info[inst_type][edge]['instance_idx'] = edge_dict[edge][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Metadata Dictionaries  <a class=\"anchor\" id=\"generate-metadata-dictionaries\"></a>\n",
    "In this step, the goal is to create a metadata dictionary for each node type that does not rely on API data. In this case, only the **Gene**, **RNA**, and **Variant** nodes require data that is not from an API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genes Metadata Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrez gene data\n",
    "entrez_gene_data = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.gene_info',\n",
    "                                   header=0,\n",
    "                                   delimiter='\\t',\n",
    "                                   low_memory=False)\n",
    "\n",
    "# remove all rows that are not human\n",
    "entrez_gene_data = entrez_gene_data.loc[entrez_gene_data['#tax_id'].apply(lambda x: x == 9606)]\n",
    "\n",
    "# replace NaN and '-' with 'None'\n",
    "entrez_gene_data.fillna('None', inplace=True)\n",
    "entrez_gene_data.replace('-','None', inplace=True, regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Create Gene Metadata Dictionary_  \n",
    "The nested dictionary of gene metadata is created by looping over the merged data described in the prior column. The `keys` of the dictionary are `Entrez gene identifiers` and the `values` are dictionaries for each metadata type: `symbol`, `description`, and `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61573/61573 [00:17<00:00, 3550.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# create metadata\n",
    "genes, label, description, synonym = [], [], [], []\n",
    "\n",
    "for idx, row in tqdm(entrez_gene_data.iterrows(), total=entrez_gene_data.shape[0]):\n",
    "    # node \n",
    "    if row['GeneID'] != 'None':\n",
    "        genes.append(row['GeneID'])\n",
    "    \n",
    "    # label -- only want metadata if there is a label\n",
    "    if row['Symbol'] != 'None' or row['Symbol'] != '':       \n",
    "        label.append(row['Symbol'])\n",
    "    else:\n",
    "        label.append('Entrez_ID:' + row['GeneID'])\n",
    "\n",
    "    # description        \n",
    "    if row['Full_name_from_nomenclature_authority'] != 'None' and row['type_of_gene'] != 'None' and row['chromosome'] != 'None' and row['map_location'] != 'None':\n",
    "\n",
    "        description.append(\"{desc} has locus group '{gene}' and is located on chromosome {chrom} (map_location: {map_loc}).\".format(desc=row['Symbol'],\n",
    "                                                                                                                             gene=row['type_of_gene'],\n",
    "                                                                                                                             chrom=row['chromosome'],\n",
    "                                                                                                                             map_loc=row['map_location']))\n",
    "\n",
    "    else:\n",
    "        description.append(\"{desc} locus group '{gene}'.\".format(desc=row['Symbol'], gene=row['type_of_gene']))\n",
    "\n",
    "    # synonym        \n",
    "    if row['Synonyms'] != 'None' and row['Other_designations'] != 'None':\n",
    "        syns = '|'.join(set([x for x in (row['Synonyms'] + row['Other_designations']).split('|') if x != 'None' or x != '']))\n",
    "        synonym.append(syns)\n",
    "    elif row['Synonyms'] != 'None':\n",
    "        syns = '|'.join(set([x for x in (row['Synonyms']).split('|') if x != 'None' or x != '']))\n",
    "        synonym.append(syns)\n",
    "    elif row['Other_designations'] != 'None':\n",
    "        syns = '|'.join(set([x for x in (row['Other_designations']).split('|') if x != 'None' or x != '']))\n",
    "        synonym.append(syns)\n",
    "    else:\n",
    "        synonym.append('None')\n",
    "            \n",
    "    \n",
    "# combine into new data frame        \n",
    "gene_metadata_final = pandas.DataFrame(list(zip(genes, label, description, synonym)), columns =['ID', 'Label', 'Description', 'Synonym'])\n",
    "\n",
    "# make all variables string\n",
    "gene_metadata_final = gene_metadata_final.astype(str)\n",
    "\n",
    "# dedup\n",
    "gene_metadata_final.drop_duplicates(subset='ID', keep='first', inplace=True)\n",
    "\n",
    "# convert df to dictionary\n",
    "gene_metadata_final.set_index('ID', inplace=True)\n",
    "gene_metadata_dict = gene_metadata_final.to_dict('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNA Metadata Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_gene_data = pandas.read_csv(processed_data_location + 'ensembl_identifier_data_cleaned.txt',\n",
    "                                header=0,\n",
    "                                delimiter='\\t',\n",
    "                                low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ensembl_gene_id', 'transcript_stable_id', 'symbol', 'primary_symbol',\n",
       "       'ensembl_gene_type', 'transcript_name', 'ensembl_transcript_type',\n",
       "       'master_gene_type', 'master_transcript_type', 'protein_stable_id',\n",
       "       'uniprot_id', 'entrez_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rna_gene_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Normal data preprocess and filtering steps are performed in order to prepare the data for the next step, which converts it to a metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows without identifiers\n",
    "rna_gene_data = rna_gene_data.loc[rna_gene_data['transcript_stable_id'].apply(lambda x: x != 'None')]\n",
    "\n",
    "# remove unneede columns\n",
    "rna_gene_data.drop(['ensembl_gene_id', 'symbol', 'protein_stable_id', 'uniprot_id', 'master_transcript_type',\n",
    "                    'entrez_id', 'ensembl_gene_type', 'master_gene_type', 'primary_symbol'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "rna_gene_data.drop_duplicates(subset=['transcript_stable_id', 'transcript_name', 'ensembl_transcript_type'], keep='first', inplace=True)\n",
    "\n",
    "# replace NaN with 'None'\n",
    "rna_gene_data.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Create RNA Metadata Dictionary_  \n",
    "The nested dictionary of rna metadata is created by looping over the cleaned human [Ensembl](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#ensembl) gene, RNA, and protein identifier data set ([`ensembl_identifier_data_cleaned.txt`](https://www.dropbox.com/s/ssf5xopzqtqyho3/ensembl_identifier_data_cleaned.txt?dl=1)). The `keys` of the dictionary are `Ensembl transcript identifiers` and the `values` are dictionaries for each metadata type: `symbol`, `description`, and `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243643/243643 [00:42<00:00, 5704.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# create metadata\n",
    "rna, label, description, synonym = [], [], [], []\n",
    "\n",
    "for idx, row in tqdm(rna_gene_data.iterrows(), total=rna_gene_data.shape[0]):\n",
    "    # node\n",
    "    rna.append(row['transcript_stable_id'])\n",
    "    \n",
    "    # label info\n",
    "    if row['transcript_name'] != 'None':\n",
    "        label.append(row['transcript_name'])\n",
    "    else:\n",
    "        rna_type = 'Ensembl_Transcript_ID:' + row['transcript_stable_id']\n",
    "    \n",
    "    # rna type info\n",
    "    rna_type = row['ensembl_transcript_type']\n",
    "\n",
    "    if rna_type != 'None':\n",
    "        # description\n",
    "        description.append(\"Transcript {desc} is classified as type '{typ}'.\".format(desc=row['transcript_name'], typ=rna_type))\n",
    "    else:\n",
    "        # description\n",
    "        description.append('None')\n",
    "\n",
    "    # synonym\n",
    "    synonym.append('None')\n",
    "    \n",
    "# combine into new data frame\n",
    "rna_metadata_final = pandas.DataFrame(list(zip(rna, label, description, synonym)),\n",
    "                                      columns =['ID', 'Label', 'Description', 'Synonym'])\n",
    "\n",
    "# convert df to dictionary\n",
    "rna_metadata_final.set_index('ID', inplace=True)\n",
    "rna_metadata_dict = rna_metadata_final.to_dict('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variant Metadata Dictionary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Download Data_  \n",
    "Only run this code block if the `variant_summary.txt` has not already been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz'\n",
    "data_downloader(url, unprocessed_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_data = pandas.read_csv(unprocessed_data_location + 'variant_summary.txt',\n",
    "                           header=0,\n",
    "                           delimiter='\\t',\n",
    "                           low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Normal data preprocess and filtering steps are performed in order to prepare the data for the next step, which converts it to a metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows without identifiers\n",
    "var_data = var_data.loc[var_data['Assembly'].apply(lambda x: x == 'GRCh38')]\n",
    "var_data = var_data.loc[var_data['RS# (dbSNP)'].apply(lambda x: x != -1)]\n",
    "\n",
    "# de-dup data\n",
    "var_metadata = var_data[['#AlleleID', 'Type', 'Name', 'ClinicalSignificance', 'RS# (dbSNP)', 'Origin',\n",
    "                         'ChromosomeAccession', 'Chromosome', 'Start', 'Stop', 'ReferenceAllele',\n",
    "                         'Assembly', 'AlternateAllele','Cytogenetic', 'ReviewStatus', 'LastEvaluated']] \n",
    "\n",
    "# replace NaN with 'None'\n",
    "var_metadata.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicate dbSNP ids by choosing the most recent reviewed variant\n",
    "var_metadata.sort_values('LastEvaluated', ascending=False, inplace=True)\n",
    "var_metadata.drop_duplicates(subset='RS# (dbSNP)', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Create Variant Metadata Dictionary_  \n",
    "The nested dictionary of rna metadata is created by looping over the human [ClinVar Variant](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#clinvar) identifier data set ([`variant_summary.txt`](https://www.dropbox.com/s/g58b0oduv7sj3ja/variant_summary.txt?dl=1)). The `keys` of the dictionary are `dbSNP identifiers` and the `values` are dictionaries for each metadata type: `symbol`, `description`, and `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 429711/429711 [02:13<00:00, 3215.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# create metadata\n",
    "variant, label, description = [], [], []\n",
    "\n",
    "for idx, row in tqdm(var_metadata.iterrows(), total=var_metadata.shape[0]):\n",
    "    # node\n",
    "    if row['RS# (dbSNP)'] != 'None':\n",
    "        variant.append('rs' + str(row['RS# (dbSNP)']))\n",
    "    \n",
    "    # label -- only want metadata if there is a label\n",
    "    if row['Name'] != 'None':\n",
    "        label.append(row['Name'])\n",
    "    else:\n",
    "        label.append('dbSNP_ID:rs' + str(row['RS# (dbSNP)']))\n",
    "    \n",
    "    # description\n",
    "    sent = \"This variant is a {Origin} {Type} that results when a {ReferenceAllele} allele is changed to {AlternateAllele} on chromosome {Chromosome} ({ChromosomeAccession}, start:{Start}/stop:{Stop} positions, cytogenetic location:{Cytogenetic}) and has clinical significance '{ClinicalSignificance}'. This entry is for the {Assembly} and was last reviewed on {LastEvaluated} with review status '{ReviewStatus}'.\"\n",
    "    description.append(sent.format(Origin=row['Origin'], Type=row['Type'], ReferenceAllele=row['ReferenceAllele'],\n",
    "                                   AlternateAllele=row['AlternateAllele'], Chromosome=row['Chromosome'],\n",
    "                                   ChromosomeAccession=row['ChromosomeAccession'], Start=row['Start'],\n",
    "                                   Stop=row['Stop'], Cytogenetic=row['Cytogenetic'], ClinicalSignificance=row['ClinicalSignificance'],\n",
    "                                   Assembly=row['Assembly'], LastEvaluated=row['LastEvaluated'], ReviewStatus=row['ReviewStatus']))\n",
    "\n",
    "# combine into new data frame\n",
    "var_metadata_final = pandas.DataFrame(list(zip(variant, label, description)), columns =['ID', 'Label', 'Description'])\n",
    "\n",
    "# drop duplicates\n",
    "var_metadata_final.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# make all variables string\n",
    "var_metadata_final = var_metadata_final.astype(str)\n",
    "\n",
    "# convert df to dictionary\n",
    "var_metadata_final.set_index('ID', inplace=True)\n",
    "var_metadata_dict = var_metadata_final.to_dict('index')                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Master Metadata Dictionary**  \n",
    "To make it easier to navigate the mapping of each instance node in an edge, a master dictionary is created and keyed by node type. This is most useful when both nodes in an edge are instances, but of different data types (e.g. `gene-rna`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_metadata_dictionary = {'gene': gene_metadata_dict, 'rna': rna_metadata_dict, 'variant': var_metadata_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Metadata Files  <a class=\"anchor\" id=\"write-metadata-files\"></a>   \n",
    "using the `Master Metadata Dictionary` created in the prior step, all of the `edge-type` data is processed and the resulting data written out `.txt` file to the `./resource/node_data` repository.\n",
    "\n",
    "- [Genes](#gene-metadata)\n",
    "- [RNA](#rna-metadata)\n",
    "- [Pathways](#pathway-metadata)\n",
    "- [Variants](#variant-metadata)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genes <a class=\"anchor\" id=\"gene-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Pages:** [NCBI Gene](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#ncbi-gene) \n",
    "\n",
    "**Output:**  \n",
    "- chemical-gene ➞ [`chemical-gene_GENE_METADATA.txt`](https://www.dropbox.com/s/fvkqnuk5xhs0huh/chemical-gene_GENE_METADATA.txt?dl=1) \n",
    "- gene-disease ➞ [`gene-disease_GENE_METADATA.txt`](https://www.dropbox.com/s/o0y21rx3b829q6d/gene-disease_GENE_METADATA.txt?dl=1) \n",
    "- gene-gene ➞ [`gene-gene_GENE_METADATA.txt`](https://www.dropbox.com/s/i4gznnct7rzh7pn/gene-gene_GENE_METADATA.txt?dl=1) \n",
    "- gene-pathway ➞ [`gene-pathway_GENE_METADATA.txt`](https://www.dropbox.com/s/yncd95vanhkp0ey/gene-pathway_GENE_METADATA.txt?dl=1) \n",
    "- gene-phenotype ➞ [`gene-phenotype_GENE_METADATA.txt`](https://www.dropbox.com/s/jghcoc5xzada011/gene-phenotype_GENE_METADATA.txt?dl=1) \n",
    "- gene-protein ➞ [`gene-protein_GENE_METADATA.txt`](https://www.dropbox.com/s/6vu961lna08qn08/gene-protein_GENE_METADATA.txt?dl=1) \n",
    "- gene-rna ➞ [`gene-rna_GENE_METADATA.txt`](https://www.dropbox.com/s/vs0kirmugdo9zkd/gene-rna_GENE_METADATA.txt?dl=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'gene'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_data_location + edge_type + '_' + node_type.upper() + '_METADATA.txt',\n",
    "                   header=True,\n",
    "                   sep='\\t',\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNA<a class=\"anchor\" id=\"rna-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Pages:**  \n",
    "- [Ensembl](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#clinvar)  \n",
    "- [NCBI Gene](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources/#ncbi-gene) \n",
    "\n",
    "**Output:**  \n",
    "- chemical-rna ➞ [`chemical-rna_RNA_METADATA.txt`](https://www.dropbox.com/s/sm0orl0waq5iqhd/chemical-rna_RNA_METADATA.txt?dl=1) \n",
    "- rna-anatomy ➞ [`rna-anatomy_RNA_METADATA.txt`](https://www.dropbox.com/s/plkrunhhusx6mez/rna-anatomy_RNA_METADATA.txt?dl=1) \n",
    "- rna-cell ➞ [`rna-cell_RNA_METADATA.txt`](https://www.dropbox.com/s/dld0eadxyyzr44y/rna-cell_RNA_METADATA.txt?dl=1) \n",
    "- rna-protein ➞ [`rna-protein_RNA_METADATA.txt`](https://www.dropbox.com/s/3g72sb2e685rptn/rna-protein_RNA_METADATA.txt?dl=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'rna'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = hmetadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_data_location + edge_type + '_' + node_type.upper() + '_METADATA.txt',\n",
    "                   header=True,\n",
    "                   sep='\\t',\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pathways<a class=\"anchor\" id=\"pathway-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Reactome](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#reactome-pathway-database)  \n",
    "\n",
    "**Output:**    \n",
    "- chemical-pathway ➞ [`chemical-pathway_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/2txg2ui4e6y7rnm/chemical-pathway_PATHWAY_METADATA.txt?dl=1)\n",
    "- gobp-pathway ➞ [`gobp-pathway_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/bq0g1g4ef40vwxj/gobp-pathway_PATHWAY_METADATA.txt?dl=1)\n",
    "- pathway-gocc ➞ [`pathway-gocc_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/6fzkzjxj08u6jfi/pathway-gocc_PATHWAY_METADATA.txt?dl=1)\n",
    "- pathway-gomf ➞ [`pathway-gomf_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/gfqt86vujnoo7j5/pathway-gomf_PATHWAY_METADATA.txt?dl=1)\n",
    "- protein-pathway ➞ [`protein-pathway_PATHWAY_METADATA.txt`](https://www.dropbox.com/s/xadtz4c0ab4a7p9/protein-pathway_PATHWAY_METADATA.txt?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'pathway'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_data_location + edge_type + '_' + node_type.upper() + '_METADATA.txt',\n",
    "                   header=True,\n",
    "                   sep='\\t',\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants<a class=\"anchor\" id=\"variant-metadata\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [ClinVar](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources#clinvar)  \n",
    "\n",
    "**Output:**  \n",
    "- variant-disease ➞ [`variant-disease_VARIANT_METADATA.txt`](https://www.dropbox.com/s/vj440u5efwdwibl/variant-disease_VARIANT_METADATA.txt?dl=1)  \n",
    "- variant-gene ➞ [`variant-gene_VARIANT_METADATA.txt`](https://www.dropbox.com/s/geui7nby9h055bc/variant-gene_VARIANT_METADATA.txt?dl=1)  \n",
    "- variant-phenotype ➞ [`variant-phenotype_VARIANT_METADATA.txt`](https://www.dropbox.com/s/hnocd802detivdd/variant-phenotype_VARIANT_METADATA.txt?dl=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = 'variant'\n",
    "\n",
    "for edge_type in tqdm(metadata_file_info[node_type]):\n",
    "    print('\\nPROCESSING EDGE TYPE: {}'.format(edge_type))\n",
    "\n",
    "    # gather vars for processing data\n",
    "    data = metadata_file_info[node_type][edge_type]['data']\n",
    "    edge_data_type = metadata_file_info[node_type][edge_type]['instance_idx']\n",
    "    inst_idx = edge_data_type.split('-').index('instance')\n",
    "\n",
    "    # get list of nodes to map and the dictionary to use\n",
    "    # when nodes are of the same type (i.e. gene-gene)\n",
    "    if (edge_type.split('-')[0] == edge_type.split('-')[1]):\n",
    "        nodes = set([x for y in data for x in y])\n",
    "        \n",
    "        if edge_type.split('-')[0] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[0]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = hmetadata_api_mapper(list(nodes))\n",
    "            \n",
    "    # when nodes are both instances, but different types (i.e. gene-rna)\n",
    "    elif edge_data_type.split('-')[0] == edge_data_type.split('-')[1]:\n",
    "        data_res = []\n",
    "\n",
    "        for node in edge_type.split('-'):\n",
    "            nodes = set([x[int(edge_type.split('-').index(node))] for x in data])\n",
    "            \n",
    "            if node in master_metadata_dictionary:\n",
    "                metadata_dictionaries = master_metadata_dictionary[node]\n",
    "                data_res.append(metadata_dictionary_mapper(nodes, metadata_dictionaries))\n",
    "            else:\n",
    "                data_res.append(metadata_api_mapper(list(nodes)))\n",
    "    \n",
    "        # combine data into single df\n",
    "        results = pandas.concat(data_res, ignore_index=True)\n",
    "                \n",
    "    # when only one node is an instance\n",
    "    else:\n",
    "        nodes = set([x[int(inst_idx)] for x in data])\n",
    "        \n",
    "        if edge_type.split('-')[int(inst_idx)] in master_metadata_dictionary.keys():\n",
    "            metadata_dictionaries = master_metadata_dictionary[edge_type.split('-')[int(inst_idx)]]\n",
    "            results = metadata_dictionary_mapper(nodes, metadata_dictionaries)\n",
    "        else:\n",
    "            results = metadata_api_mapper(list(nodes))\n",
    "\n",
    "    # write data\n",
    "    results.to_csv(node_data_location + edge_type + '_' + node_type.upper() + '_METADATA.txt',\n",
    "                   header=True,\n",
    "                   sep='\\t',\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "```\n",
    "@misc{callahan_tj_2019_3401437,\n",
    "  author       = {Callahan, TJ},\n",
    "  title        = {PheKnowLator},\n",
    "  month        = mar,\n",
    "  year         = 2019,\n",
    "  doi          = {10.5281/zenodo.3401437},\n",
    "  url          = {https://doi.org/10.5281/zenodo.3401437}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
